AWS - polices made of json script
Roles - > Allowing one service accessing the other service : eg: ec2 accessing s3 bucket
Policies --> is nothing but its a permission that we giving to the user. in aws there a many default policies, if we want we can create our own. 
IAM is universal it is not depend on regions
cloud watch - billing alarm ->set the amount so that it will alarm when it reaches the amount

Currently there are 30 regions are there in aws


we can attach the policy directly to the user , and also we can attach the policy to the role and then that role can be assigned to the user.
which is best, the policy should be attached to the role and then it have to assigned to the user because 
 Roles allow you to grant time-limited access (e.g., a temporary security token) without permanently attaching permissions to the user. whereas the policy will be permentely attached with the user. 

**************************************
S3 -> Simple storage service
Files can be from 0bytes to 5 tb | S3 universal name space (s3 name should be unique)
multipart upload in S3 is specifically designed for efficiently uploading large files, like splitting them .
IF s3 files upload got succesfull we will be HTTP 200 code response (Exam que)
MFA for deleting the object. which will protect from other person deleting the object without your permission
*******************************************************************************************************
Below are the features that we paying for s3 while using

# storage  per gig
#Request and data retrieval 
Data transfer
Management and replication 


S3 charges based on storage (amount of data stored), requests (number and type of operations like PUT, GET), and data transfer (amount of data transferred out of S3).
**********************************************************************
S3 bucket classes  - (multiple region)
* S3 standard:  This is the default storage class for S3 buckets and provides high durability, availability, and performance for frequently accessed data
        99.99% availability
        99.99999999999% Durability (11 9)
        Stored in multiple region

* S3- IA (Infequently Accessed )  - (multiple region)
      It is designed for data that is accessed less frequently, Lower fee than s3 standard, but you are charged a retrieval fee

* S3 - intellignet Tiering
      Designed to optimze costs by automatically moving data to the most cost-effective  tier without any impact . It will move the data between standard and IA.
Automatically moves data between access tiers based on changing access patterns to optimize costs.


* S3 One Zone - IA
      This is for lower-cost option for infrequently accessed data, but stores data in a single availability zone instead of multiple availability zones.

###* S3 Glacier instant retrieval:
     It is an archive storage class that delivers the lowest-cost storage for data archiving and is organized to provide you with the highest performance and with more flexibility. S3 Glacier Instant Retrieval delivers the fastest access to archive storage. Same as in S3 standard, Data retrieval in milliseconds .

###* S3 Glacier Flexible Retrieval:
      It provides low-cost storage compared to S3 Glacier Instant Retrieval. It is a suitable solution for backing up the data so that it can be recovered easily a few times in a year. It just takes minutes to access the data. 

* S3 Glacier: This is a low-cost storage class designed for data archiving and long-term storage. It is suitable for data that is rarely accessed and can tolerate retrieval times of several hours.

* S3 Glacier deep archive
     This is also low cost storage archive then s3 glacier ,It is designed for data that is accessed once or twice a year and can tolerate retrieval times of 12 hours or more.. S3 Glacier Deep Archive also have the feature of objects replication.


#####################################################################################

simple exp of s3 glacier buckets and its differences.

S3 Glacier: This term generally refers to Amazon’s archival storage options, which include different retrieval methods based on your needs.below are the options that we can use

S3 Glacier Instant Retrieval: Designed for archive data that needs to be accessed immediately, with retrieval times in milliseconds.
S3 Glacier Flexible Retrieval: Offers multiple retrieval options (Expedited, Standard, and Bulk) with varying retrieval times from minutes to hours, providing flexibility based on how quickly you need the data. Generally has a lower storage cost compared to Instant Retrieval and offers free bulk retrievals, making it cost-effective for large-scale data retrievals1.

So, while both Instant Retrieval and Flexible Retrieval are part of the S3 Glacier family, they cater to different use cases based on retrieval speed and cost considerations.

Here’s a comparison between S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive:

S3 Glacier Instant Retrieval:

*Retrieval Time: Milliseconds.
*Use Case: For archive data that needs immediate access, such as medical images or news media assets.
*Cost: Higher retrieval cost compared to Flexible Retrieval and Deep Archive.

S3 Glacier Flexible Retrieval:

*Retrieval Time: 3 types
    Expedited: 1-5 minutes.
    Standard: 3-5 hours.
    Bulk: 5-12 hours.
*Use Case: For archive data that does not require immediate access but needs flexible retrieval options, such as backup or disaster recovery.
*Cost: Lower storage cost than Instant Retrieval, with free bulk retrievals.

S3 Glacier Deep Archive:

*Retrieval Time: 12 hours or more.
*Use Case: For long-term archival of data that is rarely accessed, such as compliance archives and digital media preservation.
*Cost: The lowest storage cost among all S3 Glacier options.


 #################################################################################

properties -- versioning , tag , encryption, intelligent tireing archive configuration
permissions - block public acess , bucket policy , ACL, CORS (corss orgin resource sharing)
metrics --  bucketsize, total number of objects and other metrics we can able to view. 
Management - lifecycle rules, Replication rules, Inventory configurations.
Access point - we can create access point here 

creation -- bucket name -- acs enabled,disabled  -- block public access -- bucket versioning -- tags --  encryption -- advance setting object lock 

bucket region is must 


Restricting Bucket access - > Bucket policies ( apply on whole bucket) object policies 	(applys on object) and IAM policies for users and groups that acceess the bucket

AES-256 - encryption are done by thins method

WE can set the logs for who ever acessing the object will store in logs, these logs can be save in same bucket or also in different aws account

s3 objects and bucket can be encrypted by three ways

* S3 managed keys - SSE-S3  (sse is server side encryption) -- managed by amazon s3 , it is basic encryption and there is no cost. which is default encryption method for s3 

* Aws key management service managed keys - SSE - KMS --- where our s3 buckets can be encrypted with the amazon kms service, where we can manage our keys like rotating the passwords, auditing and other things. this one will be cost for usage. but this can be more secure 

* Server side encryption with customer provided key --- where we can create our own key using openssl and while upload the data in s3 we have to pass the key along with the data. so using this key the s3 will encrypt the data and store it in the bucket and it will clear the key which is sent to s3 for encryption. so s3 only have the data which is encrypted. so when we have to download the data we have to pass the key with the command for download the object, then it will automatically encrypt the data and it will be downloaded in our path.  this is more more secure then the other two kms option that given above. 

DSSE-KMS   -  combines two layers of encryption  ( this is for only objects) not for bucket.

S3 Managed Encryption (SSE-S3): The first layer uses the default Amazon S3 encryption, which encrypts the data on disk with AES-256 (Advanced Encryption Standard).
AWS KMS (Key Management Service): The second layer uses AWS KMS keys (either AWS-managed keys or customer-managed keys) to encrypt the encryption keys used for SSE-S3 encryption.

Amazon S3 automatically encrypts all new objects by default using server-side encryption with Amazon S3 managed keys (SSE-S3) at no additional cost12. This means that any new objects you upload to S3 buckets are encrypted by default without requiring any additional configuration.

However, if you need more control over your encryption keys, you can choose other encryption options:

SSE-KMS: Server-side encryption with AWS Key Management Service keys, which allows you to manage your own keys and provides additional security features.
SSE-C: Server-side encryption with customer-provided keys, where you manage the encryption keys yourself.
#######################################################
S3- Versioning  ( Versioning can be enabled while creating the bucket) once its enabled we cant disable it , only we can suspend it. 

Stores all versions of objects( even it is deleted) , Its like backup tool
Once versioning is enabled it cannot be disabled, only it can be suspended ( properties -> bucket versioning -> suspend)
We can use MFA in versioning for additional layer of security

					
For eg : uploading a text file named doccument in bucket, and again uploading the same doccument with different content , and again uploading the same doccument with different content. SO that we have all the three versions which we have uploaded and if we delete the doccument also it will be in "list version tab"  so that we can restore the data again by deleting the "Delete marker " file so that the files will be restored again


IF we make the object public means it will not reflect public access in all the other versions of the object. you want to make it them public for each and every version of the object seperately.

Example :  First we make the object named txt file and make it public.  And the we made some changes and reupload them again , so we can two version of files. IN this case when we try to access the different version file( which is not made public  ) it will not work because we have to make them public seperately. because after making the object public we have uploaded the two files, so the two versions of files will not be in public.  IF we have to make all of them public again  we have to make them public each verison seperately.
#############################################################
Life cycle Management
************

go to any bucket and go to management tab and we can go to life cyle rules option and create rule for it 

Life cycle rule is used to transfer the objects between different storage tiers. We can set the days limit like when to transfer the files to different tires and which files we have to transfer
 
Automates moving your objects between the different storage tiers and also it can be applied to current versions and previous versions>

Using life cycle management we can also delete the versions of the object by setting the no of days. if we set 60 days all the version that has 6o days will be remvoedd. 

#########################################################################################################

S3 object lock and glacier vault lock   (both are similar)
*****************

Object lock :  

It is used to store objects using write once , read many (WORM) model
Object Lock: You enable Object Lock at the bucket level when creating a bucket or later, and you can apply it to individual objects.

Retention Modes: You can set Retention Modes:
Governance Mode: Users with specific permissions can override the lock (useful for administrators).
Compliance Mode: The object cannot be overwritten or deleted by anyone, not even the account owner, until the retention period expires.
Retention Period: You define a retention period (e.g., 1 year) during which the object is immutable.

S3 Object Lock is used to prevent objects from being deleted or modified for a set period, ensuring data immutability for compliance, legal holds, or protection against accidental deletion.
############################################################
S3 performance
*************
Prefix : In between the bucketname and object are prefix
eg : mybucketname/folder1/subfolder1/myfile.jpg  ---> here prefix is /folder1/subfolder1
eg : mybucketname/folder3/myfile.jpg    -> here prefix is /folder
The more prefix will be better performance
#######################################################
S3 select ->  we can use simple sql expersion to return only the data from the s3 that you are interested in instead of retrieving the entire object. Which improves the performance of your application

S3 Select enables you to query and retrieve specific data from large objects in S3, improving efficiency and reducing data transfer costs.

Example: You have a large CSV file with sales data, but you only need data for a specific region. S3 Select allows you to query just that portion, reducing the amount of data transferred.

 Instead of downloading a huge file to process it locally, you can use S3 Select to pull only the needed data, saving on bandwidth and processing costs.

Extract specific log entries from large log files stored in S3 for troubleshooting or monitoring, without needing to download the entire file.
####################################################################
AWS Organisations :
 it enables you to centrally apply policy-based control across multiple aws accounts. we can consolidate all the aws account into an organisations. 
Go to aws organisations -> invite other aws account -> accept the invitation from other aws accounts.

Centralized Billing: Manage multiple AWS accounts under one billing umbrella, consolidating costs for easier management.
Resource Sharing: Share resources like VPCs, IAM roles, and AWS Directory Services across accounts.  
Organizing Accounts by Teams/Departments: Separate AWS accounts for different teams or departments to isolate workloads and manage permissions.

SHARING s3 buckets across accounts
************
3 different ways to share s3 buckets acrross accounts
* Using Bucket policies and IAM -> programmatic access only    ( Applies only on buckets not on individual objects)
*Using Bucket ACLs and IAM -> Programmatic access only ( Individual Objects)
* corss-account IAM roles - Programatic and console access.
how to do CROSS-ACCOUNT s3 access :
****************
We have to create a organisational account and add the other aws account to the organisation account  ( My account -> my orgainisation account )
( Master account A) Go To IAM role and select "Another aws account" and type the acccount iD , and then attach policy for which service you need to acess  (eg: s3 ) and then enter role name "s3_cross account " so role is created and note the link for access from other account
Now go to account B :  Go and add user with admin acess  , we cant do that in root user
Once user created copy the  user access link and sign out and login as user
Go to account and switch role and click switch role and paste the s3cross account link and click switch role  . Now we logged in to another account but we have only s3 access ( you can create s3 bucket and view the objects )


Pratical:

Account A:
Create a role --> Choose otheraccount (enter the accountB account id) -> s3   with permission s3fulll acess 

Account B:
Login to user (not as root)
click switch role  it will ask for accountid (enter the account A id) and it will ask for role ( enter the role name that created in account A) . 

Now user can access the s3 alone

Since we used the account b user as admini before , the above steps worked . but when we does not give admin acess and give normal permission to the user the above step is alone not enough, we have to additionally add permission to the user for "assumerole"  its nothing but the user should have permission for "assumerole" 

AssumeRole is a mechanism that is used in specific scenarios to enable temporary, cross-account, or restricted access, or when delegating permissions to another service or user.  (simply we have permission to access the role and attach to itself so that it can able to access the other account services )


########################################
https://www.youtube.com/watch?v=3m8xwwCnrp0

Cross region replication
*****************
Versioning must be enabled on both the source and destination buckets
Files in an existing bucket are not replicated automatically ( once we created the replication it will ask whether it should sync the prvious data or upcoming date  by running batch job)
All subsequent updated files will be replicated automatically
Delete markers are not replicated and deleting individual versions or delete markers will not be replicated. IN options we can enable this to replicate.
How to do replication
Create a bucket in same region with versioning enabled . Go the master bucket (which we need to replicate) and go to management and replication rules and create a rule
 Choose IAM rule and create a new role  and then choose applies to all objects in "soucre bucker " option  and choose a bucket which you need to replicate and enable versioning and click save
Check creating a new object and check whether it is replicated. ( If you change the object in public in source end it will not be the same in the target end

####################################################################################
Cross account sync


Create two buckets in source and destination bucket.  source bucket enter the bucket policy given below and go to destination create an Role with given below policy  and attach that to user .



Destination IAM user enter the below policy:

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::cross1",                ---source bucket
                "arn:aws:s3:::cross1/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:PutObject",
                "s3:PutObjectAcl"
            ],
            "Resource": [
                "arn:aws:s3:::otheraccrepl",            ----destination bucket
                "arn:aws:s3:::otheraccrepl/*"
            ]
        }
    ]
}


Source bucket policy :

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowCopy",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::366106965857:user/srinivasan"   - destination Iam user 
            },
            "Action": [
                "s3:ListBucket",
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::cross1/*",
                "arn:aws:s3:::cross1"
            ]
        }
    ]
}


EC2 creation:

Name -- ami -- instance type -- keypair -- networksettings like vpc , subnet, autoasign publicip,  securitygroup -- storage --

################################################
S3 transfer acceleration
*********************
s3 transfer acceleration uses the cloudfront edge netwrok to accelrate your uploads to s3. instead of uploading directly to your s3 bucket, you can use a URL to upload directly to an edge loaction which will then tansfer that file to s3. 


Amazon S3 Transfer Acceleration is a feature that speeds up the upload and download of files to and from your S3 buckets. It achieves this by routing traffic through Amazon CloudFront's edge locations (CDN). These edge locations are spread across the world, which reduces latency and speeds up transfers, especially when the source or destination is far from the S3 bucket's region.

How It Works (Simple Explanation):
Normally, when you upload or download files to/from S3, your data travels directly between your device and the S3 bucket's region.
S3 Transfer Acceleration routes your data through the closest Amazon CloudFront edge location, which is often much closer to your device than the S3 bucket's region. 

MAINLY USED FOR TRANSFERING LARGER FILES TO S3 BUCKET. and also used for reducing the latency.

Choose the bucket that you need to enable s3 transfer acceleration -> properties --> enable transfer acceleration . we will get the endpoint urlccopy the endpoint. IF we need to transfer the files to that bucket we can use that endpoint url, or if we have to view some files in that bucket we can use the enpoint (eg :  endpoingurl:/djf.jpg)
Endpoint url is cloudfront edge network. 
Once transfer acceleration is enabled, it can only be suspend . we cant disable it

[root@ip-172-31-47-80 ec2-user]# aws s3 cp set s3://otheraccrepl --endpoint-url https://otheraccrepl.s3-accelerate.amazonaws.com
upload: ./set to s3://otheraccrepl/set


[root@ip-172-31-47-80 ec2-user]# aws s3 ls s3://otheraccrepl --endpoint-url https://otheraccrepl.s3-accelerate.amazonaws.com
2023-03-27 12:47:14       1055 TIMESHEET.txt
2023-03-27 23:04:41          0 set
[root@ip-172-31-47-80 ec2-user]#

aws s3api get-bucket-accelerate-configuration --bucket <your-bucket-name>   --- to get the endpoint url of our bucket, or we can get through cli itself. 


aws s3: High-level commands for general file operations (upload, download, sync, etc.). It's designed to make common S3 tasks easier, using simple commands and flags.

s3api: Low-level commands for detailed configurations and settings of S3 buckets or objects. It allows you to manage and configure advanced S3 features, such as replication, lifecycle policies, and transfer acceleration.


###########################################

DATASYNC:

Go to aws datasync service - > choose between aws storage services -> get started -> enter the source bucket details -> enter the destination bucket details -> and start the job
In this we can filter the files that needs to be transfer 
we can also choose the storage classs to destination

we can use s3 replication for data transfer , but in different situation we can use the aws datasync, like one time transfer etc. 

AWS DataSync is used for fast, secure, and automated data transfer between on-premises storage and AWS or between different AWS storage services. It’s especially useful for migrating large datasets, automating backups, syncing hybrid cloud environments, and archiving data.

Data Migration to the Cloud:

Scenario: A company wants to move petabytes of data from their on-premises storage to Amazon S3 or Amazon EFS for backup or long-term storage.
Use Case: AWS DataSync allows for fast, secure, and automated migration without needing to write custom scripts or manually transfer data. It can run on a schedule, ensuring minimal disruption to operations.
Disaster Recovery and Backup:

Scenario: An enterprise needs to regularly back up critical data from on-premises systems to AWS cloud storage (like S3 or EFS).
Use Case: DataSync can perform regular, automated backups to ensure data availability and protection against disasters, ensuring that large datasets are transferred quickly and securely.

Set Up AWS DataSync Agent (On-Premises):

The DataSync agent is a virtual machine (VM) that runs in your on-premises environment and acts as the bridge between your local storage and AWS.
How to Set It Up:
Download the DataSync agent from the AWS Management Console. its like a image
Install the agent on your on-premises server or VM that can access your local data (either on-prem NFS, SMB, or other supported file systems).
Configure the agent and activate it by connecting it to your AWS account.
This process will create an agent within the AWS DataSync service in your account.
Create a DataSync Task in AWS:

A DataSync task is the set of instructions that specifies what data to transfer, where to transfer it to (e.g., your S3 bucket), and how often the transfer should occur.
How to Set It Up:
Go to the AWS Management Console → DataSync → Create Task.
Select your on-premises data source (which will be the data server you installed the DataSync agent on).
Select the destination (in this case, Amazon S3).
You can also choose to enable data encryption during transfer or set up a transfer schedule (e.g., to run daily or on-demand).
Example:
Source: On-premises NFS share, SMB, or local file system.
Destination: Amazon S3 bucket.
#################################################
Cloud front : CDN content delivery network  -- GLOBAL

orgin domain (s3 bucket name ) -- originpath (/index)--- protocal policy (http,https) -- httpmethod (getmethod, put method )-- restrict viewr access - cache policy (some default policy is already there) -- waf -- location(3 types of location already mentioned below) -- cname for alternate domain -- ssl (if cname we used) -- standard loggin (logs for viewer request )  --ipv6 enable or disable


Edge location : this is loacation where content will be caached . This is separate to an aws region
Orgin : This is the orgin of all the files that the CDN will distribute. This can be an s3 bucket , an ec2 instance, or an elastic load balance or route 53
Distribution: This is the name given the CDN which consists of a collectionof edge location 

Amazon cloud front can be used to deliver your entire website, including dynamic, static,streaming  content using a global network of edge locations. requests for your content are automatically routed to the nearest edge location 

Cloud front has 2 different types of distribution 1) web distribution - typically used for websites 2) RTMP - used for media streaming

Edge locations are not just READ only  you can write to them too (ie putting an object to edge location and that transfer to s3 bucket)
Objects are cached for the life of the TTL (time to live ) option will be available while creating 
You can clear cached objects , but you will be charged.




When to Use OAC: origin access control   -- It allows CloudFront to access the content securely, without exposing your origin (e.g., S3) to the public internet.
if you have a file in s3 ,and you created cloud front endpoint to that. and while creating cloudfront you can avail the option like only cloudfront can access the file in s3 , and no one can access it via publicly so that it will be safe. 
Private content: If you're serving private content from Amazon S3 and want to ensure that only CloudFront can access it.
Enhanced security: When you need fine-grained access control and want to keep your origin protected from direct public access.


once we enabled that, cloudfront will create below bucket policy to the s3.  

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::cloudfront-user:role/OAC-Role"
            },
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::your-bucket-name/*"
        }
    ]
}

this below code is the real one which i get after creating the Cf

{
        "Version": "2008-10-17",
        "Id": "PolicyForCloudFrontPrivateContent",
        "Statement": [
            {
                "Sid": "AllowCloudFrontServicePrincipal",
                "Effect": "Allow",
                "Principal": {
                    "Service": "cloudfront.amazonaws.com"
                },
                "Action": "s3:GetObject",
                "Resource": "arn:aws:s3:::regdxed/*",
                "Condition": {
                    "StringEquals": {
                      "AWS:SourceArn": "arn:aws:cloudfront::156041398855:distribution/E35D1COHNIG1PS"
                    }
                }
            }
        ]
      }

so we have to update this in s3  -- so that only that role can access you s3 bucket object 

########################################################
Create a cloud front distrubution - LAB

Go to network - > cloud front  ( cloud front is global services it is not region basis )

cloud front -> create distribution -> web -> get started -> choose the origin  | we can also restrict bucket accesss - by users always need to access amazon s3 content using cloudfront url not amazon s3 url . and also we can set TTL value in the filed. and also we can restict viewer access (use signed urls or signed cookies ) eg: amazon, netflix only the user have prime account can access the video . and create distribution 
copy the domainname   paste it in web and type the  file name in the s3 bucket eg : domainname/file.jpg (web)
Go to the distribution and go to settings and there will be invalidation options ( which is used to remove the objects from edge locations ) we can enter the path of the file in the box which will remove from the edge location . We can invalidate individual objects and also full directories like giving in the box /* 

Invalidation : Invalidating objects removes them from cludfront edge caches.  we can do this in aws console like choose our distribution and create invalidation

delte the distribution it is not free tier


Cloudfront example:

if s3 has a certain videos or pictures, so when user try to access the s3 he may see latency , because many of them will try to access the s3 at a time, and also cost will be high for each and every get request in the s3.  in that case we using cloud front. we can create a distribution in the cloud front and point to our s3 bucket. where cloud front itself will create a user (functional id) to access the s3 bucket for datas, to disply to the end user. where we blocked public access to s3 bucket no one can access the s3 url to get the datas, but cloudfront have access to access the data in the s3 bucket by functional id. where it will create bucket policy that only this function id  can access the datas from the bucket. so when we try to access our videos or website that hosted in the s3 bucket it will reach the cloudfront and cloudfront will access the datas from the s3 and then it will share the data. where in this we giving security to the s3 for not access the data directly by any user and also the latency where the datas will be cached in all the edge routers. 




####################################################################################

Cloudfront signed url and  signed cookies :


A Signed url is for individual file  1file = 1 URL
A signed cookies is for mulitple files   multiple files = 1 cookies

When we create a signed url or signed cookies, we attach a policy

We can also put below settings
* URL expiration * IP range * Trusted signers (which AWS account can create signed URL )

Signed URLs: You can create signed URLs that grant time-limited access to specific content. Only users who have the signed URL can access that particular content, and the URL expires after a specified period.

Signed Cookies: Signed cookies are used for granting access to multiple files (e.g., an entire folder or a set of assets) without generating signed URLs for each individual file. This is useful when you need to grant access to a set of resources without having to create individual URLs for each resource.

######################################################################################

cloud front signed url / s3 signed url
**************************************
Use signed URls/Cookies when you want to secure cntent so that only the people you authorize are able to access it  (netflix amaozon)
IF your origin is EC2 then use cloud front signed url  . and if it is S3 then use s3 signed url insted of cloud front signed url
Check
*****************************************************************
Snowball :  it is used to import to s3 and export from s3
If we need to transfer the large amount of data to aws we can use snowball ( physically they will transfer the data  , there will be box )
######################################################################


What is AWS Storage Gateway?
AWS Storage Gateway is a hybrid cloud storage service that connects on-premises environments (like your NFS or SMB servers) to AWS cloud storage (e.g., Amazon S3). It allows your on-premises applications to access cloud storage seamlessly as if they were using local storage.

Types of Storage Gateway:
File Gateway (NFS/SMB) – Provides cloud-backed file storage (mount S3 as a file system).
Tape Gateway – Used for backup and archive data.
Volume Gateway – Used for block-level storage for virtual machines (VMs).

Key Concepts
File Share: A File Share in Storage Gateway is a mountable network drive that allows access to cloud storage (Amazon S3) through NFS or SMB protocols.

NFS/SMB:

NFS (Network File System) is typically used for Linux-based systems.
SMB (Server Message Block) is typically used for Windows-based systems.
Cache Storage:

Cache is a small local disk (e.g., 15-100 GB) used by the Storage Gateway to store frequently accessed data, reducing latency and improving performance.
Data Written to Cache: When data is written to the mounted file share, it is first written to the cache and later asynchronously transferred to S3.
Data Flow:

When a file is saved to the mounted file share, it is initially saved to the cache (on your local machine or VM). The file is then asynchronously uploaded to S3.
If data is not in the cache, the gateway fetches it from S3 when needed.

Steps to Setup File Share with Storage Gateway:

Create Storage Gateway:

Launch an AWS Storage Gateway instance (choose File Gateway).  and choose where we have to launch our storage gateway in our case we going to launch it on our on-premise, so that it will be the mediator between our on-premissis and the cloud s3. so we have to launch it as vm, so download the image (which will be available there while configure the storage gateway) download it and run it as vm in the on-premisis. and once done it will have the IP. copy that IP.  so we launched the gateway in the on-premises. Now already we started the configureation in the aws storage gateway in that only we choose the vm exi and downloaded the images, so we have to continue that . we copied the IP right that ip should be entered there (it will ask for ip that have to connect with gateway) so once done it will connect with the gateway (vm) . and we can add extra disk in the vm where we launched the gateway, that disk can be used as cache disk. Will tell you how its used.  so the next step is file sharing step . in that we have to choose the s3 bucket and we have to choose the protocal as Nfs or smb  . choose nfs because it is Linux where we going to mount. once we created the file sharing, we will get the command to mount in our vms. so copy that and run it on the vms . so that it will be mounted in our vm , so if we store any file first it will store in our cache folder and then it will sync to the s3.   cache folder is for temperory, whatever the file we stored it will be stored in the s3 . but frequently used filed will be in cache so that it will be faster access whenever it needed.

Example:

we have to mount the s3 bucket in our vms . so what can we do is we can mount the s3 in the NFS server as /mnt/share . and this mount can be shared via multiple vms via export from the NFS . so that we can centraly manage the s3 that mounted in the NFS server. we can also mount the s3 directly to all the vms instead of mounting them in nfs server. but if we mount in the nfs server we can manage them and we can control them who can access and who can write it . 

so what and all we have to create .. NFS server , storage gateway appliance in the on-premises , s3 bucket in aws, and simple vm which will have the mount point. 

Create storage gateway applicane in the on-premises with the cache volume (extra disk) .  and copy this appliance IP and configure it on the storage gateway (it will ask the ip while configuring) so once we connect it then we have to configure the file share. create the file share (it is the part of the congiguration of storgage gateway in aws ) choose the s3 bucket and choose the protocal as NFS . once done it will give the Mount configuration. copy the mount command and go and mount it on the nfs server. 
once the mount point is stored in the nfs server. we can then share that mount point to other vms (in normal way how we share our nfs volume). just /etc/export edit and save the mount point details that we mounted . and then that mount point can be assessed in all the local vms. 

volume gateway : it is used for block level storages, 
example -- choose volume gateway option  --> choose the stored volume  -->and enter the size of the volume  and once done 
Attached to Server via iSCSI: The volume is connected to your on-premises server using iSCSI and then
The volume is formatted and mounted on the server, where the database is installed, and the database stores its data on this volume.
Backup to S3: AWS Storage Gateway periodically backs up the volume to S3 for protection.


if have free time check for volumegateway videos.

####################################################################################
Athena :

Athena is interactive query service  which allows you to query data located in s3 using standard SQL,
Serverless,nothing to provison , pay per query/per TB sccanned
commonly used to analyse log data stored in s3

DIFF BETWEEN s3 seelct/athena 

S3 Select: Primarily focused on querying and extracting specific data from individual objects (files) stored in S3. It operates on a file-by-file basis and is best for simple queries on a single file (e.g., CSV, JSON, or Parquet).

Athena: Used for querying and analyzing larger datasets stored in S3 by running SQL queries across multiple files or even entire datasets. Athena can handle more complex analysis and can aggregate data from multiple files or even databases.

Macie :
Macie helps organizations protect sensitive information and ensure compliance with data privacy regulations such as GDPR or CCPA. It automatically identifies sensitive data like personal information (PII) in your cloud environment, so you can better secure it and prevent accidental exposure.

Macie will automatically scan the S3 buckets for sensitive data, classify it, and provide you with alerts if it detects any risky or unauthorized exposure of sensitive information.

In short, Amazon Macie is designed to help organizations automate the discovery and protection of sensitive data, improving security, compliance, and reducing the risk of data breaches


Encrypted data cannot be directly scanned by Amazon Macie unless it has access to the necessary decryption keys.
If the data is encrypted with SSE-KMS, you need to ensure that Macie has permission to use the KMS keys to decrypt the data.
####################################################################

S3 & IAM summary
*****************
S3 Object lock - to store objects using a write once, read many (WORM) model 

 Policy will be in Json format, Sample is below

{
"version":"2012-10-17",
"Statement":[
   {
  "Effect":"Allow",
  "Action":"*",
   "Resource":"*"
   }
 ]
}

*The key Fundamentals of S3 are 
*Key  (This is simply the name of the object)
*Value ( This is the data and is made upa a sequence of bytes)
*Version ID ( important for versioning)
*Metadata (Data about data you are storing )
*Subresources:
  Access control lists
  Torrent

Prefix

You can also acheive high number of requestes : 3,500 put/copy/post/delete and 5,500 get/head requests per second per prefix
You can get better bperformance by spreading your reads across different prefixes(folder)  for example if you are using two prefixed, you can achieve 11000 requestes per second

* IF you are using SSE-KMS to encrypt your objects in S3, you must keep in mind the kms limits   (region specific)

*UPloading and dowmloading will count towards the kms Quota, and you cannot request a quota increase for KMS


#################################################################################
EC2 - Elastic compute cloud 
****************************

Ec2 is a webservice which provides resizable compute capacity in the cloud . And also it helps to create a new server within a minute .

Pricing Modules:

ON demand :

Without any upfront payment or long term commitment we can create a servers and also which is more flexible


Reserverd :

Have to pay upfront to reduce the total cost  and have to be in contract from 1 year to 3 year

Reserverd pricing types

  Standard reservered instance :  the more you upfront and the longer the contract, then there will be greater discount

   Convertible reservered instance : We can convert the instance type ( by increasing CPU usage Ram usage ) 

  Sheduled reservered instance : These are used to launch within the time window you reserve ( like school college timing 9 t0 6 )


Spot      : Amazon selling there excess capacity at lower range to occupy their capacity fully. once they needed their capacity then you have to pay what they ask otherwise  it will be back to them without any notification 

***************************
Create a EC2 instance - already done    (2 type of check "system status check and instance status check")
If we need to login to the instance from command line use " ssh ec2-user@ip -i key.pem "  (you should run the command where the key is located ) we can give the permission to the key as chomod 400 

If we create a windows , we have to add an extension to chrome "secure shell App " from that app we have to login to the windows console
* We have to get the Pub key to access the console. IN that case we can get the pub key from the private key which we downloaded while creating the VM
* ssh-keygen -y -f mykey.pem > mykey.pub   (which will give the pub key) we can do this from command prompt
Once we get the pub key , we have to change the mykey.pem to mykey (remove .pem) . And the upload both mykey & mykey.pub in the secure shell app to login to the windows 

windows server login:

EC2 Instance Running Windows: Make sure your EC2 instance is running a Windows Server (e.g., Windows Server 2016, 2019, or Windows 10).
Security Group: Ensure that the security group associated with your EC2 instance allows inbound traffic on RDP port 3389.
Private Key File (.pem): You'll need the private key file (.pem) you downloaded when you created the EC2 instance (used for initial login).

now go to aws windows ec2 machine and in setting choose get password and it will ask for pem file. give the pem file it will give the password. once get that using rdp you can login to that. 


Creating a webserver :

Yum install httpd -y
cd /var/www/html/   (go and create on html page )  <html><h1>hello</h1></html>  index.html(file name) | Service httpd start | chkconfig on ( will check and turn on the service if itis down)


* If we terminate the instance, the root partition will also be deleted but if we add the addtional volume (EBS) it will not be deleted while the instance is terminated , we should manually do it.

While creating the instance we can encrypt the root volume in the storage option and also we can encrypty the addtional volume

#####################################################################
Security groups - LAB

The changes made in security groups will take effects immedietly. 
If me remove all port from outbound rule and then try to access the webpage it will work , because in inbound we allowed the port 80 so obviously it will allow in outbound also even if we removed the port from outbound list . because security group is statefull.
We cant block list any ip or port in security groups, there is no block option. But default everything is block  in default we have to add it for allow, but we can do that in NACL (network access control list )
We can attach more than one security group for Ec . Action -> networking -> change security group -> and choose the other security grup

All inbound traffic is blocked by default
All outbound traffic is allowed
We can hae any number of EC2 instances withina security group.
#################################################################################
EBS 

Elastic block store . Comes 5 different types
                                                                                                                 USE CASES   
General purpose (SSD)                          - is gp2(API name)   volume size: 1 GB -16 tib  Max iops: 16000  (Genral purpose) SSD
provisioned IOPS (SSD)                         - io1 (API name)     volume size: 4gb - 16 gb   max iops :64000 (Databases) mission critical SSD
throughput optimised hard disk drive(low cost) - st1        : 500gb-16tb     :500  (big data & data warehouse)Freq access data and low cst Hdd
COLD hdd                                       - sc1          500gb-16tb     :250   (File servers) lowest cost and less feq access HDD
magnetic                                       - standard     1gb-1tb        :40-200  (data is Infreq Accessed) HDD 



General Purpose SSD (gp3):
Use Case: Suitable for a wide variety of workloads, including development, testing, boot volumes, and general-purpose applications.
Performance:
Baseline of 3,000 IOPS (Input/Output Operations Per Second) and 125 MB/s throughput.
Can provision up to 16,000 IOPS and 1,000 MB/s throughput.


Provisioned IOPS SSD (io2)
Use Case: High-performance workloads requiring low-latency and high IOPS, like large transactional databases or applications that require fast access to data.
Performance:
Up to 64,000 IOPS per volume and up to 1,000 MB/s throughput.
Consistent low-latency performance.
Cost: More expensive than gp3.
Example: Databases such as Oracle, SQL Server, or high-performance applications with heavy data transaction requirements.

A banking application requires high IOPS to process many small transactions, such as transferring money or querying account balances, in real-time.

3. Throughput Optimized HDD (st1)
Use Case: Large, sequential read and write workloads that are not latency-sensitive. Ideal for big data analytics, data warehousing, and streaming applications.
Performance:
500 MB/s throughput maximum per volume.
Lower IOPS (compared to SSD).
Example: Big data, log processing, and large data set storage.

A video streaming platform needs high throughput to deliver large video files quickly to users.

In short:
Throughput = Amount of data transferred (e.g., MB/s).
IOPS = Number of individual read/write operations (e.g., how many files can be read or written per second).

example:
Example in computing terms: This is like a database with many small read/write operations (such as querying a small piece of data). For example, reading small 1 KB records from the database repeatedly. There are many operations (high IOPS), but each operation handles only a small amount of data (low throughput).




 Cold HDD (sc1)
Use Case: Low-cost storage for infrequently accessed data that is cold, or for archival purposes.
Performance:
250 MB/s throughput maximum per volume.
Designed for workloads that require lower throughput.
Cost: Cheapest EBS option, suitable for less critical applications.
Example: Cold storage, backup, and archival data.

Magnetic (standard)
Use Case: This is the legacy EBS type, mostly replaced by modern SSD and HDD types. It was used for applications that do not require high performance but still needed reliable storage.
Performance: Lower IOPS and throughput compared to modern options.
Cost: Less expensive than SSD options but not commonly used anymore.




General Purpose (gp3) is often the best choice for most users due to its cost-effectiveness and balanced performance.
io2 should be used when you need high performance and low latency for database workloads or high-throughput applications.
st1 and sc1 are ideal for large, sequential workloads like big data and backups, where cost savings are a priority over low-latency performance.

gp2 is still an existing EBS volume type, but it is now considered legacy. Amazon Web Services (AWS has introduced gp3 as a more advanced, cost-effective, and flexible alternative to gp2.


gp2: Provides 3 IOPS per GB (e.g., a 100 GB gp2 volume gives 300 IOPS).
gp3: Allows you to provision IOPS independently of volume size, with a baseline of 3,000 IOPS and the ability to scale up to 16,000 IOPS regardless of the size of the volume. This makes it more flexible and performance-efficient than gp2.



#############################################################
EBS volumes and snapshots
**************
Ec2 instance and volume should be in same availablity zone .

Create a instance with 4 different EBS volume (except provisoned iops)
To extend the volume we have to click action and modify volume and enter the size that we need , and then we have to login to the server and extend it the volume that we create newly-allocated space.

two commands to extend the added space

lsblk  -> check whether the additional storage is added

[root@ip-172-31-47-80 ec2-user]# lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0  10G  0 disk
└─xvda1 202:1    0   8G  0 part /


sudo growpart /dev/xvda 1   ----- xvda is volume name and 1 is first partition 

[root@ip-172-31-47-80 ec2-user]# lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0  10G  0 disk
└─xvda1 202:1    0  10G  0 part /

now it is added to xvda1 . but when we check the output in df -h it will not be added for that we have to extend the file system

xfs_growfs -d /   -- once this command executed now root partition has the extra volume that added


We can also change the EBS volume type from gp2 to io1 in the modify volume option , no need to stop the instance or anything 

Creating IMAGE FROM EBS snapshot:  

Choose the volume and -> action -> create snapshot  (snapshot created succefully , it is nothing but complete copy of your volume) snapshots exits on s3 . 
We can look on the snapshot module to check the snapshot that we created
Once created snapshot , again choose the snapshot and click action and choose "Create image from EBS snapshot"  in that we have to choose the virtualization type "hardware-assisted virtualization" . and click create . once created then we can able to use that image to create a new Ec2 instance

Note :There are two types of virtualization one is "hardware-assisted virtualization" and other is paravirtual (PV)  - study deeply
In PV - will not allow us to choose many instance type ,only few or one 
in HVM - we will get many instance type that we can choose we wish


The created images will be list in AMIs module . Choose the Image and click launch which will direct to create instance  and while choosing choose the different zone in subnet option  . so that instance is lanched in that Availablity zone

AVailablity zone -> is nothing but us-east-1 us-east-1a like that
Region -> london , north verginia like that
  
We can also copy the AMI from our default region (NV region ) to different region like london and then we can create the instance there 
Go the custom ami that you created and action -> copy -> select the region that you want to make a copy of that AMI.

By doing this way we can move our ec2 instance from one region to other region by taking sanpshot and converting as AMI and moving that AMI to other region and make the ec2 on that region


EBS Snapshot: Focuses on the data stored on the volume(s). It's a good way to backup or replicate storage.
AMI Creation (Direct Snapshot of EC2): Captures the entire instance including both data and metadata, making it ideal for cloning or scaling EC2 instances.
By using an AMI, you are essentially creating a complete template of your EC2 instance, including both the operating system and application setup, while an EBS snapshot is a backup of the volume data only.

EBS SNAPSHOT convertion ami will not contain "Other attached EBS data volumes and Instance configuration like IAM roles, user data scripts, etc. and Instance metadata (like security groups, key pairs, instance type, networking config)) and also userdata scripts

but EC2 instance convertion ami will have all , exact clone of that instance .



***********************************************************************************************************************************************************************
Moving EBS from one AZ to other AZ --> take snapshot --> from snapshot create volume  create the volume in the zone that you needed -- but in this we cant move to one region to other region

IF we have to move EBS to other region , move the snapshot to other region and then create a volume from there 
********************************************************************************************************************************************************************************
Copying snapshot from one region to another region:
********************************************************
Go to the volume where your EBS snapshot resides.
Select the EBS snapshot you want to copy to another region and then click on the Copy Snapshot button.
Put a name and description on the EBS snapshot you want to copy to another region and then select the region you want to copy it to.
*********************************************************************************************************************************************************************************
Copyiing snapshot from one account to another account :

We cannot copyy the snapshot that are encrypted with default KMS key.  But we can do by changing the key from defafult to custom and them copyting to another account
To change the default kmskey -> choose the snapshot and copy (while copy option choose different key for encryption) and once copy completed -- we used the key from encryption key from IAM
Choose the copied snapshot and choose modify permission and give the account ID (which you need to share) 
Now we need to share the encryption key to the other account -> go to IAM -> encryption key - > choose the key that you used -> "OTHER AWS account" option enter the  other aws account id

Now we can see both KMS key and snpashot in another aws account 
To avoid dependencies from source account.  choose the shared snapshot and click copy and choose your own kms key again and copy it. so that it will be in your account. 
***********************************************************************************************************************************************
	
Delet the snapshot once used ( it will charge)

Snapshots are incremental - this means that only the blocks that have changed will be updated in your last snapshot 
################################################################################

What is Source/Destination Check?
The source/destination check is a setting in AWS that controls whether an EC2 instance or Elastic Network Interface (ENI) is allowed to forward traffic. By default, this setting is enabled, meaning the instance can only send or receive traffic meant for itself. If you disable the check, the instance can forward traffic for other instances or devices, meaning it can act as a router.

Source Check:
This is about sending traffic.
If source check is enabled, the instance can only send traffic where it is the source of the traffic (e.g., sending requests to a web server).
If source check is disabled, the instance can send traffic on behalf of other instances (e.g., forwarding requests from a private subnet to the internet).
Destination Check:
This is about receiving traffic.
If destination check is enabled, the instance can only receive traffic meant for itself (e.g., traffic directed specifically to the instance).
If destination check is disabled, the instance can receive traffic meant for other instances (e.g., acting as a router, forwarding traffic to other instances or destinations).



Latest update , ec2 has 3 check 
System Status Check: Monitors AWS infrastructure and hardware issues.
Instance Status Check: Monitors your EC2 instance's operating system and configuration.
Attached EBS Status Checks: These checks monitor the health of the Amazon EBS volumes attached to your instance. They detect issues like hardware or software problems on the storage subsystems or connectivity issues between the instance and EBS volumes




#############################################################################
EBS vs instance store
***************************

All AMI are categorized as either by Amazon EBS or instanse store

EBS volume  : The root device for an instance is lanched from the AMI  (amazone EBS volume) created from an amazson EBS snapshot(we normally use this one)
Instance store volume : The root device for an instance launched from the AMI in "instance store volume", created from a template stored in amazon s3  (once added to the ec2 we cant add extra volume in instance store volume and also we cannot stop or start the instance state on this storage type only reboot option and terminate). If the host fails you will loose our data . reboot doesnot loose the data . When terminating the instance for this type of storage the root device volume also delete automatically, and we cant keep that .

Instance store volumes are sometimes called Ephemeral storage

TO lanch the instance in instancce storgae , choose the custome ami and in that there will be option called instance store , select the option from that choose the ami that you wanted. 
Once the ec2 is lanched you cannot see the storage in volume (like EBS) . 

######################################################################

ENI vs ENA vs EFA  

ENI :Elastic network interface : its a virtual network card (when we provision an ec2 it will add automatically ) it is used for basic networking

EN : Enhanced Networking -> uses single root i/o virtualization (SR-IOV) to provide high-performance networking capabilities on instance types
Its just used for speeding up our network essentially , no additional cost we can add it on ec2 instance( only supported ec2 instance) 
when you need speeds between 10gbps and 1000gbps you canuse EN 
Enhanced networking can be enabled using:

      ENA : Elastic network adapter  which supports network speeds of up to 100 GBPS for supported instance types
	  or
	  Intel 82599 Virtual function (VF)interface - which supports network speeds of up to 10 gbps for supported instance types  ( used for older instances)
	  
   
EFA : Elastic Fabric Adapter - A network device that you can attach to your ec2 instance to accelerate high performance computing(HPC) and machine learning applications.

If you need to do an os by-pass , or HPC or Machine learning  what network adaptor you want to choose ( choose EFA)

##################################################
Encrypted root device volumes and snaphosts
******************************************

*While creating a ec2 instance in storage option we can choose the root volume as encrypt and then we can create a instance
* If the ec2 instance is already created with unencrypted root volume and now if we have to make it encrypted then follow the below steps

Create a snapshot of unencrypted volume and then go to snapshot section and make a copy of that same snapshot . while making a copy we can encrypt that snapshot there will be a option while copying the snapshot ( action -> copy ) . once the copy snapshot (encrypted) is created we can use that copysnapshot and create a AMI and with that ami we can  create an instance.

TIPS :
*If we are taking a snapshot of encrypted volumes then it will be also a encrypted snapshot
*We can share snapshots, but only if they are unecrypted, and also we can be shared with other aws accounts or made public  but it should be unecrypted
* you can encrypt the root device volume upon creation of the EC2 instance ( while starting itself we can encrypt the root volume)

############################################################
Spot instances and spot fleets
********************************

Spot instances let you take advantage of unused ec2 capacity in the aws cloud. it is available at upto a 90% discount compared to on-demand prices. We can use the spot instance for flexible application such as bigdata, and other test and development workloads and data processing, batch jobs, containerized applications, and development/test environments, where interruption and fault tolerance are not critical to the application’s success.

To use spot instances, you must first decide on your maximum spot price. The instance will be provisioned so long as the spot price is below your maximum spot price . The spot price varies depending on capacity and region 

IF spot price goes above your maximum , you have 2 min to choose whether to terminate or not the instance

SPOT BLOCK : It is use to stop the instances form being terminated even if the spot price goes over your max spot price. You can set spot blocks for between one to six hours currently

Usecase for spot instance : bigdata , ci/cd and testing , web services, image and media rendering

Spot instance can be done in two way 1) one-time 2) persistent . 
One-time  : Once the bid price is increased above your max price then it will be terminated 
Persistent : if the bid price is increased abover your max price it will be disabled and once the bid price decressed it will create a instance again

Spot fleet : Spot fleet has collection of spot instances and optionally on-demand instances . for eg : IF we request a no of servers in spot fleet to create , it will used to create the spot instances to full fill your request but in case if it cant create no of instances that you mentioned with spot instances then it will go for on-demand instance and create the remaining instance and full fill your request.
##############################################################################
Hibernate : if we hibernate the system , THe RAM content(memory) are stored in root volume, so that once we start the system again it will load the ram content from the volume and all the previous task will be there 

While creating the instance we can select the hibernet options to include while create the instance "configure instance module" . and also the root volume must be encrypted to use this option 

#####################################################################
Cloud Watch:  it will watch performance

we can use cloud watch for - > ec2 instance, autoscaling groups, ELB , Route 53 healthccheck , EBS volumes , cloud front 

IN EC2 instance it will monitor host level metrics like CPU  , Network, DISK, status check.

It is used to monitor our AWS resources as well as the application that runs on AWS  ( cpu \ network , volumes etc)

Cloud trail:   It will monitor the API calls in the Aws platform  ( aws console actions and API calls that made in console ) 

It shows the user and resource activity by recording aws management console actions and API calls . YOu can identify which users and accounts called the aws, the source IP address from which the calls are made and when the calls are occured 


cloud watch is used for monitoring, alerting, reporting ,logging , cost optimization . we can also create custom metrics and monitor . 
We can also monitor lambda function, cost optimization (Monitor resources such as EC2 instances, RDS databases, and S3 storage to track underutilization or over-provisioning, which can lead to cost savings) , application monitoring ( Monitor custom application metrics such as response time, user requests, or database queries to ensure service reliability.)

there are over 1100 + aws metrics are available in cloud watch , in that we can create custom metrics also according to our needs. 


Amazon CloudWatch Events is a service that helps you monitor and respond to events happening in your AWS environment. It captures changes and activities from AWS services (like EC2 or S3), and can trigger automatic actions like sending alerts, starting a Lambda function, or changing settings based on these events. It’s useful for automating tasks and monitoring the health of your infrastructure in real-time.


DIFF:
CloudWatch Monitoring: Tracks metrics like CPU usage, disk I/O, etc., and sends alarms when thresholds are crossed. It’s focused on monitoring and alerting.

CloudWatch Events: Watches for specific events (like an EC2 instance starting or a new object being uploaded to S3) and can trigger actions automatically based on those events. For example, you can set it to automatically start a Lambda function, or even update a security group when a specific event happens.


Example Use Cases:
Automatically trigger a Lambda function when an object is uploaded to an S3 bucket.
Send a notification when an EC2 instance state changes (e.g., from running to stopped).
Schedule tasks to run on a fixed schedule (e.g., every day at 8 AM).

CloudWatch Dashboards are customizable, graphical views that display metrics and logs. These dashboards can be used to create real-time visualizations for monitoring AWS resources and applications.


CloudWatch Alarms:

Alarms allow users to set thresholds on CloudWatch metrics to receive notifications or trigger actions automatically. For example, you can set an alarm to send an email notification when CPU utilization exceeds 80% for a particular EC2 instance.
Alarms can also trigger actions such as auto-scaling an EC2 instance, invoking an AWS Lambda function, or publishing a message to an SNS topic.	


A Log Group is a container for log streams in Amazon CloudWatch Logs. It’s a way to organize and manage logs in a hierarchical structure. Each log group can contain multiple log streams, and each log stream holds the actual log events generated by AWS services or your applications.


Log Group: Think of it as a folder that holds your log files.
Log Stream: Inside each log group, you have individual logs (like files), each containing log entries (events).

Imagine you have an application running on multiple EC2 instances. You might have a log group named MyApp-Logs and create a log stream for each EC2 instance, like EC2-Instance-1 or EC2-Instance-2.
This allows you to easily organize the logs from different sources (such as different EC2 instances) into one logical group, but still keep them separate and manageable.

CloudWatch Logs Insights is a powerful query feature in Amazon CloudWatch that allows you to interactively explore and analyze the logs stored in your CloudWatch log groups. Using SQL-like queries, you can quickly search through large volumes of log data to identify patterns, troubleshoot issues, or gain operational insights.

You write queries to filter and aggregate the data from your logs. This can help you find specific events, errors, or trends within your logs.

Example:
Let’s say you have logs from an application that records HTTP requests, and you want to analyze errors (e.g., HTTP status code 500). You could run a query in CloudWatch Logs Insights to search for all logs where the status code equals 500, to find out when and why the errors happened.


Note : Cloud watch will not monitor the memory usage in the ec2 instance natively. we have to install the cloudwatch agent in the ec2 instance where we need to monitor the memory usage .

why memory usage is not monitored by cloudwatch by default like cpu and other resources


CPU: The hypervisor controls CPU scheduling and usage across all instances on the physical server, so it can easily track CPU usage.
Disk I/O and Network I/O: These are managed by the hypervisor because they involve shared hardware resources.
Memory Usage: Managed entirely by the operating system running on the EC2 instance, so the hypervisor cannot track it without OS-level access.

so memory usage can be track only if we have os level access. where cloudwatch cannot access the ec2 whithout any permission from the user level . so we have to manually instal the agent inside the ec2 instance to monitor the cloudwatch

The hypervisor in awss can directly mangages the cpu resources, but not the memory. once memory is allocted to the ec2 instace the memory level resources can be access by ec2 only. not by hypervisor. thats why it cannot be monitored we have to install the agent.

####################################################################
Cloud watch LAB :

While creating a instance we can choose the monitoring enabled with detailed monitoring . But it will cost

We can create a alarm by get into the cloudwatch module and choose alaram and choose the instance and choose the metrics that you want to set an alarm and entry the email id that you want the notification 

while true; do echo; done  -> this script will increase your cpu utilization 


standard monitoring - 5 minutes | detailed monitoring - 1 Min interval 

We can also create dashboard to see what is happening with your aws environment
###############################################################
Aws command line
******************

IF you loose secret acces key an access key we can go to the user go to security credentials and incactive  the old acessskey and create a newone


The secret key and access key is for IAM user. LIke you can use this by access the s3 bucket through the instance

"aws configure" and then enter the details, once done we can list our the objects in s3 buckte by "aws s3 ls" ( the buckets thats are in the region)

We can also create a bucket through command line:

aws s3 mb s3://"bucketname"   : mb is make bucket and also bucket name should be unique

Note : go to cd~ (home directory )and ls ( no files will be there ) but there is hidden directly .aws  in that your accesskey and secret key are stored ( it is security risk)  do not store your keys anywhere  .  To avoid these we can use roles which wil be learn further

#####################################################################
IAM roles : LAB 

Go to IAM -> Roles -> Create a role -> choose the service that should be using this role ( choose EC2 --> ec2 will be using this role) -> now need to attach policy (give admin access policy ) --> and mention rolename( give any name that you want to keep for this role) --> create role.  role created. 

Need to attach the created role to EC2 instance . Choose the ec2 instance and action -> attach/replace role -> choose the created role 


Service -> EC2   role -> Admin acess  .  So using any Ec2 instance we can access anything through command line . but we have to choose  which EC2 instance have to get full acess by folling below action ( IF there is any directory like .aws in home directory before delete it )

Choose the EC2 -> action -> instance settings > attach/replace IAM role -> and attach admin access role

Now go to ec2 instance and type aws s3 ls , it will list the bucket without giving key  ( without giving key we can acces it )

roles are univeral we can use it any region . While creating a EC2 instance itself we can give IAM role access

#################################################################
Using Boot strp scitpts - LAB   automation while  aws ec2 deployment , we can install software and update using commands and save in the box 

While creating instance - in congigure instance module there will be adavanced details box in that box we can give some commands like install update , install http , so that while instance are creating these commands will execute too and install whatever given in that 

#!/bin/bash
yum update -y
yum install httpd -y
service httpd start
chkconfig httpd on  ( will check if appache is stopped or not , if stopped it will start automatically )
cd /var/www/html
echo "<html><h1>hello</h1></html>" > index.html
aws s3 mb s3://bucketname 
aws s3 cp index.html s3://bucketname

and then create a instance once instance created dont need to go inside the ec2 instance we can directly type the ip in address bar it will open the webpage 

####################################################################################
Instance Meta data : LAb

Go to ec2 instance and we can get the details of commands that we run on the bootstrap  . The IP give below is general .

curl http://169.254.169.254/latest/user-data/  ->  While creating ec2 the given user data information can be acccessed here . It will show the "userdata" commands that we given while creating EC2 instance.
curl http://169.254.169.254/latest/meta-data/  ->  it will give all the option to which we want information ,like pubic-ipv4, private ip, etc . we can use below command for public ipv4
curl http://169.254.169.254/latest/meta-data/public-ipv4  -> it will give the details  ( so in meta deta all the information will stored)

##############################################################################
EFS - LAB  - Elastic file system - file storage system  . it is simular to EBS (where EBS can be mount only one istances , but we can share EFS volume to multiple system ) . In this storage capacity is elastic, growing and shrinking automatically as you add and remove files, so your application  have the storage they need, when they need it

EFS is within the region and for all the availablity zone

Here is the eg what we going to do . create a EFS  with default(everything)  and then create a 2 instances with bootstrap scripts below

#!/bin/bash
yum update -y
yum install httpd -y
service httpd start
chkconfig httpd on 
yum install -y amazon-efs-utils

and add them to security group which ever you uses like "web dmz" and launch the instance
Go to the security group and choose the default security group  one which we provisioned the EFS in that . so choose the default and remove the NFS which is created before and reenter the NFS and  in the custom field choose your existing security group "web-dmz" in which you created the ec2 instance 

Go to one server and go to cd /var/www/  and then go to EFS and choose the EFS that you created and click the mount instructions   and you will get the list of instruction in that choose the below one 

choose the encrytion command and paste this on the server - sudo mount -t efs -0 tls fs-9816b2269:/ /var/www/html  ( do the same in the next server )

So what ever changes made in html direct it will reflect in the other instance also.

###################################################################################
Amazon fsx for windows and amazon fsx for lustre
***********************************************

Amazon fsx for windows   --> nothing but windows file server (centralised storage for windows based and windows based app )

#A managed windows server that runs SMB based file services   SMB(server message block) but efs is not smb based it is NFS . So we cannot connect this on linux system

#It is designed for windows and windows application . Supports AD, ACL , and secutiry polices etc

If you're running Windows applications and need shared file access, you'd use FSx for Windows File Server.


FSx for Lustre: Imagine a movie studio working on a new film and needing to edit very large video files. The team needs fast access to those video files for things like rendering and special effects. FSx for Lustre can handle this scenario because it is optimized for high-speed data processing. The system allows the team to work with these massive files quickly, and because Lustre is a high-performance file system, it can handle the large number of read and write operations required during film production.

FSx for Windows File Server: Now imagine the same movie studio also needs to store and share some regular documents like scripts, contracts, and other smaller files. The team wants to access these documents from their Windows computers, using standard Windows file sharing. In this case, FSx for Windows File Server would be the right choice, as it supports Windows-style file sharing and integrates well with Active Directory for access control.

Summary of the Key Points:
FSx for Lustre is for fast, high-performance workloads like big data, scientific computing, or video rendering.
FSx for Windows File Server is for Windows-based workloads like file sharing, enterprise apps, or shared folders in Windows environments.
In short:

If you need speed and performance for heavy workloads, use FSx for Lustre.
If you need standard file sharing with Windows apps, use FSx for Windows File Server.

Lustre is a high-performance parallel file system commonly used in Linux-based environments, especially for applications like scientific computing, machine learning, and data analytics that require fast data access.
The Lustre protocol is not natively supported by Windows, so you cannot mount FSx for Lustre on a Windows server directly.
##################################################################
Ec2 placement groups   - 3 types   
*************************
Cluster placement :  Grouping of instances within a single availablity zone.  this group recommended for applications that need low netwrok latency and high network throughput or both . This group cannot be in mulitple availablity zone

Use cases : If these instances are close to each other so there will be low latency and high network throughput

Spread placement group :  Group of instances that are placed in different hardware.  where each instance are seprate from each hardware so that if one hardware failes the other one will work  ( small no of instances) , This group can be in mulitple availablity zone in same region
Only 7 running instances per availablity zone

Partition placement group : It is simalar to spread placement and it has partition group , it will seprate into 3 partiton group each group have multiple instances, where three partition group is isolated to each other. The name should be unique for placement group. this is used for large number of instances. and also  certain type of instances can be lanched in this like "compute optimzed, GPU, memory optimized and storage optimized  . This group can be in mulitple availablity zone in same region

You can also move an existing instance into a placement group but while moving it should be in stopped state . and also we can remove the instance from placement group by using aaws cli but not by console

#####################################################################################


AWS direct connect -> It will establish a dedicated network connection from your premises to aws, which will be private connectivity  and also it will reduce your network cost, increase badnwidth, provide more network consistency.  Through this we can transfer the data which is one type of data transfer like snowball, aws data sync 



Let’s walk through an example where your company, TechCo, wants to set up AWS Direct Connect:

TechCo’s Data Center Location: TechCo is based in San Francisco, and it wants to establish a private connection to AWS for low-latency access to AWS services.

Step 1: Choose a Direct Connect Location:

TechCo logs into the AWS Console and chooses San Francisco as the nearest Direct Connect location. AWS tells TechCo that the Direct Connect location is located at Equinix SF1 in San Francisco.
Step 2: Contact Network Provider:

TechCo contacts AT&T, a network provider that operates in San Francisco, and informs them that they want to establish a connection to AWS Direct Connect at Equinix SF1.
Step 3: Network Provider Provides Fiber Link:

AT&T runs a fiber optic cable from TechCo’s data center in San Francisco to Equinix SF1, where the AWS Direct Connect infrastructure is located.
Step 4: Set Up the Cross-Connect:

Once the fiber link is in place, AT&T configures a cross-connect to the AWS router inside Equinix SF1. This ensures that TechCo’s data center is physically connected to AWS’s network.
Step 5: Set Up BGP:

AT&T and TechCo configure BGP routing so that traffic can flow securely between TechCo’s on-premises network and AWS’s infrastructure.
Step 6: AWS Direct Connect is Active:

The connection is established, and TechCo can now send and receive data between its on-premises data center and AWS VPCs via the private, secure Direct Connect link.

##############################################################################################################################
Ec2 Instance type and its usages:


1. General Purpose Instances ---   c (2) m (0.5 – 2 GB)
These instances provide a balance of compute, memory, and networking resources, making them suitable for a variety of general-purpose applications.

Example Instances:
t3.micro, t3.small, t3.medium
Use case: Small websites, development environments, and microservices.
Storage: EBS only (Elastic Block Storage)


m5.large, m5.xlarge, m5.2xlarge  c (2-8)  m (8 - 32)
Use case: Databases, backend servers, and application servers.
vCPUs: 2–8
RAM: 8–32 GB
Storage: EBS only (Elastic Block Storage)


2. Compute Optimized Instances  c (2 -18)  (4 - 36)
These instances are designed for compute-intensive tasks that require high CPU performance, such as high-performance web servers, batch processing, and scientific modeling.

Example Instances:
c5.large, c5.xlarge, c5.2xlarge
Use case: High-performance web servers, batch processing, and compute-heavy applications.


3. Memory Optimized Instances  c (2–96) RAM: 16–768 GB
These instances are designed for memory-intensive tasks, such as high-performance databases, in-memory caches, and real-time big data analytics.

Example Instances:
r5.large, r5.xlarge, r5.2xlarge


x1e.2xlarge, x1e.4xlarge
Use case: In-memory databases, data analytics, and real-time big data processing.
vCPUs: 4–128
RAM: 122–3,904 GB



4. Storage Optimized Instances

These instances are designed for workloads that require high, sequential read and write access to large datasets, such as distributed file systems, data warehousing, and big data applications.

Example Instances:
i3.large, i3.xlarge, i3.2xlarge

Use case: High I/O performance for workloads like NoSQL databases and data warehousing.
vCPUs: 2–64
RAM: 16–512 GB
Storage: Local NVMe SSD (high IOPS)   --above instance type will have ebs , but here it is different. because it is especially for storage. 

d2.xlarge, d2.2xlarge

Use case: Big data storage, data lakes, and Hadoop workloads.
vCPUs: 4–24
RAM: 30–192 GB
Storage: HDD-based (high throughput)
Example:


5. GPU Instances
These instances are designed for graphics processing, machine learning inference and training, and 3D rendering.

Example Instances:
p3.2xlarge, p3.8xlarge

Use case: Machine learning, deep learning, and AI applications.
vCPUs: 8–32
RAM: 61–244 GB
Storage: EBS only (Elastic Block Storage)
GPU: NVIDIA Tesla V100 GPUs
g4dn.xlarge, g4dn.2xlarge

Use case: Machine learning inference, video transcoding, and game streaming.
vCPUs: 4–16
RAM: 16–64 GB
Storage: EBS only (Elastic Block Storage)
GPU: NVIDIA T4 Tensor Core GPUs
Example:
p3.2xlarge: Used for deep learning model training and AI applications that require heavy GPU computation, such as image classification and natural language processing (NLP).


COMPARING CPU AND GPU 

Comparing CPU and GPU Task Handling:
CPU:

Can handle a few tasks at a time (based on how many cores and threads it has).
Switches quickly between tasks (context switching).
Best for complex, sequential tasks or tasks that require high-level decision-making.

GPU:
Can handle many tasks simultaneously due to its large number of smaller cores.
Best for parallel tasks (like rendering, video processing, and machine learning).

In Summary:
While the CPU does have multiple threads and cores, it typically processes tasks in a sequential manner (one after the other), switching between tasks quickly. This makes the CPU great for handling complex, sequential logic and tasks that require high flexibility and decision-making.

However, when it comes to parallel tasks (like processing large amounts of data at once), the GPU is far more efficient because it has many more cores that can work on different tasks simultaneously.





6. High Performance Computing (HPC) Instances
These instances are designed for scientific computing, modeling, and simulations that require massive computational power and very high memory bandwidth.

Example Instances:
hpc6id.32xlarge
Use case: HPC workloads like genomic sequencing, engineering simulations, and financial modeling.
vCPUs: 128
RAM: 512 GB
Storage: EBS only (Elastic Block Storage)
Example:
hpc6id.32xlarge: Ideal for workloads like genomic research, climate modeling, and high-performance simulations that require very high processing power and memory.

7. Bare Metal Instances
These provide direct access to physical hardware and are useful for workloads that need to run on physical servers or that require specialized hardware configurations.

Example Instances:
i3.metal, m5.metal
Use case: High-performance databases, HPC, and customized workloads.
vCPUs: Up to 96
RAM: Up to 384 GB
Storage: Local NVMe SSD (high IOPS)



#####################################################################

AWS WAF : web application firewall like which IP addrss can allow, which string can allow the req  and which region can allow 

AWS waf allows 3 different behaviours:

Allow all requests except the one you specifty 2) block all request except the ones you specify 
3) count the requests that match the properties you specify

Exrra protection against web attacks  using conditions you specify . You can define conditions by below

IP address that requests comes from
Country that requests are comimg from
values in request headers
String that appear in request
Lenght of request
presence of sql code that is likely to be malicious ( known as sql injection)

SO above are the things were WAF can allow or denied the request


Mostly WAF is used of DDOS attac
WAF can be set before cloudfront or LB .  From Route 53 normally the request will reach to cloud front or LB . In this we going to place the WAF in cloud front.  While creating cloudfront we can mention the WAF , or also we can create a WAF seperately and attach with cloud front.

there will be default amazons rule, where we can add them to WAF, or we can create a set of custom rule by ourself and add them to waf.
If we have to block or allow specifc ip in particular region. Then go to IPSET and choose the region and enter the IP list and choose allow or deny
once the IPSET is created , go to rule and create a rule and in that rule choose ipset that we created . and add that rule in wAF
IN the rule we can choose IPSET , Headers and extra protection whatever we nneeded.


The request first reaches CloudFront, and then CloudFront forwards the request to AWS WAF for inspection.
AWS WAF evaluates the request based on configured rules.
If the request passes WAF’s inspection, it continues to your application.
If the request is blocked by WAF (based on your rules), it does not reach your application, and the user sees the blocked response.

#############################################################################################

Database:   Relational database

6types : sql,mysql,postgresql , oracle, mariadb,Aurora(amazaon owned)

RDS has two key features

# Multi-AZ  : Its for disaster recovery  (failover)

IF primary db failes it will automaticall point to secondary database . Db has a Domain URL  which we can used to connect the app to the db , The URL points to both primary and secondary . IF primary failes the secondary will be online and automatically point to that URL ( LIKE LB)

#Read replicas : For performance  (sharing load)

Its nothing the primary DB will replicate the datas to secondary DB but it is read only . IF the primary DB gets no of requests and it will share the load to secondary DB .  In this the URL points only to primary DB , IF the primary DB failed then it will not automaticlly switch to other (failover is not done here ) so in that case we have to create a new connection (URL) and point the app to secondary db one which is read only .  we can use this case in blogs because most of these are used for read . we can have maximum 5 replica copies 

Dynamo DB : No sql ( amazon properity)

RED shift : Its database warehousing - for eg: in this query will be more complicated and more querys are run on this.  its amazon properity

Elastic cache : It will cache the data so that next time it will not go to database for the information it will use the cache



####################################################################
Creating an RDS Instance :



we cannot able to ssh RDS like we do ec2 . And also we cant able to patch RDS its amazaon responsiblity to patch the os and DB . Rds is runs on virtual machine , its not serverless . But we canat able to access it but amazon can .  Aurora alone serverless in RDS group. All others are server(virutual machine)


#####################################################################
Backup , mutliaz, read relplicas -  RDS

Automated backups  - It will take the back up and put it on s3 bucket with the timings we mentioned and also while doing this automated backups it will suspend the storage iops so that we will have some latency 

Database snapshot : Its taken by us manually . Even if we delete thE rds the snapshot will be available unlike automated backup


If we delete RDs and trying to restore it through backup, we will get a newly created RDS with different endpoint not as same the previous one. but datas all be present as pervious one . In this case we have to apply the new endpoint in ec2 server by removing the old

Multi-AZ : In this we have database which will replicate all the things to another DB in different region . SO if the main db fails or the region failes, it will be automatically connect to the standby DB (other one ) without any admin intervention . the replication will be done by amazon .Multi-az is only for disaster recovery it cannot be used as primary . 
To performance improvement you need read replicas .


In exam they ask how will you improve the performace - > WE can add read replicas which wil allow you to have a read-only copy of your production database and which will also share the workload . We can have mulitple read replicas server connect with one production server which will share the workload ,but in the case of multi-az it is used for onlyh DR. you can force a failover from one az to aonother by rebooting the RDS instance.


Things to know about read replicas :

It is used for scaling, not for DR 
Must have automatic backups turned on in ourder to deploy a read replica, and we can have upto 5 read replicas of any database
We can have read replicas of read replicas
Each read replica wil have its Own DNS end point, read replicas can be promoted as own database( seperate database) so that it breaks the replications with primary one. And also we have read replica in second region

############################################################################################
RDS - Backups, multi-az and read replicas - LAB

Multi-AZ : i To enable the Mutli-az in previous created database , we can choose the database and click modify and tick mark the multi-az and apply
If you do this it will  create a another database and replicate the content. while doing this we will face performace issue while replicating the datas, so we can do this on sechudeled timings, which we can also set the timings for this. 

Once it is created, We have options like , if we have to rebbot the db we have a option called reboot with failover, this means while rebooting it will switch to secondary databse. ( its changing one availablity zone to another AZ )

Read replica : In oder to enable read replica we have to turn on the back up, without backup turn on readrelipica option will be greyout .  

To enable backup , click the databse and click modfiy and turn on the back up  ( It will also affect the performace impact )
Once done click action and click create read replica. and choose the destination region. So it will create a another db in the name of replica.

Once it is done , click the replica DB and click action and click promote read replica (this is make this replica to master )  -> once we done this, we can create replica for firstly created replica.
 
Note : In multi az it is not list the DB that we created, but in the read replicat it  listed the Db . Check in details about this

Mutli AZ is used for DR, You can force a failover from one AZ to aonother by rebooting the RDS instance.  ( Got that point )

#################################################################################
Dynamo DB - > Its apposite of RDS, its not amazon 

Stored on ssd storage

Spread across 3 geographically distinct data centers

Eventual consistent read : All the copies of data is usually reached within a second from the application , But it will take a few seconds to return the updated data ( Best read performce) it is default

Strong consistent read : Once the data is updated in db it can be accessible withing a second or less. 

Set up wil cover is serverless section (upcoming sessions videos)
`				
########################################################################
Aurora - Aws database

# 2 copies of your data are contained in each availablity zone, with minimum of 3 availablity zones.  so totally 6 copies of your data
# You care share Aurora snapshots with other AWS accounts
# 3 types of replicas available , Aurora replicas ( we can have upto 15 replicas), read replicas ( 5) and postgresql replicas (1) .Automated failover is only available with Aurora replicas , We can also take snaphots, which will not impact the performace like others
# Aurora has automated backups turned on by default. And also it is serverless database and if you want a simple, cost-effection option for infrequent , intermittent, or unpredictable workloads


aurora replicas -- especially designed for aurora db.
read replicas --> it is designed for rds and also can be used in aurora. but best is aurora 

aurora replicas is best for using in auraro db , it can have upto `15 replicas and which will do auto failover but this is not in read replicas. 

aurora has two types -- one is MySQL compactible and other one is postrgress compactible. we have to choose any one. we can also do cross-region replication on the aurora. 


aurora has maximum storage of 128 tb and others has only 64tb. it will automatically scales the resources based on the load. if the load is high it will extedn the ram to max size if it is low it will automatically comes to low ram. aurora is serverless. 



Migrating Msql to Aurora

Click on Action and choose  aurora read replica.  We migrating using read replica concept. once aurora read repllica created we can make this as a master ( your ec2 will be connected from mysql to aurora replica) by clicking "promote read replica" option 

##############################################################
Elasticache -  it is used to increase database and web application performace

This services improves the performance of web applications by allowing to retrieve information from in-memory caches instead of relying entirely on databse . which wil be fast in performace 

In exam if they ask what step will you take if your databse is overload -  1) create read replica and point your reads to read replica and other one is using elasticache

#####################################################################################
Database Migration service (DMS ) :

Aws DMS (used for migration which will convert the database like msql to auroro etc..) is a server in the AWS cloud that runs replication software .we can specify the source and target connection to tell AWS DMS to migrate the database. it will also use SCT (schema convertion tool for migration )

AWS DMS is nothing but which will convert the database( migration ) from source to target . it will convert the datas and replicate them to target.

Diffent types of migration :

Homogenous migration  : From oracle -> oracle  from onpermises to aws  ( we dont need SCT because we migration oracle to oracle )
Hetrogenous migration : from sql server -> aurora   , it will use DMS , SCT ( schema convertion tool ) ( we need SCT because source and target DBs are different, and we are converting )


steps to do db migration

 1) Replication instance :   first we have to create a replication instance. which is used for replicating the data from source and target.. this is also like creating an ec2 instance. which is used for replication task . 
Give a name for the replication --> choose the configuration for the instance --> choose the dms version (this is wat will be installed in instance and that going to do the replication task ) --> vpc -> subnet --> security group --> and then click create replication instance.

2) ENDPOINT:  create a source endpoint and choose the engine "MySQL" whichis source db --> provide access information (credentials) for that db (for accessing like we have to give the source endpoint name , port number, username and password like that) --> select the replication instance (in above we created on replication instance , select the replication instance) -- run test ( it will check the connection between the source endpoint and replication instance )-> once the test is passed and then click create endpoint .----> till now source enpoint is created .   SAME as we have to create a target endpoint with the target details that we have . 

so now both source endpoint and target endpoint is created. 


3) DATABASE MIGRATON TASK:   create a name --> choose the replication instance --> choose the source and target endpoint which is created above --> choose the migration type --> turn on the data validation --> and then create task 

once done it will start the migration .




Migration type  : 3 types  " migrate existing data" & " migrate existing data and replicate  ongoing change" & "replicate data changes only"

migrate existing data : it will copy all the data from source to destination, but not the ongoing data that comes inside the db newly .
migrate exisiting data and replicate ongoing change : this will copy all the data and also ongoing data that comes inside the db will copied to the target db continuously 
replicate data changes only: it will replicate the new data only , (already existing data copy is done  and if we have to replicate the newly copy things we can choose this ) 

we can validate the task , we enabled while setting up the migration task . so we can validate how many records passed and any failure like that. 


##################################################################################################

RDS , Backup and restore:

While creating the RDS , we can choose the automatic backup option for a specific time, where it will take the back up on that time . there is a chance of taking backup with different time which we didn't mention , because some time the load will be very high so that it will postpond the back up time and will take it on the time when load is low.  When we enabled the automatic backup option, we will have a additionaly PITR (point in time restore)option enabled with that. 
What is point in time restore point?
lets say we set up back up on morning 6am on each day, and automatic backup will take the backup and the specific time as mentioned. but our server crashed on 10am morning and we have lost 4 hours of data.  but that is not the case just choose our RDS DB and click action and there will be option called point in time restore. choose that option and it will show options 1" restore to latest restorepoint 2" restore to custom time . in the first option we can see below there will be time of the latest restore point . in our case it will be 9.55am  so there is only 5 mins of data only not available. so we can get the latest restore point in this option and we can restore it . this will automatically enabled when we enabled the automatic back. 

Restore means it will not be restored in the same db where it failed. It will create a another copy of db and then it will restore the data. so we will get a new db and we have to point our application to our new db endpoint. while restoreing itself we can choose different instance type and other option for creating the new db.  all about is restore is nothing but creating the newdb with our old data . it will not be restored in the same db where its failed.

so lets say our application is pointing to db1 which is failed and we created a db2 with the restore option. so for db2 there will be different endpoint. so one option we can change the end pint in the applicaton level. or just the change the db1 -> db1_old and change the db2 -> db1  once you change the name of the db , the endpoint wil also be changed as same as db1. mostly endpoint will be starting in dbname_somevalue.  the somevalue will not change it will be same for all the db. so if we change the dbname alone it will be enough so that we can get our db1 endpoint to db2. and no need to change that in the application end. 






##############################################################################################
RDS  (2024)   2years back video

why should we use ec2 for running RdS or database

Access to the os where the db is installed ( in rds we dont have access to the base system like where the db is placed)
Advance db tuning (DBROOT) which cannot be done in  rds
DB or DB version that AWS dont provide (mostly it will provide all the things , in case)

Why should we use rds insted of ec2 database

admin kind of things will be completly take care by aws( managed service)
Backup and DR 
Ec2 is running in single AZ (in ec2 database but not in rds , rds have multiple AZ)
performance will be slow in ec2 compare with rds


We can use 40 dbinstance in rds. by default

CREATING RDS:

before creating rds, let us create a subnet group (group of subnet) . create a subnet group give it a name, choose the availability zone where your subnets are placed. and once availability zone choosed, choose the subnect that you need to group ( say you already have 5 subnets, you can choose 3 on that or more to make a group) once you choosed the subnets then create it . now subnet group is created. 

Now create a rds   -- ( MySQL, MariaDB ,PostgreSQL, oracle, Microsoft sql, aurora ( its aws propriety ) 

Database can use the instance db.m5genral ( for genral ) db.r5 memory ( for memory ) db.t3 burst

DB CREATE :

go to rds, create rds  -> choose standard  --> choose MariaDB (anyting you want which i gave above ) -- version of maria db -- environment (prod, dev,free-tier) --> Database instance identifier (name of the database)  -- credentials for db (user, password) -- instance configuration (choose db.t3.burst) -->storage (databse use ebs volume so choose gp2) --> storage size (20 -min 6144 - max )  -- multi az  --> connect to ec2 instance or not (if you want we can connect it later or we can connect it will creating) -- IPV4 -- choose the vpc --> choose the subnet grup -- choose the security group -- choose the az where your db should place -- db port --  authencticaton ( we can use the authentication method  using database password (or) IAM database authentication )-- automatica back and rention setting -- enable deletion protection .

Once everything is created we will have endpoint of our rds db.

to connect with our dabase 

MySQL -h endpoint-of-our-db -P 3306 -u username -p   enter it will ask for password and enter that it will login to your database

RDS BACKUP AND RESTORE :

RPO -- recovery point objective

lets say we have scheduled the backup of every 6am, and if the failure happens at 7am, then the recovery point objective will be 1 hour.  so 1 hour data will not be available (these things can be given in SLA)

RTO - recovery time objective

time that taken for DB to recover completely ( like the time between the db fails and the db got recovered)

RDS backup will be incremental . and manual snapthot that we taken for our db will be deleted manually , it wil not done by automatic ( like we setting retention period in db backup while creating the db)

we can delete the db , they can be retained but they will expire based on the retention period. 






#########################################################
Caching serives :

cloud front, api gateway, elastic cache , Dynamo DB accelerator (DAX )
############################################################
EMR : Elastic MAP reduce (cluster)  ----nott need

It is used for big data processing, consists of a master node, a core node, and optionally a task node. By default log data is stored on the master node ( in case if master node gone all the logs will be gone in that case ), you can configure replication to s3 on 5 min intervals for all log data from the masternode ( copying logs from master to s3) however this can be only configured when creating the cluster for the first time.

##############################################################################################
AWS directory service :
there are two types, one is AD Connector and other one is "AWS Managed Microsoft AD".  these are only ad services, but dosent mean this is used for login the aws console, but with the help of this service we can use the sso and then we can able to login with our aws. combination of both only we can able to logi with aws console. 
Use AWS Managed Microsoft AD or AD Connector to integrate your on-premises Active Directory with AWS.


AD Connector:

AD Connector allows AWS resources to authenticate directly against your on-premises Active Directory without storing user credentials in AWS. It works as a bridge between AWS and your existing on-premises AD, enabling hybrid authentication.


AWS Managed Microsoft AD

This is the AD service for aws, where we can create this service and sync our on-premises AD with this , or else we can use this separately in your aws. while creation itself we can configure trust relationship with our onpremises ad, so that on-premises users can access the aws services using their credentials. this doesn't mean all the userdata wil be sync to aws, this process is just for authentication . when on-premsis user trying to access aws services it will be authenticated in the on-premisses and then that user can access in the aws, so this is trust relation between on-prem and aws. 
we have to add the services in the aws ad, so then only the services can be accessed by on-prem users. Genrally any user its belongs to on-prem or the aws ad, if want to access any services ,that services should be enabled in the AWS managed Microsoft ad. 

AWS Managed Microsoft AD, the AD environment is fully managed and hosted in AWS, but it doesn't mean that it migrates your entire on-premises Active Directory to AWS. Instead, it creates a new, separate Active Directory instance in AWS with our on-premisis data.  we can set one-way setup or 2 way setup between microsft ad <--> on-prem ad.  if we need bothway we can set 2 way communication 	
WE can also use advanced AD features like domain-joining EC2 instances or group policies.



Step 1: Set Up AWS Directory Service
You need to set up AWS Directory Service to connect your AWS environment to your Active Directory.

Option 1: AWS Managed Microsoft AD (Recommended for fully managed AD in AWS)
Log in to the AWS Management Console and go to the Directory Service dashboard.
Choose Set up directory and select AWS Managed Microsoft AD.
Provide Directory Details:
Directory DNS name: Choose a name, e.g., corp.example.com.
Directory NetBIOS name: Shorter name for your AD (e.g., CORP).
Admin password: Create a password for the AD administrator.
VPC Configuration:
Select the VPC where your directory will be deployed (make sure the VPC has the necessary network configurations for AD to work).
Review and Create: Review the configuration and click Create directory.

ONCE DONE, we have to enable trust relation ship , thenonly on-prem users can be authenticated. 


Option 2: AD Connector (If you want to use your on-premises AD)
In the Directory Service dashboard, click Set up directory and choose AD Connector.
Provide Directory Details:
Directory name: Choose a name for the directory.
Directory type: Select the type (e.g., Active Directory).
On-premises AD details: Enter the on-premises AD details like DNS IPs, NetBIOS name, and AD administrator credentials.
VPC Configuration: Select the VPC and subnets where the AD Connector should be set up.
Review and Create: Review and create the directory.


NOTE:  The on-premises AD is the source of authentication (since AD Connector is essentially a bridge between AWS and on-premises AD).

all the authentication is done in on-prem only.

*****************************************************************************************************
Solution for Console Access (AWS SSO or SAML Federation):

 To log in to the AWS Management Console using your Active Directory credentials, you would need to set up AWS Single Sign-On (SSO) or use SAML (Security Assertion Markup Language) federation.

AWS Single Sign-On (SSO) can integrate with AD Connector or AWS Managed Microsoft AD, allowing users to log in to the AWS Console using their Active Directory credentials.

SAML Federation allows you to configure a SAML 2.0 connection between your on-premises AD and AWS, which would allow users to log into the AWS Management Console using their AD credentials.

Step-by-Step Solution (Using AWS SSO):
Set up AWS SSO in the AWS Management Console.
Choose Active Directory as the identity source (this can be AWS Managed Microsoft AD or AD Connector).
Assign IAM roles to the users/groups from your Active Directory.
Enable Console access and provide users with access to specific AWS resources.


REAL TIME CHECKED FOR SSO SIGN IN FOR AWS CONSOLE> 

Enable AWS IAM Identity Center  -- go and enable it IAM IDENTITY CENTER
Connect Your Active Directory ---  we have to choose our AD on-prem   ( which is already connected using aws managed AD or ad connector , so it will list)
Create an IAM Role for SSO
Assign Permissions to AD Users
And then we can access Access AWS Management Console



SYNC BETWEEN ON_PREM AND AWS (if new usercreated in ad, it will sync to aws )

To synchronize your on-premises Active Directory (AD) with AWS Managed Microsoft AD, you can use the AD Connector or AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) with AD Replication. Here’s how you can set it up:

Using AWS Managed Microsoft AD with AD Replication
Set Up Trust Relationship:
Ensure you have a trust relationship established between your on-premises AD and AWS Managed Microsoft AD. This allows the two directories to communicate and share information.

Enable AD Replication:
AWS Managed Microsoft AD supports multi-region replication. You can configure your on-premises AD to replicate with AWS Managed Microsoft AD.


Synchronize Your On-Prem AD with AWS:

You can use AWS Directory Service for Microsoft AD Migration tools, or third-party tools, like Active Directory Migration Tool (ADMT) to sync your on-prem AD with AWS Managed AD.
ADMT helps with the migration of users, groups, and other AD objects to the AWS AD.


AD connector is just doing proxy for authentication , but micorsoft ad is a activedirectory service which is managed by aws, we just doing trust relationship for authenticating our on-prem user for accessing aws services . 


#############################################################
IAM policies :

Amazon resource NAME ( ARN )

Arn all begin with :  arn:partition:service:region:account_id
And end with :
Resource
Resource_type/resoure
resource_type/resource/qualifier
resource_type/resource:qualifier
resource_type:resource
resource_type:resource:qualifier

Example :

arn:aws:iam::12345678:user/mark    -> resource type - user ; resource - mark (since its IAM we are omiting the region because its glbl servce)
arn:aws:s3:::my_awesome_bucket/image.png ( we omiting the account id here because fr s3 no account id only unique name)
arn:aws:dynamodb:us-east-1:123455444:table/orders
awn:aws:ec2:us-east-1:1231423:instance/* ( all ec2 instance in that region )

A policy is a json doccument , we have two types
Identity policy  : permissions for users, groups  
Resource policy  : permission for resources like s3 bucket , kms, . We can specify who has acess the resource and what action they peform on it

Policy structure : its a list of statement

each statement will call API request, for eg ( if we create a statment to create a ec2 instance it will complete through api request)

sid : it is used to tell what the statement is for ( for wat we created this policy)
Effect : allow or deny
Action : service:action name  
resource : allows/dny  all the action on the particular resource (table) (mention the resouce which we need to do the action )>

IAM POLICY is a JSON doccument that defines permissions. 

Two types of policy :
Aws managed policy and customer managed policy . Aws managed policy is default created by aws which will be in orange box icon which we canot edit .

JSON script for (listing object in s3 bucket and allowing to put , get ,delete object on s3 bucket in specific folder)

{
 "version": "2012-10-12",
 "statement":[
   {
     "Effect": "Allow",
	 "Action": ["s3:ListBucket"],
	 "Resource" ["arn:aws:s3:::test"]    -here test is bucket name
   },
   { 
     "Effect": "Allow",
	 "Action":[
	   "s3:PutObject",
	   "s3:GetObject",
	   "s3:DeleteObject"
	  ],
	  "Resource": ["arn:aws:s3:::test/*"]    - It means we can put, delet and create object inside the test s3 bucket
   }  
  ]	 
 }

Once we created the policy , we have to attach to the roles, create a new role, we want to allow ec2 istance access to s3 bucket with new created polices, choose the aws service , ec2 choose, select permisstion ,and choose the policy that we created. 

If we allow to access s3 bucket in policiy , and create another policy to deniy s3 bucket, it will choose the deny.z

Permission boundary :   If an IAM user has full administrator acess policy , and we want him to have only full access for dynomodb alone then we can use permission boundary option and choose the dynomodb acess alone . eventhough he has admin acess policy we mentioned to allow only dynamodb acess in permison bounday . So no other acess he can have

permission boundary can use for delegating the permission. even the iam policy has the control to delete the ec2 instances, if permission boundary set not to terminate the ec2 instance. it will take priority what permission boundary says. it it the maximum control what we can give in this  permission boundary. 

#####################################################################################################

AWS resources access manager (RAM):  https://www.youtube.com/watch?v=Oo1C0pDrDxI

We can share resources from one account to another account , like we can share aruora db  frome one account to another account by using RAM,

Go to RAM console and create a resource share , and choose the resourse type and choose the DB name and give the another account id to share 
Once done we have to go to account to and accept the invite and then we can see the DB is shared . from that we can clone the db and we can use.


IF the two accountts are linked with organisation then invite and accept will not ask, it willl directly share


TRANSIST GATEWAY:

using transist gateway we can attach multiple vpc (that are in same region) . First create a transitgateway, once created go to transitgateway attachment and attach the VPC that you need to group. lets say we have attached VPC1 and VPC2 that are in the same region to the transitgateway. So that both VPC can be communicate with each other , but we have to route the traffic between them otherwise it will not work. Go to router VPC1 and add route like "12.0.0.0/16(vpc2 cidr) connect with transitgatewayid "  so from VPC1 any traffic that related to 12.0.0.0/16 that want to reach VPC2 via transitgateway. by this we can connect many vpc using routetable we can connect them each other.  

Note: using the transitgateway we can connect the different region vpc. For that we have to use transitgateway perring. we have to peer the transfitgateway of the two regions and then we can connect them. the other way of connecting the vpc from different region we can use VPC peering.  and also we can use vpc endpoint to access particular resources from other VPC using private link . 

so in RAM , we going to share this transit gateway to other account . go to RAM and create resource share --> in that choose the transitgateway (we do have toher services like subnet, securitygroup , these things also we can share using RAM) -->choose the account that we need to share this resources. once done go to the other account and accepet the invitation that comes from the account1. so that transitgateway is now shared with different account using RAM. 





###########################################################################################

DNS - Route 53

SOA , NS records, A records, cnames , mx records (mail ), PtR records (reverse of A records domain name -> ip )

NS - Name server records : They are used by top level domain servers to direct traffic to the DNS server which contains the authoritative dns record :

hellogurus2019.com  -> .com (top level) which will check the ns record for this domain and transfer the requested to ns records -> NS records -> SOA (start of authority) which has all the IPS 
   
A record -> stands for address  -> Name to Ip address   (google.com -> 123.23.23.23)
Cname -> Canonical name -> which can be used as another domain name . one IP has two domain name(two url)

Elb do not have pre-defined ipv4 address, you resolve them using a dns name

We can also buy domain name directly with AWS . itt will take upto 3 days to reflect . mostly will be done within a hour or 2 hour.

An alias record is a Route 53-specific DNS record that allows you to route traffic directly to an AWS resource, such as an ELB, CloudFront distribution, or S3 bucket.

@@@@@@@@@@@@@@@@@@@@@@@

If we purchase the domain in google and configured our dns details in route53 . below are the steps that are followed.

Root --> TLD -->authority server --> Nameserver

ONce we purchased the domain in google it will assign the ns server for our domain. so normally when the user hits www.example.com  the root will check whether its .com or .org or .net (all these are TLD) and if it is .com then it will forward the request to ".com" server (tld) .the TLD will check the example.com records, and and it will say that this domain is registerd by google domain so request will be forward to "google authorative name servers" and then google check the dns setting and it will forward the request to the nameserver which is listed in google settings.

in our case we configured the dns details in route53. so our records are all in route53 and google dont have details about our records, so what we have to do is once we configured the records in the rout53 , it will give the nameservers, copy that nameserver and go to google domain and edit the dns settings and paste our route53 name servers. 

so once the request comes to google authorative name server , it will check the dns settings and it will forward the request to route53 because we have the route53 name servers in the settings. and the request will go there and it will return the IP .


godaddy /google anything

User Request → Root DNS Servers → TLD Name Servers (.com, .org, etc.)
TLD Name Servers → GoDaddy (Registrar’s Authoritative DNS Servers)
GoDaddy → Amazon Route 53 (since you configured Route 53’s nameservers in GoDaddy)
Amazon Route 53 → Returns DNS records (e.g., IP address for your domain)
User Resolver → Connects to the IP address provided by Route 53.


go to route53, create hosted zone, eneter the name(mydns) and then it will ask whether it is for public hosted zone  or privatezone. choose the public  and then create it , now our route53 created and it will create NS record and SOA record by default.  Now we have to create A record inside that. so click "mydns" and go inside and we have ns record and soa record inside it and choose create record --> and choose the routing policy and then it will ask the details about our records and enter the details. now records are created. 



##################################################
Routing polices available on  AWS

simple routing , weighted routing , latency-based routing , failover routing, Geolocation routing, Geopromixity routing (traffic flow only),multivalue answer routing 

Simple routing policy :

In simple routing policy we can have only one record with mulitple IP address

We have to create 3 instance with different region (nv , ohio, etc) with http enable with site named with region name to easily understand

#!/bin/bash
yum update -y
yum install httpd -y
chkconfig httpd on
service httpd start
cd /var/www/html
echo "<html><h1>Hello Cloud Gurus! This is the X Web Server</h1></html>" > index.html

in x we can mention the region name for each instance . so that we can find which region the page is accessig from

Once done with instance creation and domain creation in aws . choose our domain that created and click option " create record sets " and in side column enter the 3 ip address in value fileds . so once done we can use the domain name to check the site whether the site is opening and from whch region it is accessing . once the site is accessed from ohio it will show "Hello Cloud Gurus! This is the Ohio Web Server" even if we refresh the page it will show the same content because the browser cached the content . so we can go and change the TTL value to 1min then it will remove the cache for 1min and then we can get the result from three diff region . the ttl value can change in same filed where we entereed the 3 ips value to store in dns . 
To reflect the changes immedietly we can do flus dns instead of waiting for ttl for clearing cache in dns.




Weighted routing policy: split up the traffic to differen region
***************************
We can set weight to each ip address , like 30 % of traffic will goes to this ip adreess and 50% of traffic goes to this ip address like that.

First we can delete the old record sets and create a new one by giving ip address one by one , and setting the routing poicy to wighted and giving the weitage value . so we have to create 3 different records for each ip address and give them a weight value. We can give "set id" value as region name like sydney

We can also set health check for each IP address , go to health check column and create a health check and enter the details . create 3 healthcheck for 3 IP address . IF one fails it will automatically removes from route53 untill it healthcheck  pass. we have to check box the healthcheck while creating the records to apply this option.we can also get healthcheck notification.

Name - give region name like sydney, ohio
IP  - enter the sydney IP 
Hostname - Domain name (Url)  hellocloudguru2019.com
path - Index.html




the weightage can be given in 0 - 255 . if we have to give 50% of traffic means we have to give 128. 

latency-based routing
********************

It  will send the query to near by region and get the output . If we are in australia then it will send the request the to sydney and get the result. so whichever the site is near it will get the output from that.

remove the previous records . create a new record for each IP . and change the policy to latency-based, region will take automaticcally, set id to region name , and enable the healthcheck if needed .

failover routing
*******************
 Its nothing but if one region failes the request will go to other region . it will automatically point the request to secondary region .
 
 remove the old record. create 2 ip records one by one . by setting the policy to failover and also set one ip to primary and other ip to secondary . and enable health check. and set the ttl to 1 min . now turn off the one instance (primary) then check it will automatically movess to secondary ip and get the request from that site.
 
 Geolocation routing 
 *********************
 
 We can choose where our traffic can be sent based on geographic location. for eg : if europen customers want to access the europe based region site we can set it up in that way ( for eg : europe currencyy value is different so we can set it up in that way) . SO here we taking two IP and create a record for each one , setting policy to geolocation , and set id to region, and from which location the request can be comme from (europe ). so erupe people while accessinng the site the reqquest will go to europe server ( These servers may have the locallanguage of europen customers in that way we can set it up ) 


in the geolocation routing policy we can configure german to point to one loadbalancer and finland to point to another loadbalance. so if the ip comes from german it will go to that loadbalancer (where we hosted a seprate application based on the location customized) .

if we configure the geo location for two regions, and if the request comes from other region then how it will react ?

there is an option called default record in geo location policy. we have to configure that default record where it have to point.  so other than 2 regions if we get request then it will forward to the default one.  If the default is not set then your request will  be failed. 
 
 Geopromixity routing
 *****************
 
 in this we can set the traffic rules deeply and widely. This will come in advance . To use thie routing policy we must use route53 traffic flow
 
 Multivalue Answer policy:
 ***************************
 It is same as simple routimg policy , in simple routing policy we will add all the 3 ips in single record without health check , but in multivalue we can add each IP in different record as like other policy and in this we can have health check too where we dont have healthcheck in simple policy . So if one ip fails it will automatically failover to other using healthcheck

########################################################################################################################
ACM  -- aws certificate manager

lets thinks we have a complete setup of our application where route53 points to loadbalancer and then it points to our app.  Normally in loadbalancer we will create a targetgroup and attach our instance into it to server the traffic. and in the load balancer we have to put the listener like which port the request will handle. normally we add http 80. so that if the request comes with port 80 then the loadbalancer will validate the request according to the settings and forward it to the application . in the listner only we will attach the target group (where our instance added ).
Now we have to make our application with secure, so that we going to create a certificate in ACM ow

GO to acm and create certificate. and enter our FQDN (www.example.com) 	the validation can be done through dns( we have two option one is dns and other one is email validation) we can choose the dns validation. dns validation is nothing but once we created the certificate, it will be in pending state. ACM will provide us one DNS record, we have to push that record to our Route 53 record. once done the pending state will be cleared. in this way the ACM will understand you are the owner of that domain and you are validated by saving that record inside the route53. so this is only for validation purpose. we dont have to manually copy the record and paste in our route53 record. in our ACM itself it will show whether we can push this record to the route53 . we we give yes and create record. it will push the record to our route53 directly. how it will know our where it should push the record ?  (because we have mentioned in starting while creating the ACM  we have entered the domain name . so using that it will find out where to push . It will add the record in CNAME type. 

Now certifacte is created and now we have to attach that to our loadbalancer. previously we created a listner with 80  http. and attached our target group. now we have to add one more listner 443 https, and attach the target group (same which we used for 80) . and it will ask for certificate -- choose it from ACM and it will list our certificate  and we can choose that. (dont forget to add 443 in the security group)

so now we have both the listenr for 80 and 443 . when we run the website with http it will work and when we run the website https it will also work. but our site should not open in http because it is not secure. we cant remove that because someone will mistakenly run the website in http on that time it will show error. now what we can do that

go to listner and choose the 80 listener and edit it . and in the 80 lister we said to forward the request to target group (and we attached the target group) now we have to choose the option redirect the url instead of choosing the forward to target group. now choose the option redirect the url and enter https to that ( we can also enter the full website https://www.example.com) . and save it . so now anybody who run the website in http it will automatically moves to https and shows the website. 


 
 ##########################################################################################################################
 
 VPC :  Virtual private network 
 
 * We can easily customize the network configuration , for eg : we can create a public facing subnet for your webservers that has acess to the internet, and place your backend servers such as databases or application servers in private facing subnet with no internet access.
 
 We can create our own subnets and launch instances withing that and assign custom ip address ranges in each subnet
 We can create a internet gateway and attach it to our VPC 
 We can have much security control over our aws resources 
 
 There is default VPC and custom VPC
 
 Default VPC is user friendly allowing us to immediately deploy instances,  and all subnets in default VPC have a route out to the internet
 Each EC2 instance has both a public and private IP address
 
 
 VPC Peering:
 
*Allows you to connect one VPC with another Via a direct network route using private IP address
*iNSTANCE behae as if they were on the same private network
* It consists of Route tables, network access control list (stateless), subnets and security groups and also VIrtual private gateway
*You can peer VPCs with other aws accounts as well as with other VPC in the same account. Peering is star configuration that is 1 central VPC peers with 4 others . No TRANSITIVE peering ( if A and B has peer connection and C has peer connection with A, since C has peer connection with A it cannot connect to B . to connect with B it should have seperate Peer connection with B )
*we can have peer between regions

Create your own custom VPC  part-1
*******************************
Click create new VPC -> enter the name for VPC and CIDR - 10.0.0.0/16  and choose tenancy default . VPC created

Once new VPC created , route table will be create for new VPC and network ACL and security groups will be created by default .  Subnets and internet gateway will not be created.

Now need to create a new subnet -> enter a name for subnet with region name (10.0.1.0 - us-east-2a) that we going to choose below.   and choose the VPC that we created and choose the region  and  cidr 10.0.1.0/24  and now we going to create another one again  with 10.0.2.0/24 with region us-east-2b .

Turn on autoassing public address by choosing the subnet and action -> modify auto assgn IP and enable it . we have to do this for 1st subnet so that Ec2 instances that are attached with that subnet will have public IP address which can access to the outer internet and let the other one be private. So one subnet is public and another one is private which will not have public address (public - 10.0.1.0/24 prviate - 10.0.2.0/24 )


Now we going to add the internet gateway for VPC - > create a new gateway in the name of "myigw" once created it will be in detached status , now we need to attach that with our VPC. only one igw per VPC

Now we going to create a route table . 
Note : in the route table there will be a route which is used for communication between two subnets within the VPC . In VPC if there is two subnets so they can communicate between them . so automatically there will be a route will be created default in the route table.

Lets start with route table . Once we created a New VPC we will have a default route table (which is called as main route table) . ("which will be facing public so need to change it as private by removing the route 0.0.0.0 "  why because whatever subnets that we created will be defaultly associated with the main route table, so for safety purpose we making the main one as private facing . So now we going to create a another route table associated with new VPC as facing public by adding routes(des- 0.0.0.0/0 tar- igw ) and for ipv6 also we can make a route as " des - ::/0  tar - igw "  . As for now we have two routes one is private and another is public. Now subnets are defaulty associated with main route table so associate "10.0.1.0 - us-east-2a"  to second route table becasue that subnet is already public . so another subnet one will be default in main route table which is private.

We have to create a 2 ec2 instances one with private subnet and other with public subnet . put db server in private and webserver in public

Note : amazon always reserve 5 ip adderesses within your subnets 
US-east-1 in one account will be completely different AZ to us-east-1 in another account . 	AZ are randomized
Only one IGW for vpc
Route table, NACL,security group will be created defautly with VPC that we created.


Part-2
*****

Now we cant able to ssh the DB server through App server because the security group in DB server is not having allow ports . So creating a new security group for db server and attaching with VPC and allowing ssh, http, https,mysql/aroura, icmp ports in that security group.and attach that security grup to db server.

Now we taking ssh to app server and then ssh to Db server. to ssh DB server we need private key , so just copy the private key content from our computer and create a txt file in appserver "myprvtkey.pem" and save that content into that file. Now using this key file we can ssh to Db server  .

Now Db server ssh work . while doing yum update its failed because it is not connected to internet. so moving forward going to see the steps to cover this prob

##################################################################
NAT instances and NAT Gateways   - Network address translation 
****************************

Nat instances vs nat gateway : NAT instances is a single EC2 instances and nat gateway highly available gateway which is spread across multiple available zone thy not depened on single instance

NAT instances:

we can choose community ami and search for NAT  and select nat operating system to install in EC2 and choose the New VPC that we created and place this ec2 in public subnet  and put in same security grp which we used for app serer . Once nat server created go to action networking and change source/destination and disable it . We need to disable sour/dest check .

Now we are going to add route to communicate with nat server . I.e our Private route should talk to NAT. in that case edit the private route table(which is main) and add the entry destination :0.0.0.0/0  and target- nat instance (type i for instance search) . Now DB server can talk to internet and  yum update will work.  

So if we have 1000 of servers in private subnet and using this nat instance it will not be good. SO we going to use NAT gateway.  SO terminate the NAT instance and remove the route from private route table . In the route table the created route (for NAT ) status will be in blackhole which means the instance no longer exist.

Now we are going to create a NAT gateway  (under VPC dashboard) (nat instance and nat gateway should be in public subnet) only one NAT gateway can be created in one AZ
*****************************************
VPC -> Natgateway -> create a new nat gateway in the public subnet and then create a EIP and then edit the route table (MAIN) and enter destination 0.0.0.0/0  and target- nat gateway . Now db server can talk to internet

if our resources (instance) in different zone are connected with single NAT gateway (which will be in one zone) if that zone is failed all the resources in different zone will loose internet. so we have to create a NAT gateway for each Availablity zone to connect with their own resources.

A Elastic IP address to associate with the NAT Gateway (AWS will allocate this automatically during the process).if EIP Is not created manually it will be autocreated while creating the nat gateway


#####################################################################################
ACL - Network acces control list vs security groups ( we can add mulitple NACL under one VPC)  IT is stateles
*************************************
Note: everytime when we create a subnet it will automatically assign to our4 default NACL (which is created while creating VPC) . One subnet can associate with only one NACL at a time. 
NACL will act before security group . First it will check the NACL and then only it will move to security group . so If you allow the port in security grup and deny in NACL. First it will deny it. because after NACL then only security group will check the port .

Whenever we create a New network ACL the default will be deny all inbound and outbound traffic . once we creating rules to our inbound and outbound rule please add ("custom TCP rule"  port 1024-65535 ) then only yum update and other all will work.

* Create a new NACL attaching with our new VPC that we created ( which will deny all inbound and outbound traffic defaultly)
* Create a web sever , now default out subnets are  attached with prvious NACL (while testing the website is reachable) . Now we need to move the subnet (in which we created the websserver) to newly created NACL (where default all are deny) now site is not access . Now go do the new NACL and edit the inbound rules by adding the ports to allow ( 80,443,22) 100 200 300 rule number . Now the site is accessible .

NACL will work under the priority of rule number that we set . For example if we set rule number 100 for port 80 to allow in inbound and in the same we setting port 80 (rule 300) to deny in inbound . First it will take the rule 100 and allow the port. it will not deny because it is in rule 300. So now we setting the 80 port to deny in rule 99 . now it will deny because 99 will be first ( in 100 we allowed ). 

Now remove the 99 rule . Now website will work . When we try to do yum update in server it will not work . because we should add the ("custom TCP rule"  port 1024-65535 ) in both inbound and outbout . then only other things will work . this is for only the NACL that we created. Default NACL it will be there automaticaly

NOTE : NACL will work incremental oder. first number wil be the first preference . above is the example 

NOTE : VPC automatically comes with a default NACL and by default it allows all inbound and outbound traffic .  But when we create a NACL newly which will be default deny all inbound and outbound traffic . 

WE can block IP address using NACL but not with Security groups. 

We can have mulitple subnets in one NACL but one subnet will not be attached with muliple NACL . 


Security Groups (SG):
Operate at the instance level. They are attached to EC2 instances (and other resources like RDS, Lambda, etc.).
Stateful: If you allow inbound traffic to an instance, the corresponding outbound traffic is automatically allowed (even if you don’t explicitly allow outbound traffic).
Network Access Control Lists (NACLs):
Operate at the subnet level. They control traffic entering or leaving a subnet in a VPC.
Stateless: If you allow inbound traffic, you also need to explicitly allow outbound traffic, as NACLs do not automatically allow the response traffic like Security Groups do.



security group for each instance
nacl for subnet where multiple instance are connected. 
################################################################################################

Custom VPC and ELB
********************
Three types of load balancer
1) application load balance 2 network load balancer 3) clasic load balancer

While provisioning the ELB we need atleast two public subnets. ( one public is enough, but for multi availability zone we need atlease 2 public)
IF you are creating internet facing load balancer , the subnet should attached with internet gateway

#######################################

VPC flow logs :
******************

VPC flow logs is used to capture information about the IP traffic going between network interfaces in your VPC. the flow log data is stored using amazon cloud watch logs. 

Flow logs can be created at 3 levels 
VPC ,Subnet, Network interface level  --> we can get the logs for these

To create flow logs, we have to choose the VPC that we created and click action and create flow log.

In filter choose ALL (to store accept and reject logs ) and we can choose the destination either the logs can be stored in cloud watch logs or in s3 bucket. now we going to choose the cloud watch logs to store the log. So in the "Destination log " we have to choose the log group but we didnt create one . so we have to create one by going to cloudwatch and then logs and cxreate log group in the name  "VPC FLOW LOGS"
Now we can choose the "destination log as "VPC FLOW LOG" and then we have to choose the IAM role but we didnt create one either , so there will be a link to create a IAM role for this "set up permissions "  and then enter the role name " flowlogsrole" and once role created we can attach with IAM ROLE which we mentioned before. we can see the logs now in flowlogs that we created (by triying to acess the webserver which is placed in VPC)

After created flow log we cant change the IAM role that we attached with flowlogs

WE can also set up logs for VPCs that are peered with your VPC. but both VPC should be in same account.

VPC flow logs will not monitor all ip traffic , below that are not monitered

* Traffic generated by instance when they contact the amazon dns server will not monitor. IF you use your own dns server then all traffic to that DNS server will be logged

* traffic generated by a windows instance for amazon windows licence activation will not be monitored
* traffic to and from 169.254.169.254 for instance meta data  will not be monitored
* DHCP traffic will not be monitored
* traffic to the reserverd IP address for the default VPC router


Here is a breakdown of the information captured in VPC Flow Logs:

Timestamp: When the flow was captured.
Source IP Address: The IP address of the source.
Destination IP Address: The IP address of the destination.
Source Port: The port on the source machine (for TCP/UDP traffic).
Destination Port: The port on the destination machine (for TCP/UDP traffic).
Protocol: The protocol used for the traffic (e.g., TCP, UDP, ICMP).
Traffic Accept/Reject: Indicates whether the traffic was accepted or rejected by the security group or network ACL.
Packet and Byte Count: Number of packets and bytes transferred during the flow.
VPC ID: The ID of the VPC where the flow was captured.
Subnet ID: The ID of the subnet where the traffic originated.
Network Interface ID: The ID of the network interface that generated the flow log entry.
Flow Direction: Whether the traffic is ingress (incoming) or egress (outgoing).
Action: Whether the traffic was accepted or rejected (based on the security group or NACL).


FORMAT OF FLOW LOG

version account-id interface-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status
2 123456789012 eni-abc12345 10.0.0.1 192.168.1.1 12345 80 TCP 10 500 1621849210 1621849270 ACCEPT OK

We can also customize this format while creating the flow log itself. so that we can choose the parameter what we want inthis format. so that wil be displayed. 


Security:

A security engineer notices suspicious traffic patterns — a specific EC2 instance is trying to communicate with an external IP that isn’t in the allowed list. By reviewing VPC Flow Logs, they identify the IP address, destination port, and protocol used, helping to understand the nature of the attack or unauthorized activity.
Troubleshooting:

A developer is debugging an application that isn’t able to reach the backend database. VPC Flow Logs reveal that traffic from the EC2 instance is being rejected by a security group rule. The developer can update the security group to allow the required traffic.
Compliance:

For compliance purposes, an organization needs to ensure that no sensitive data is sent outside the VPC. VPC Flow Logs are used to monitor and review any outbound traffic to untrusted IP addresses, helping to maintain compliance with regulations like PCI DSS.
Network Optimization:

After reviewing VPC Flow Logs, a network administrator notices that traffic between certain instances in different Availability Zones is unusually high. The administrator can optimize the application’s architecture by deploying the instances in the same Availability Zone, reducing cross-AZ data transfer costs.

############################################################################################################

BASTION Hosts: which is used to ssh or RDP to the instance which are in private subnet . we do have AMI for bastion , so we have to create a 
a bastion host in public subnet .  so if we have to ssh the instance that in private subnet , we have to ssh the bastion and then have to ssh the instance that in private. Bastion is also called as jump boxes

We cannot use a NAT gatewway as bastion host. A Nat gateway is used to provide the internet traffic to EC2 instance that in private subnet (like doing yum update etc)

Bastion is used to securely administer EC2 instances(like ssh)

##########################################################################
Direct Connect :
##################

Direct connect directly connects your data center to aws.  Direct connect is a cloud service that makes it easy to establish a dedicated network connection from your premises to aws . which can reduce your network cost, increase bandwidth and provide more consistent network.

##########################################################
Setting up direct connect 
**************************

www.youtube.com/watch?v=dhpTTT6V1So

#################################################################
Global Accelerator :
******************
Aws global accelerator is a service in which you create accelerators to improve availablity and performance of your application for local and global users.


AWS Global Accelerator is a service that improves the availability and performance of your applications by routing traffic through the optimal AWS edge locations. It speeds up the flow of data for users across the world by directing their requests to the best-performing endpoints. 

Ensures a seamless experience for users, no matter where they are located globally.

so wherever the application is hosted , the global accelrater will check the best path and route the traffic to the service with lowest latency. 

cloudfront and global accerlaeration both use edge location. but cloudfront uses edge location to  cache the data and serves it , but global acceleration uses the edge location to find the shortest path to reach the application or lB. and cloudfront will use dynamic ip address where golabal aceleartor will use static ip 


creation of global accelrator>:

NAME --> Add listenres (80 port to listen) -- endpoint group (region where your application is placed)  and we can configure healthcheck --> add another enpoing group ( if your application hosted in different region , and if you have two endpoint we can add here as second endpoint group) ---> endpoint type (select your ec2 instance or LB where our service is hosted)

name -> listernes --> endpoint group -> endpoint type ( shortly)

if one region failed (endpoint group) it will automatically forward to another region ( endpoint group)  it is checking the health check right so it will forward automatically . 

we can also distribute the traffic between the two region like giving percentage for one region 70% and oether 30% like that 

once we configured this it will give 2 static IP and one Dns, 

so lets say we have configured with two regions in endpoint group , one is us-east-1 and another is ap-south-1 . ourapplication is hosted in these two regions separately. so when I run the dns name that provided by the global accelerater it will reach the ap-south-1 because that is our nearest location . it will find the shortest path and pass the request to nearest region. 


shortly : if we get request from user it will directly connect to aws global accelerator and send request to our endpoint group (which can be created for each region ) from endpoint group it directs to endpoint (webservers, elb,etc)

User req --> global acc      --> endpoint group                          --> endpoint
                             group can be created region wise                from that region it will go to the ec2 or lb etc
  

Normally if we didnt use the global accelerater the request will pass through each dns to resolve and then reach the endpoint. but in this case it will reach the nearest edge location , then it will forward directly to our global accelerator  and that will find the best route to reach our service (LB). it will find the best path to reach our location fast. in normal case it will go through multiple hops and then it will reach the service. 


Now we get global acceelerator end point. Using that enpoint we can access our serivices or application . Global acclertor will provide two static IP address , where we can give that to client end for whitelisting the IPS . 


ROUTE 53 latency based routing vs global accelerator 

Example Scenario:
You have an application running in US-East (Region 1) and EU-West (Region 2).
Route 53 is configured with latency-based routing to route US users to Region 1 and EU users to Region 2.
If an issue occurs in Region 1, and the ALB becomes unhealthy, Route 53 will detect the failure via health checks.
Route 53 will start rerouting traffic to Region 2.
However, DNS caches and propagation delays mean that users who have cached the old IP address may still be routed to the unhealthy Region 1 until their DNS cache expires.
Once the cache expires, they will start getting the updated IP address for Region 2, and the traffic will flow there.

but this is not the case of Global accelerator , we have two static ip , when user its the ip , the global accelrater itself find the best path and route the traffic to nearest location. so if one region is not available it will auto route to another region. so there will be very fast routing process and downtime will not be  there. but route 53 changes effect will take some time to propagate and also browser from client side some time uses the same IP resolution which is poiting to unavailable region that became downtime. until the ttl value expries the browser will send the reqest to same IP which is unavailable. 

#####################################################################
VPC endpoint:
******************

VPC endpoint enables you to privately connect your VPC to supported aws services (like s3) . VPC endpoint services powered by private link without requiring an internet gateway ( withing our VPC we can connect our private subnet instance to s3 bucket without internet gateway, NAT device , VPN connection or Direct connection  )And also instance in our VPC do not require public IP address to communicate with the resources (s3) . 

2 types of VPC endpoints  1) Interface enpoints 2) gateway endpoints

Interace endpoints is related to network interface that can communicate using vpc endpoint to the other services

gateway enpoints : 1) amazon s3  2)dynamo db  (currently these two are available )

Now if we need to put or get the file from Db server(which is in private subnet) it will use NAT gateway to reach the s3 bucket and will upload and download the file . 

Now in the case of gateway enpoint  now the instance will directly send the request or file to VPC gateway and then it will send the file to s3 bucket. ( in this we will not use nat gateway to connect the dbserver to s3 bucket , we are using VPC gateway )

LAB session:

1 ) need to check whether roles are assigned for commnunicating between the ec2 and s3 services . if not go to IAM and roles and create a role for accessing s3 bucket with full permission . and then assign the role to the EC2 instance(DB server) by choosing the EC2 instance and action and then instance settings and attach role . So now Db server has access on s3 bucket.

Move the both subnet(private and public) to default NACL group(default one which is created while VPC creation ) . Already one is attached with this NACL so attach the other one also to this NACL role ( This is not needed in this process just for safety purpose . just skip this)

Now ssh the webserver and then ssh the Db server (private) and now type command aws s3 ls , it will list the bucket list. But it is working througth NAT gateway . so once we remove the NAT gateway route from route table and then we try to list out the s3 bucket it will not work. 

Now we have to create the VPC endpoint. Go to vpc dashboard and Enpoints and "create a end point" - chosse aws service and then choose "amazon s3 gateway" and then choose the VPC ( we created ) and the choose the route table(main default) that need to associate . Now VPC enpoint created if we want to check go to the route table and check there will be a route which is created by VPC endpoint. 

Now try to list our the s3 bucket by " aws s3 ls --region us-east-2" it will work . 


While creating VPC itself we can choose to create s3 endpoint , which will create an endponint and attach with private route. 


VPC PEERING : once we peered both vpcA and VPCB we have to edit the route table to connect with each other , example below.

Modify Route Tables:

In VPC-A, add a route to the VPC-B CIDR block (e.g., 192.168.1.0/24), using the VPC Peering connection.
In VPC-B, add a route to the VPC-A CIDR block (e.g., 10.0.0.0/24), using the VPC Peering connection.

##############################################################################################SS
Aws private link
****************
Private link is used for opening your services in a VPC to another VPC

We can also open up services in a VPC to another VPC but there are some disadvantages

1) Open the VPC up to the internet : Its nothing but just allowing all internect access to VPC through Gateway so it will be highly risk and also there is lost more to manage like put high security to protect the instances etc.

2) Other way we can do is peering the one VPC to other VPC , but in this case if we have 1000 of VPC to connect then it is not possible to peering each and every VPC to each other 

So we are using Aws private link for this , we no need VPC peering ,No route tables, NAT , IGW, etc, only private link is enough to communicate between vpc to other vpc 

Aws private link requires NEtwork load balancer and ENI (elastic network interface) to communicate between source and target. source has netwrok load balancer which has static ip , and target has ENI ( which every instance will have default) . So using source static IP (Network load balancer) we can open up connection to target using aws private link.


You can create aws private link to make services in your VPC available to other AWS accounts and VPCs. AWS Privatelink is highly available ,scalable technology that enables private acess to the services across VPC boundries. Other accounts and VPCs can create an VPC endpoints to access your endpoint service.

endpoint service will provide service where as endpoint is used to access the service  ( endpoint service need network load balancer , endpoint need ENI elastic network interface


Endpoint service ( Service that we are providing through this endpoint service )

In this we creating Ec2 server and attaching that to Network Load balancer. Consider ec2 is providing some sort of services. Without network load balance we cant create a endpoint services.
Create a endpoint service (under VPC) choose the Network load balancer that we created and click acceptance required ( for communication acceptance)  click create service. Now LB will be associated to endpoint service.
Now copy the service name (endpoint service name that we created ) 

Endpoint  ( going to acess the service through endpoint)

Create a endpoint , choose a service catagory and paste the "endpoint service name that you created above" and choose the vpc whatever you need like default and choose the security group and az for this endpoint that you creating. Once endpoint created there will be a request sent to your endpoint service, go and accept the request. 

Now the private connection between endpoint and endpoint service has been created .  In this we didnt create ENI for endpoint  it will automatically create by itself once we created endpoint. 


IF we need to connect the rds from another account means, go the enpoint service where you created, and choose the allow principle tab and click add principel and enter the account ID of that other account where you enpoint is created. so only then the enpoint from the another accont can be connected to the endpoint service ( and also we have to accept the conection in the enpoint service which is normally done above)

its not like that if we have enpoint service na, we can give request to the endpoint service to create a connection, before that we have to mention the account ID in the allow principle tab, then only it can able to give the request to endpoint service and then only the endpoint service can access the request. 


Use Endpoint Services if you’re hosting a custom application or API for private consumption.
Use Interface Endpoints to consume AWS-managed services or custom Endpoint Services.
Both rely on AWS PrivateLink for secure, private communication.

Clear explanation :

VPC endpoint has two types

gateway enpoint : A gateway that connects your VPC privately to specific AWS services like S3 and DynamoDB.
interface endpoint : A private network interface (Elastic Network Interface) in your VPC that connects to AWS services or custome services ( using endpoint service)

interface endpoint is from consumer side configuration , who is going to access the resources from producer end.  producer end will be endpoint service or any other aws services . 
so using interface endpoint only we goint to access the resource or services from the other side.

example:

VPC A and VPC B , in VPC B we create a DB and connected with NLB and NLB is associated with the endpoint service . So now from VPC A  they need to connect with this endpoint service in the secure way.

So in the vpc A we have to create a interface endpoint and then attach the endpoint service (which is created in vpc b).  so ink VPC A if ec2 instance have to access the db means , it should hit the interface endpoint ( which has the dns name like servicename.dns.fhafosdhfahd.amazon ) so the ec2 has to hit this dns name , so it will directly hit the endpoint service via interface endpoint and get the result.

This interface endpoint has private ip , so once we hit this dns name "servicename.dns.fhafosdhfahd.amazon" for accessing the db , it will resolve to that PRivate IP and then it will hit the endpoint service and get the output.  so we cant directly hit the endpoint service . if we need to access the enpoint service then it should be go through the interface endpoint. so we have to hit the interface endpoint , that will connect to endpoint service ..  so one interface endpoint can be configured with only one endpoint service or other aws services. 

IN this private link is nothing but it is a technology enabling private communication over Interface Endpoints. using this technology only the above senories are working. 

########################################################################################
Aws Transit gateway  (simplify the netwrok architecture topology)
**********************

Allows you to have transitive peering between thousands of VPC and on premises data centers, it will work has hub.

#######################################################################################
AWS VPN endpoint ( connect our aws vpc from any where using the VPN Client (user can use this)
**************** 
two type of authorization mode

identiy based
certificate based

Ussing Identiy based, using simplead mode we using now. 

1) create directory service 

just go to directory service, choose the simple AD
create a directory dns name -- srinnivasantech.org
set the administrative password

once done, choose the VPC -- srinivpc (where our ec2 are placed)
now the directory service is created, which is used for authentication our VPN using VPN client

2) create certificate and upload to ACM

Now create a certificate for the vpn using ECRSA command in Linux server and upload the file in aws certificate manager.  once we create certificate we will get server.crt , server.key, ca.crt , client.crt, client.key

in the above keys we going to upload the server.crt and server.key and ca.crt to the ACM

using cmd, aws acm import-certificate --certificate fileb://pki/issued/server.crt --private-key fileb://pki/private/server.key --certificate-chain fileb://pki/ca.crt --profile admin-genral( this is the profile that we created by access key and secret key)

create the certificate in the same region where your vpc is located. 

3) create vpn endpoints

Now go to client vpn endpoints, and create a vpn endpoints and in that it will ask for cidr ip  just give anything between /12 or /22  subnet. will discuss below why this is used exactly. and then it willask for certification choose the certificate that we imported and then choose user-based authentication. there will be two type one is user-based and other one is mutual authentication(certificate based aauth). so once we choose the user-based authentication and then choose the active directory that we created

then it will ask for dns address (which is used for resolivng the quueirs when client request ) so go and check the activie directory we created it will have 2 dns ip address copy and paste that in the field.  and then choose the vpc where we need to connect our client . nd then we can choose the timeout session like 24 hours or 12 hours to disconnect the vpn session 

once done the vpn is created, but it wil in pending state, we have to associate the subnet that in vpc which we choose. any subnet we can choose only one subnet can be choosed from on AZ.  once done the vpn will be in available state . and then download the client configuration file which need to be attach int he vpn client software.

the last thing is we have to add the authorize rule which is authorize the client like which cidr range that the client can access from the vpc . lets say we have vpc with cidr 10.0.0.0/16
and we have public subnet and private subnet with below cidr range
Private Subnet: 10.0.1.0/24 (application servers).
Public Subnet: 10.0.2.0/24 (bastion hosts).

lets say VPN User Groups:
Developers.
Admins.


We can set like 

Authorization Rules:
Developers:

Destination CIDR: 10.0.1.0/24.
Access: Only developers can access the application server subnet.
Admins:

Destination CIDR: 10.0.2.0/24.
Access: Only admins can access the bastion hosts.

or we can give the full vpc cidr which can able to access all the subnet. 

CLIENT SIDE: finally

now download the vpn client and install it , and open the software and upload the configuration file and then give the uername and password which we created in directory service. and then access the vpc network. 


in the above details I have mention some CIDR while creating the VPN , why its used. 
The Client IPv4 CIDR in a VPN setup is used to assign IP addresses to the VPN clients when they connect to the VPN. These IPs are virtual addresses that the clients will use while communicating with your AWS VPC or on-premises resources over the VPN.


###################################################################
SITE-to-Site VPN connection 

SITE-SITE VPN connection ( on-premisis data center -> aws)

From the AWS side we have a vpc with Virtual private gateway where our traffic will flow through the virtual private gateway instead of IGW. Customer gateway device is nothing but the router from the on-premisis . using the these we going to connect our on-premisis dc to aws 

The first step is to create a Customer gateway in the aws ( which is representing the client customer gateway )  go to customer gateway and create customer gateway it will ask for public IP addresss just enter our on-premesis customer gateway IP address ( which is router ) once done click create. that's it  customer gateway is created with the public IP from the client side 

the second thing is create Virtual private gateway and then attach to the VPC which we have to connect with . 

Now create a site-site vpn connection - enter the name of this vpn connection , it will ask for the target connection --> choose the Virtual private gateway that we created and there will be another option also we can choose the  transist gateway instead of virtual private gateway if needed. In our case now we using VPG so wechoosed that. and then it will ask for customer gateway --> choose the customer gateway that we created --> and then it will ask for routing option 1) dynamaic (requires BGP) 2) static. so choose the static one and enter the CIDR range of our client environment (on-prem). aand there is also tunnel option (if needed we can configure it) . so once done it will create on vpn connection . You can check the vpn connection that we created it will have 2 tunnel which will be defualty created , that is for fault tolerance if one is failed another one can be in active. but both are currently down. we have to make them up . 

Once everything done, download the configuration file which will be in the page we created the vpn connection.  when we click the download configuration file it will ask for gateway device name ( strongswarn) and choose in which os we going to install, so based on this configuration file will be downloaded.

Go to the client side and install the strongswarn (its like checkpoint . check the flow and how its working) . once installed , we have to do some configuration setup for this software , so these steps also provided by the aws,  these configuraton settings will be clearly explained in the above configuaration file which we downloaded. so just edit the configuration settings in the strongwarn as explained in that configuration file (simple just we have to follow the steps that given in the configuration file to make the tunnels up ) in that configuration file also there is tunnel settings. so once everything is done we can see 2 interface will be available in ifconfig . and also go and check the vpn connection , both the tunnels will be up ( which is down before) . now everything is set up 

Now go and create an ec2 instance in the vpc in the private subnet . and then go to the route table and mention the destination as  CIDR of on-premis  and target as virtual private gateway which we created and save it . 	so all the traffic that are destinated to the cidr (that we mentioned) will go through the VPG. 
now go to your datacenter and try to ping your ec2 instance private IP we can get the response .

##############################################################################
diff between vpn endpoint and site-site vpn connection

Use Cases
Site-to-Site VPN Example Use Case
Scenario:
A company has an on-premises data center with its own network (e.g., 192.168.1.0/24) and needs to access AWS-hosted applications in a VPC (e.g., 10.0.0.0/16).


VPN Endpoint Example Use Case
Scenario:
A company has developers working remotely from various locations. They need secure access to the company's resources in AWS VPC (e.g., 10.0.1.0/24) without exposing resources to the public internet.


########################################################################################

AWS network costs:

Use private IP addresses over public IP addresses to save on costs. This then utilizes the aws backbone network

* If you want to cut all network costs, group your EC2 instances in the same availablity zone and use private IP addresses. This will be cost-free, but make sure to keep in mind if This AZ failes then its going to be downtime.
##############################################################################

VPC Summary :

VPC : VPC is a logical dataceter in AWS , Consists of route table, network , ACL, subnets and security groups . Security group are statefull, NACL are stateless . And there is no TRANSITIVE PEERING

VPC create default route table ,NACL , security group . WOnt create any subnets and Internet gateway

Only one IGW can create with one VPC , Amazon always reserve 5 IP address with your subnets

NAT instances : We have to disable source and destinataion check on instance . Nat instances must be in public subnet. There must be a route out of the private subnet to the NAT instance,in order to work. NAT instances are always behind the secuirty group. 

NAT Gateway : NO need to disable source and destination check . NO need to patch . Not associate with security groups.  Make sure the Resources use the NAT gateway in the same availablity zone 

We can have upto 5 VPC in each AWS region . 

NACL : Your VPC automatically comes with default NACL which allows default all inbound and outbound. But when we create a new NACL which will deny all inbound and outbound traffic. VPC must be associated with a NACL. IF you dont associate a subnet with NACL then subnet will automatically associated with the default NACL.  Block IP address using NACL but in security groups we cant. NACL can have muliple subnet, but one subnet can have only one NACL.

ELB and VPC : You need a minumum of two public subnet to deploy an internet facing loadbalancer.

VPC flow logs -  only same account VPC we can enable flow logs, not from different account . You can change the configuration once flow log created.
Not all IP traffic is moniotred. IF we are using Amazon DNS server it will not log the traffic , only if you using the outside DNS it will log the traffic which comes and go . Not monitor the amazon windows license activation . DHCP will not be monitered . Default Reservered IP also will not be monitored.

Bastion : It is also called as jumb box. NAt gateway or nat instances is used to provide internet traffic to ec2 instance in a private subnets. But Bastion is used to administer EC2 instances (like ssh or RDP). You cannot use NAT gateway as bastion .

Direct connect : Direct connect directly connects your data center to AWS which is useful for high throughput workloads. And also if you need stable and reliable secure conection we can use Direct connect

Global accelerator : Is a service in which you create accelerators to improve availablity and performance of your applications . The users reach AWS edge network and then through AWS backbone network it will reach the endpoint (ec2 or ELB  etc) . 

VPC endpoint : Privately connect your VPC to supported AWS services . Which does not require internet gateway or nat device or vpn connetion.  Instances in your VPC do not require public IP addresses to communicate with the resources(s3) .
Two types of VPC enpoints 1) interface 2) Gateway 
Currently gateway endpoint supports only s3 and dynamo DB


AWS private link : It is used to peering VPCs to tens,hunderds,or thousands of customer VPCs . To do this Private link is used. Doesnot require VPC peering, no route tables , NAT, IGW etc.  Only Network load balance  and ENI (Elastic network Interface) needed for this service.  NLB is on the service VPC (application server ) and ENI is on the customer VPC 

Transist gateway : Allows you to have transitive peering between thousands of VPCs and on-premises data center , words on hub and spoke mode. ITs supports IP mulitcast . 

AWS VPN cloud hub : If you have mulitple sites, which can connect to VPC with VPN ( AWS VPN cloud hub) . In this each sites can connect with VPC and also each site can talk to other sites aswell 
######################################################################################################

ELB : Elastic Load balancer
***************************

!) Application load balancer 2) Network load balancer 3) Classic load balancer

Application load balancer : It will loadbalance the HTTP and HTTPS traffic.  works in layer 7 ( Application) : For example if we choose the language option in website like choosing US language it will route to the Specific application servers (we can configure like this way) . So it will load the balance in application level

Network load balancer  :  it will loadbalance the TCP traffic . which will give extreme performance. Operating at layer 4 (network layer) . It is capable of handling millions of requests per second .

Classic load balancer :  are legacy elastic load balancer ( OLD lB ) WE can load balance HTTP/HTTPS applications and user layer 7 features such as "X-forwarded  " (this will be cover below) . it will opereate in both Layer 7 and layer 4. If your application stops responding the (CLB) responds with a 504 error. This means that the application is having issues. 

X-forwarded-for : This is nothing but if you need the IPV4 address of your end user, look for the x-forwarded-for header . For eg: The user has public IP address, when the user hits the load balancer ( the load balancer has private IP address internally facing the applications ) So Now the load balncer sends the requests to application server but it will send the source IP has its own IP not the USER public IP . In this case if we need to know the user public IP address we have to look up for "x-forward-for" header which will have public IP address of the user.

###################################################################################
Remove all the configuration that we done above till now in AWS .


Load Balancers and health check - LAB :
****************************************
Lanch 2 different instances with two different region with webserver installed (app001 - webserver01 app002 - webserver02) mention this in website . apply security grp as webDMS which is previous one, and check the site whether its working for both apps

Create a load balancer : create a LB with classic LB. name:myclassicELB and choose the default VPC and choose the listner port 80 and use the security grup as same as we used for app server. and then configure health check and then we have to add the EC2 instances and now LB is created. There is no IP address for ELB only DNS name for ELB.  and also check instance are "in service" once LB created it will take time to show this status in healthcheck.Now check the site using DNS NAME . Now stop one instance and check the LB will take out the app server from LB .

General:

you cannot add EC2 instances from different regions to the same target group for a load balancer in AWS. Target groups are specific to a single region, and all instances in the target group must be in the same region as the load balancer1.

If you need to distribute traffic across instances in different regions, you would need to set up separate load balancers in each region, each with its own target group. Then, you can use Route 53 or another DNS service to manage traffic distribution between these load balancers.


Now we going to check with Application lB
*******************************************
Remove the old LB that we created . Now go to the target group under LB and create a target group and in target group add the instances . and now create a ALB . same as above we did. in this we can choose the targetgroup that we created. Once LB created we have to register the target by addeing two servers (it will be in target moudule setting). Now LB is ready. Go to the LB and check there will be edit and rules option will be there. in that we can configure and route manything in application wise. (intelligent routing)


Advanced load balancer theory:
*****************************
 Sticky sessions : this will enable your users to stick to the same EC2 instance. Can be usefull if you are storing information locally to that specific instance . For eg : IF you have two ec2 instances both are sharing equal request. If you enable sticky session on one specific EC2 instance then traffic will not send to that EC2 instance. Only the User which  you bind to that EC2 can send the request to that EC2 instance . Others will be sent to the second ec2 instance.  IN exam they will ask you are not receiving any traffic to an single instance(sticky enabled ) in that case we have to disable the sticky sessions on that server. 

How It Works in Practice:
Login Request:

A user logs into the e-commerce website.
The request is routed to EC2-1 by the load balancer.
EC2-1 processes the request, generates a session ID, and sends it back A session token (e.g., session_abc123) to the user via a cookie in the HTTP response..so for the further request user will add the session token/cookie along with the request so the ALB will check the cookie and send the request to the ec2-1 

Browsing and Adding to Cart:

Subsequent requests from the user (e.g., browsing products, adding items to the cart) are routed to EC2-1 because of sticky sessions.
EC2-1 keeps track of the user’s shopping cart and session details locally.
Checkout:

When the user checks out, the request is also sent to EC2-1, ensuring the cart data and session state remain consistent.
What Happens if the EC2 Instance Fails?
If EC2-1 becomes unhealthy:
The load balancer detects this via health checks and stops routing traffic to EC2-1.
The user’s next request is routed to a different EC2 server (e.g., EC2-2).
Without a centralized session store: The session data on EC2-1 is lost, and the user may need to log in again or lose their cart data.
With a centralized session store (e.g., Redis): EC2-2 retrieves the session data from the centralized store, allowing the user to continue seamlessly.

like: The EC2 instance (e.g., EC2-2) receiving the request extracts the session ID (session_abc123) from the cookie. and EC2-2 queries the centralized session store (Redis/DynamoDB/RDS) using the session ID.


Cross zone load balancing : This enables you to load balance across the multiple availablity zones. For eg. In one availablity zone we have 4 instance with LB which can send the traffic equally . IN other Availablity zone we have only two ec2 instance with LB which also shares the equal traffic to them (both availablity zones are created for single site ). Incase if the second availablity zone cant handle the traffic due to overload what is the fix for this.  Cross zone load balancing will help us to share the traffic from one zone (2 instance) to the other Availablity zone LB(which has 4 instance). 

Path Patterns : This allows you to direct traffic to diferent EC2 instances based on the URL contained in the request. For eg : IF any request comes like www.tinyurl.com then it will goes to the first availablity zone(which has 4 instance) and if any request which comes like www.tinyurl.com/images/ then it will goes to the second availablity zones which has two instance. We can configure in that way like any request to access the images content will send the request to that particular instance. 



we have domain facebook.com and which points to our ALB . genrally in the route table we cant create the A record since the ALB will not provide IP address it will provide only the DNS name so we cant enter the IP for the a record. So we will create alias in the route 53 and point our domain facebook.com with the ALB dns name. Alais will accept the dns name. so this is the process . 

Question is , when the user try to resolve the facebbook.com the route 53 will check and say like the facebook.com dont have Ip and it has dns name and it will forward the dns name to the user. again the user will query the dns name sperately to the aws DNS and then aws dns will provide the IP of the LB. ALB has different public IP which is managed by aws. so aws will resolve the IP and again it will forward the right IP to the user and user gets the ip and it will give the page. 

ALB has different public IP which will be belongs to each availability zone where we configured. While we configuring the ALB we will chose 2 different AZ and each AZ has to be connected with one subnet. so while configuring we have to give these details. so the LB has unique IP for each subnet in the AZ. NOte only one subnet can be associated with the AZ while configuring the LB. We can have multiple subnet in the Az but while configuring only one is allowed to be attached in the LB. the same way we have mulitiple az in the region , so we can choose atleast 2 AZ while creating the LB for fault tolrence.  SO that LB will be deployed on all the AZ where ever we choosed in the configuration. 

Visualization of the Process:
Step 1: User enters facebook.com.

Step 2: Browser sends DNS query to DNS resolver.

Step 3: DNS resolver queries Route 53 for facebook.com.

Step 4: Route 53 responds with my-loadbalancer-1234567890.us-east-1.elb.amazonaws.com.

Step 5: DNS resolver queries AWS DNS for my-loadbalancer-1234567890.us-east-1.elb.amazonaws.com.

Step 6: AWS DNS responds with the IP addresses of the ALB.

Step 7: DNS resolver returns one of the IP addresses to the browser.

Step 8: Browser connects to the ALB using the IP address.

The public IP addresses of an Application Load Balancer (ALB) are not static and are managed by AWS, which means they can change and that will be updated in the dns by itself which is managed by aws. 

To see the current IP addresses, you’ll still need to use a command-line tool like nslookup or dig



Usually when the public IP got by the user , it will go through the IGW , and IGW check the route table and it will forward the traffic according to the route table. but in the ALB case it is not like that. since we dont configured the public IP of the ALB in the route table how come the IGW knows where to route the traffic. below is the process for this question . once the IGW gets the public IP of the ALB it will directly route the traffic to AWS internal infrastructe route. so that will have the right path to reach the destination

User Request: User requests my-loadbalancer-1234567890.us-east-1.elb.amazonaws.com.

DNS Resolution: DNS service returns public IP 52.86.22.1.

IGW Routing: IGW forwards the traffic to 52.86.22.1.

AWS Routing: AWS’s internal infrastructure routes the request to the appropriate ALB instance in one of the specified subnets.



TRAFFIC FLOW WITHOUT THE LB


Instance Configuration:

Private IP: 10.0.0.5

Public IP: 203.0.113.10

Outbound Request:

Instance 10.0.0.5 sends a request to the internet.

IGW translates the private IP (10.0.0.5) to the public IP (203.0.113.10).

Traffic reaches the internet with 203.0.113.10 as the source IP.

Inbound Request:

External user sends a request to 203.0.113.10.

IGW receives the request and translates the public IP (203.0.113.10) to the private IP (10.0.0.5).

Traffic is routed within the VPC to the instance 10.0.0.5.


***************************************************************
ALB -> target instance  complete flow

Once user hit the ALB , DNS will provide the public IP to the user (50.0.0.1) which is belongs to ALB . (note: ALB will have multiple public IP  associated , according to the AZ that are attached. If the ALB is attached with 2 AZ then it wil have two public IP and two private IP . Each AZ will have one public and one private IP . but these are not exposed to the user because it will provide only dns. Aws give only dns name to the user not the public IP of the ALB because those are not static it is dynamic and maintained by aws infrastructure.) so once the request enters the IGW . this IGW will do NAT , like it got the public ip of that ALB right, it will convert it into the private ip ( which is already linked and saved in the aws infra , already this public IP will be linked to private ip as I said above each AZ have one public and one private right, so that public IP will be linked to that Private  IP )

ONE AZ - public ip --- link --- private IP same as another AZ . if you want you want to check the public ip and private ip of that ALB means go to network interface which will be in EC2 dashboard and where you can see the private ip and public ip of the ALB in each AZ. 
the private IP will be in the created from the subnet that we associated in the each az. 

so once the IGW convert that public IP to private IP , then it will check the  route table and forward the packet to the right subnet and then it will reach the ALB. once ALB get the request it will check the listner and according to the port mention in the listerner it will forward the traffic to the target group. 

once the ec2 process the request it will give the reponse to the LB and LB forward the request to IGW , again IGW convert the private ip of that LB to public IP and then send back to the user. 


CROSS ZONE LOAD BALANCING -there is an option is alb and nlb where we can enable this or disable this. by default it will be enabled in alb.

With cross-zone load balancing enabled, the traffic is routed to the EC2 instances across both AZs according to the number of EC2 instances. In your case:

25% of the traffic will go to AZ-1 (because it has 2 EC2s out of 8).
75% of the traffic will go to AZ-2 (because it has 6 EC2s out of 8).
This ensures balanced traffic distribution based on the number of targets, regardless of which AZ the request originates from.

##########################################################################
three-tier architecture -


User (Frontend): User interacts with the application.
Frontend (Presentation): Sends requests to the backend, displays data to the user.
Backend (Application): Processes requests, performs business logic, and communicates with the database.
Database (Data): Stores and retrieves the necessary data.


Frontend: When you visit the homepage, you are interacting with the frontend. It loads product data, provides interactive buttons, and handles user actions like searching, filtering, and adding products to the cart.
Backend: When you add a product to the cart or place an order, the backend handles the logic (e.g., checking stock, calculating prices, processing payments).
Database: The backend interacts with the database to fetch product details, update inventory, and store your order information.

so in this frontend servers maya use the internet facing LB,  and the backend servers can use the internal facing lb, which is not exposed to the outside of the vpc.. so when the user need some data  it will first hit the Internet facing LB, which is frontend. and then frontend server will commnunicate with backend servers to fetch the details with some business logic. so these backend servers are in internal facing LB. so the frontend will call the internal facing LB to communication with the backend server to get the details, the backend will then communication with db and give the result to frontend. 





###########################################################
Autoscaling : There are different types of scalling options

1) Maintain current instance leveles at all times :   For eg: we have four webservers and incase any one instance went to unhealthy the auto scaling will remove that instance and add the new instance to that group. So it will maintain 4 servers all time ( we have to configure in that way)

2)scale manually : we can manually add or remove the instance manually 
3)scale based on schedule : In specific schedule time autoscale will scale the instance to specific level(as requested )
4)Scale based on demand : Will do in the LAB (if cpu reaches above 80% it will autoscale)
5) Predictive scaline
##############################################################
Autoscaling group :

Remove previously created LB , target grup and ec2 instance

Now we going to provision the auto scaling group.  But before creating autoscaling grup we have to create launch configuration . Choose what the configuration that wee need to create the instance . we can create a instance with bootstrap script for creating as webserver. once lanch configuration completed. we can create autoscaling grup. ( instance will be created in the configuration that we choosed in lanch configuration) 

Create a autoscaling group : Enter group size as"3" which will be creating 3 instances. and in the subnet we can add mulitple subnet. which will create the instance equally in all subnet. and then choose "use scaling policies to adjust the capacity of this group" and choose scale betwee 3 and 6 instances. (minimum there will be 3 instances, if the metrics that we choose is reached above percentage then it will automatically add the instance) . and then we have to choose the metrics (like cpu utilization, LB request count, average network in ) if we choose the CPU utilization and configure 80%  as target value. Then if CPU is utilized 80 percent then addtionaly instances will be added automatically. Once done we can see 3 instance will be launching (in starting we given group siz is 3) . IF any one instance is terminated Autoscaling group will automatically add the 1 instance to the group with the same configuration that we added in "launch configuration" (it will keep the instance state 3, untill it reaches the cpu utiliztaion 80% then it will add the instances newly as utilzation ( that wat we mentioned in scale between 3 and 6) . min is 3 and max is 6 . If we delete the autoscaling group that we created then it will delete all the 3 instances. 
###################################################################
NOTE : Scaling out : It means adding extra instances like that    scaling up : it means increasing the resourses inside our ec2 instances like adding extra ram like that .

WORD press site
***************

create 2 s3 bucket , One is for images, and another one is for code . and the go to cloud front and create a distribution for images(s3 bucket that we created). and now create a RDS . And then create a role for accessing s3 bucket from Ec2 instance . Now create a instance with appache, php-mysql,and wordpress . (script will be given below) and then attach the role . Now login to the instances and check whether https are up . and then copy the App ip and login through browser and connect the database steps using wordpress . In the wordpress site we can create a website and upload the photos that we want and create a website .
the upload photos will be stored in /var/www/html/wp-content/uploads . Now we need to copy the images(/var/www/html/wp-content/uploads) to s3 bucket (images folder) and then copy the complete code to (/var/www/html) to s3 bucket(code folder ) now complete site is backed up in s3 bucket. Now wat we going to do is instead of serving the images from the ec2 ( while accesing the site the images are coming from ec2) , we going to use cloud front to distribute the images instead of ec2 instances. now in html folder edit .htaccess file in that paste the cloudfront domain (which we created for images above). so from now it will server the images from cloud front and also we have to edit the httpd.conf file (Allowoverride None) we have to change it has (Allowoverride All). once httpd config changed restart the service. And also make the s3 bucket(media) as public to access to photos . Now when we open the site and check the images will be coming from the cloudfront not from ec2.

HTACCESS : which is hidden file where we can say take the picture from cloudfront insted of images folder(in server)  in that we have to paste our cloud front url which we created for images. we have to edit in rewrite line

and also we have to update in apache config file to allow rewrite option .  go to http config file and there will be line allowoverride none , we have to chaange that allowoverrider All . 
###################################################################################
Building fault torlernt word press site

1. Main concept is there will be writer node (which is created above for wp site) where admins will make changes in the server but this will not serve the site. We have to create a image from this main server and create a reader node . and place it in autoscaling group (max 2 server). while creating autoscaling group mention that it should be create under application loadbalancer . so whenever new server created it will be automatically goes under lb. dont forget to remove the main server from LB . Main thing is we have to create a cron job in writer node ( sync all the code and  media files from writer node to  s3 bucket accordingly) and in reader node we have to create a cron job for( sync all the data from s3bucket (code and media) to reader node path (/etc/var/html) ) so whenver we make changes in writer node it will sync in s3bucket and reader node will get the updated information from s3bucket. so if one server is terminated autoscaling will create another one and launch in LB. 

#########################################################################################
Cloud formation:

IS a way of completely scripting your cloud environment
Already a bunch of cloudformation templates created by aws solution architets which will allows to create a complex environment very qucikly.

For eg :the above word press site is created by us . The whole configuration has been already created as a template in cloud formation so once we use that template it will create a complete word press site with DB and instance qucikly automaticaly.

CFT - cloud formation template. it can be written in yaml and json. yaml is prefable because it is easy to write and read the things. and also we can comments the description like "# this is used for something" like that.

cloud formation will not only used for creating the infra, it is also do drift detection. Like using CF we created EC2 and s3 bucker , where someone change the bucket version of s3 bucket to disabled. so this drift detection will find out the difference and it will notify us on the change. So we can check and we can change it accordingly. 

There will be many CFT defaultly created by aws, if we written by your own  in yaml file, we can import them to CFT using "create stacks " option. 

in CFT we can write our code with these structure ,
version - version of the CFT
description - description of the CFT
metadata
parameters
rules -- rules is nothing but we can set some rules like if any one is about to create s3 bucket, it should be in proper naming convertion other wise it wil not create the s3 bucket . 
mappings  -- assigning the parameters to variables
conditions
resources -- like ec2 s3 like that 

All the example for each resources are available in the aws cloudformation page using that we can create the resources and also we can use the visiual studio and install the two plugings "yaml and aws tool kit"   which is verymuch helpful for creating the template . while typing something it will show what exactly you want and then we can choose that . so aws tool kit is used for this.  yaml toolkit will check for the intentation. 
we can also store our template in s3 bucket , so while creating stack we can choose the template from the s3 bucker , or we can just enter our code directly .

############################################################################################
Elastic beanstalk

With elastic beanstalk, we can qucikly deploy and manage application in aws cloud without worrying about the infrastructure that runs those applications. onlything is we have to simply upload our application or code which will automaticallyy handles the details of capacity provisioning, load balancing , scaling  and appliccation heath monitoring. 

Just simply upload our application or code which will create a whole environment.
##############################################################################
SNS: Simple notification service . SEnd notification from the cloud

It is used to publish messasges from an application and immediately deliver them to subscribers or other applications.
SQS - simple queue service 
Both sns and sqs are messaging service in aws. SNS - push ( push the information) and sqs - pull ( we can pull the information )

publish the messages to millions of customers (10 millions)  in the form of (email, http endpoint, SQS , texting ), its fully managed service and auto scaling. it consist of topic and subscription . we can use that for "application to person" and aalso for "application to application"

Appllication to person : from topic we can send the messages to multiple customers and also we can choose single customer also for sending the messages
Applicaation to application: from topic it can send to lamda for processing the message and save it to db , and also it can send it to sqs which can be used by other application for processing 

Create SNS topic --> name  --> encryption --> access policy ( who can publish the topic and who can subscribe the topic)  -> retry policy --> delivery status logging

ONce topic is created we have to create a subcrition --> choose the topic(arn) where you need to attach the subscription -> protocal (sms, email, sqs ..etc) --> subscription filter policy (anything you want to filter like only the topic which are in www  have to send the topic to the subscriber like that)  -- redrive policy( dead-letter queue)  ( this is we can enable or disable  and  this is used for storing the undelivered messages so that we can resend them again whenever its needed. SO These messages can be saved in the sqs queue)

once subscription is created . we can publish the messasge directly from the console or else if application need to publish the message means we have to configure the sdk for aws sns in the application and using the arn of our topic we can publish the message from the application so that it will deliver it to subscribers. 


Step-by-Step Flow: EC2 Auto Scaling Alerts with SNS
1️⃣ EC2 Launch/Terminate Event Occurs
Suppose an EC2 instance is launched or terminated inside an Auto Scaling Group (ASG).
2️⃣ CloudWatch/EventBridge Detects the Change
AWS CloudWatch detects the event and forwards it to an SNS topic.
3️⃣ SNS Publishes the Event to All Subscribers
SNS receives the event and sends a notification to all subscribers (Email, Lambda, SQS, etc.).


How to Set Up SNS for EC2 Launch/Terminate Alerts
✅ Step 1: Create an SNS Topic
Go to AWS SNS → Create a Topic (e.g., EC2-Scaling-Alerts).
Choose Standard Topic.
Copy the Topic ARN (e.g., arn:aws:sns:us-east-1:123456789012:EC2-Scaling-Alerts).
✅ Step 2: Add Subscribers
Go to the SNS topic → Click Create Subscription.
Choose Protocol (Email, Lambda, SQS, etc.).
Enter the Subscriber Endpoint (e.g., your email, Lambda function, or SQS queue).
Confirm the email subscription if using email.
✅ Step 3: Create a CloudWatch Rule to Trigger SNS
Go to AWS CloudWatch → Click Rules → Create Rule.
Select Event Source → Event Pattern.
Choose AWS Service: EC2, and Event Type: EC2 Instance State-change Notification.
Select the event types:
Running (when EC2 launches).
Terminated (when EC2 shuts down).
Set Target as SNS and select your SNS Topic (EC2-Scaling-Alerts).
✅ Now, whenever an EC2 instance launches or terminates, an alert will be sent to SNS subscribers!


FOR HIGH CPU USAGE OR RAM USAGE

cloudwatch sends notification to sns topic , sns receive the messages and send the message to sqs. sqs store the messages in the separate que ( we have to create a separate que for highcpualerts)  . once new messages stored in the sqs queue, lambda automatically polls the messages from the queue and sort out and fix the iissues according to the function we written in the lamda code.  question is how lamda knows the sqs as a new message?  AWS invokes Lambda when new messages arrive in SQS  . we have to set some setting accordingly in the lamda 

✅ Option 1: AWS Console
1️⃣ Go to AWS Lambda → Your Function
2️⃣ Click "Triggers" → "Add Trigger"
3️⃣ Select "SQS" as the event source
4️⃣ Choose the SQS queue you want to monitor

so once the new messages arrived in sqs, aws will note that and trigger the lamda function , and lamda do fix it accordingly or scale the ec2 instance. 	


##############################################################################################
SQS : SImple queing service 

SQS is a pull based not a push based. ( ec2 instances can pull the messages from queue and prepare for its need , like creating meme or etc. )
Messages can be in 256Kb in size. IF it is more than than then it will be in s3
Visiblity time out :   the amount of time that the messages will be invisible in the sqs queue once it is picked by any ec2 instance for processing it. If the ec2 instance complete its work before the visiblity time then the message will be removed from the queue. IF the ec2 instance cannot complete its work with that message within the visiblity time then again the messages will be visible in the queue so that another instance can again pick the message , which will cause duplicate entry.  to ignore this we have to increate the visiblityy time, The maximum visiblity time that we can set upto 12 hours .


Two types of queue :

Standard (default) : we can use unlimited transactions per second. standard queue guarentee that a message will be delivered  atleast once ( may more more than one also due to visiblity issue)
Standared queue provide best effort to ensures that messages will be  generally delivered in the same order as they are sent (but not 100 percent sure )


FIFO :  Firstin first out deliver exactly in the same order (in sequence) . It will not create duplicate entry becauuse it will wait and remove the data once the ec2 complete its work. 
300 transaction per seconds,  not fast as standard queue.


Scenario:
A payment gateway processes transactions submitted by users. Each transaction involves validation, fraud detection, and updating the database. By using SQS with Lambda, the system can handle transactions asynchronously while ensuring reliability and fault tolerance.


Frontend: Submits transaction details to an API Gateway.

Producer Lambda:
Validates the transaction details and places it in the SQS queue.

SQS Queue (transaction-processing-queue): Stores transactions for processing.

Consumer Lambda:
Processes transactions from the SQS queue.
Performs fraud detection and updates the database.
and also it can call the payment gateway for processing the payment.
once paymenet done lambda will notify the customer via email (SES) or anything 


SQS uses a Visibility Timeout mechanism:
When the Lambda function starts processing a message, the message becomes "invisible" to other consumers for a set duration (default is 30 seconds, configurable).
If the Lambda function completes successfully within the visibility timeout, AWS deletes the message from the queue automatically.
If the Lambda function fails or times out, the message becomes visible in the queue again for reprocessing.


###############################################################################
Elastic transcoder 

it is a media transcoder in the cloud. it converts media files from their original source format into different formats that will play on smartphones, tablet, pc etc
#######################################################################################
API Gateway :  (LoveToCode -- youtube channel ) for this video  	
API that acts has a front door for appliction to access the data or functionality from your back end services such as ec2, code running on lamda , or any web application. API gateway has cache capablities to increase performance . and api is low cost and it scales automatically. and also we can logs results to cloud watch


CREATING API GATEWAY:

CLick create api gateway --> we have 4 option 1) http api 2) rest api 3) websocket api 4) rest api private (REST API that is only accessible from within a VPC.)

choose rest api --> choose "new api" (we have other option like cloning exisitin api and import api if we have any like that ) -- api name -->  choose the api endpoint type ( regional , edge optimized , private ) regional means in that region only api will works , edge optimized will work on all the regions (use cloud front) ) choose the 
edge optimized so that our api endpoint can be accessed from anywhere.

Now we have to create a resource ( customize our api endpoint how it have to work ) create a resource  --> enter the name as user  (/user) 
now we have to customize how /user will work .  click "create method"  choose the method type (get , put, post, delete , etc..) in our case choose Get --> choose the integration type ( lamda function, http endpoint , aws service, vpc link (private link) ) choose lamda ---> then choose the lamda function that we created ( we have to create one before. lets say we have a lamda function which will get all the user details from the dynamo db ) and also choose the region where lamda function is created. and then click create method.  ( now method is created)

Once done we have to deploy the api that we created (so then only we will get a link endpoint to use ) . so click on deploy and choose the stage (we can creaste like dev, prod, stage  anything we named) we create a stage name "dev" in our case so choose that dev while we click on deploy buttion.  now the api is deployed in the stage "dev" (what is stage ? its like namespace just think like that) once we done that . we will get a link (https.dahodhadf.amazon.com/user) so once use this link in postman,  it will trigger the lamda and lamda will check the db and get the user deteails from the db and reply to the user via the same api. like that we can use the post method to enter the details in the database (like save the user details in the dynomo db)

we can also authorize this api ( so that all of them canot use this api endpoint to get the result ) we can use lamda authorize. so when we use the api endpoint in the postman we have to pass the "token" so then only the api will response otherwise it will give unauthorized. so lets checck how to do this. 

so first we have to create a lamda function ( which will used for autorizzation like it will check and validate whether the token is passeed is valid or not , if not it will not authorize ) so we have to create lamda function for that and in that function we have to mention the "token". so whoever using that api should pass this token then only they will get the response back.

create authorizer--> choose lamda ( we have two types lamda, cognito) and then choose the lamda function that we crete for authorization --> choose the option "token" (using the token option only we going to validate right) --> type the token source  just enter "auth-type" , (so when we using post man we have to pass "key and value" for authorization , in that way key will be "auth-type" and value will be the token that we mention in the lamda function ( eg: xyz123). so whatever the value that we passing in the "auth-type" in the postman that value willbe validated in the lamda and if it is right then it will work .) so once the authorizer is created . we have to apply this authorizer that we created to the api gateway where we have to put authorizer.  lets say we created " user" api right  in that go and edit that and in the authorizer field choose our authorizer that we created. so whoever trying to use this link have to pass the token to get the result.  (AWS Made Easy - youtube channel)





Difference between http api and rest api:

Both HTTP API and REST API handle similar tasks like GET, PUT, POST, and DELETE, but the difference lies in how they are implemented and what features they provide:

Http api:

 No API Gateway caching or request validation.
 cheaper than rest api
 very fast compare to rest api
 Use HTTP API for simple, fast, and cost-effective APIs.

rest api"


Suitable for advanced scenarios where you need more control and features.
Offers API Gateway caching, request validation, throttling, and more
Supports monitoring through AWS CloudWatch.
More expensive compared to HTTP APIs
Building APIs with complex authentication (e.g., IAM, Cognito or lamda auth).
Use REST API for feature-rich, legacy, or enterprise-grade applications.

More detail:

Request Validation:

REST APIs in AWS API Gateway support built-in request validation. You can define schemas for incoming requests (e.g., JSON structure) to ensure all required fields are provided and properly formatted.
HTTP APIs do not support request validation.

Caching:

REST APIs allow you to enable caching for specific endpoints (e.g., transaction history), improving performance by reducing backend load.
HTTP APIs do not have built-in caching capabilities.

Advanced Security:

REST APIs support AWS IAM and Amazon Cognito for advanced authentication and authorization.
HTTP APIs support simpler authentication methods like JWT but lack the full integration with IAM policies.

Monitoring and Throttling:

REST APIs offer detailed CloudWatch metrics and throttling for better monitoring and controlling API usage.
HTTP APIs have basic logging and monitoring but not as comprehensive.


Why Not Use HTTP API?
HTTP APIs are great for simple use cases but lack the advanced features needed for critical systems like a payment gateway:

No request validation = Risk of incorrect or incomplete data being processed.
No caching = Increased latency and backend server load for frequently accessed endpoints.
Limited authentication options = Reduced control over secure access to your APIs.

Conclusion:
For a payment gateway, where security, reliability, and data validation are critical, a REST API is the best choice.
If you just need a simple API for, say, serving static content or querying basic data (e.g., fetching public blog posts), then an HTTP API is sufficient.


Purpose of WebSocket in API Gateway:

WebSockets are ideal for apps requiring real-time updates, such as chat applications, gaming, live notifications, or stock price tracking.
Bi-Directional Communication: Both client and server can send messages to each other at any time after the connection is established.


 
###########################################################
Web identity federation - cognito
*********************************

for eg :IF a user want to login our website, she going to use the facebook or google account for login , once the facb authorized the login credention then it will pass the authorization token to cognito , then cognito will convert it to jwt token and then user sends that jwt token to identity pool and then the user will have the access 

USer pools : It handles things like user registration, authentication, 	and account recovery
identity pool : authorise access to aws resources

###################################################################
WAF - web application firewall which is used to block the sql injection attack and scripting attacks. and also it is used to blok ip address which we specified . For eg , If we managing public website, we have to use this WAF for safty security . NACL will also block the IP address but if user trying to attack with different ips then we have to go with WAF which will stop these attacks and it will work on layer 7 
####################################################################
Parameter store :
It is serverless storage for configuration and secrets like passwords,license codde, API keys.
###############################################################################
LAMBDA :

AWS Lambda is a compute service,where you can upload your code and create lambda function. AWS LAMBDa take cares of provisioning, and managing servers that you use to run the code. you dont have to worry about os, patching and sccalling etc.

lambda is most commonly used for like, example if any one who created the s3 bucket with public access, so we can write a function n lamda saying remove the s3 bucket which ever created with public access or we can also send sns notification to the user who created the s3 bucket . 

NO servers 
continous scales,  one request will create one lambda function.

Lambda will scale out automatially but not scale up . one req = one function , lambda is serverless (like s3)

AWS xray will alows to debug whats happeening if any prob in your serverless architecture 


LAB : Now we are creating a website with serverless and also using lambda we going to call a function ( like pressing a click buttion it will show whatever we coded likeyour name )

First create a lambda function in that put your code ( anything like your name tht should come in page) and this code can be accessed through api gateway so we have to create api gateway for this (this will be in same page where we creating the lambda function )once api gateway created it will give one link . through this link we can acess that code (That means if we click that link we will get our name which is coded ) .  But when we click that link it will show error " Missing authentication token") . this means we dont have api gateway permisssion to acess the lambda function (code) . so we have to give permission by clicking the name of the api gateway (api gateway name is myfirstlambda function) so if we click that we will get some settings in that we have to make changes and integrate the lambda function with API gateway in that setting page. once we done we will get one link, so that link will work it will show our name if we click tht link .

As of now what we done is we have one link which will show our name in page thats it . Now what we going to do is create a webpage (index.html) in that html page we have to code like (if we click "GET" buttion it should redirect to the API gateway link which shows our name)
EG : There will be webpage with somecontent in that webpage there will be "GET" button if we click that buttion it will show your name " thats it

But the website will be hosted in s3 bucket, we have to upload the index.html page in the bucket and make it has website page . 
each and every object in s3 have a link to acess, so get the index.html file link and make a dns entry for your domain . so when you enter your url it will redirect to s3bucket website. mainly we have to make the bucket as public otherwise it wil not work . 


LAMDA LAB:

here we going to do an task like, deleting the stale sanpshots which means the snapshot which is taken from the volume should not be attached with any ec2 instances, for eg, an ec2 instance which was running before and the user take 10-20 snapshots for backups, after few years he has to delete the complete ec2 instance and other resources which is created for that app. he deleted the ec2 instance and the volume which is attached with the ec2 everything. finally he forget to delete the snapshot which is taken . for this we going to trigger lamda function which will check what are the stale snapshots (nt use) is present and it will send notification to the user or it will delete the snapshot (according to the function that we written in python). which is used for cost-optimization . snapthost will create cost to the company , so we can do these kind of things to optimize the cost . using python code doing these things, in python we use boto2 module which will interact with aws api and get all the details of the resource which is useful for this task.

How Lambda Knows When an Object is Put in S3
S3 Event Notification Configuration:

Event Notification is the mechanism by which Amazon S3 can notify other AWS services, like Lambda, about an event (e.g., an object being created, modified, or deleted in a bucket).
To set this up, you configure an event notification on the S3 bucket. This involves specifying the event type (e.g., s3:ObjectCreated:* for an object upload) and the destination (in this case, a Lambda function). like if any object is newly puted in s3 this event will trigger the lambda

Once the event notification is set up, whenever an object is uploaded to the S3 bucket (based on the event type), S3 sends a notification to the Lambda function.
The notification contains information about the event, such as the bucket name, object key (file name), and other metadata.

Then The Lambda function then processes the file, and you can write your logic to do things like image resizing, data parsing, or other custom actions on the object.  we can also do like processing the log file and take out the errors count and their description and put them in separate s3 folder. so that errors can be easily found on that s3 folder separately and also if any critical error we can also integragte sns to send the notification. these can be done in the python code itslsed. but sns should be created by you in the sns service, which can be called in python.




#############################################################################

Kinesis : 


Which is used to store the streming datas from ( ec2, mobile,laptop, lamda ) . it will store  the data upto 24 hours to  7 days. The data contains shard .  These datas can be used by data consumers.  The data consumeres can access the data and analysis and use it for their process. 


KMS :

customer master keys (CMK)

3 types:

AWS managed CMK  : free, used by default if you pick encryption in most aws services. only that service can use them directly. 
custom managed cmk : Allows key rotation , controlled via key policies and can be enabled/ dissabled.
aws owned cmk : used by aws on a shared basis across many accounts. you typically wont see these. (this can be used by aws to protect the customers data) 

Two types of encryption :

symmetric cmk :

Same key used for both ecryption and decryption .
AES-256
AWS services integrated with KMS use symmetric cmks


Asymmetric:

Mathematically related public/private key pair 
we can download the public key and use outside the aws to access the services
Used  outside aws by users who cant call KMS api .
AWS services integrated with KMS wil not use asymentric cmk



AWS managed key
customer managed key



symmetric keys - both encryption and decryption are done with same key
asymentic keys = the encyprtion and decryption will happen with two different key that's called public and private

Aws managed key is a symmetric key


Customer managed key :
Is the key where we can create our own key . click create key --> choose symmetric or asymettric ( symmetric choose) --> key usage ( choose entrypt and decrypt) -- advance option
1 kms 2 external import 3 aws cloudhsm key store   ( choose the kms which is recommended - where kms will generate the key for you or if we choose exteral we can import our own key)
and then choose single region or multi region (whatever we use)
and then it will ask for define key permission ( who can manage the key or administrate the key , we can give IAM user or roles if we need) and also there is option "allow key administrator to delete the key" if we want we can enable or disable. 
next we have to choose the keyusers ( who is going to use tis key for encryption and decrption) IAM users groups or roles. and there is also a option for adding another aws account to use this key.  that's it we can review and click done to complete . Now we have a customer managed key with us (symmetric). if you need to enable the key roatation policy , choose the custom key that you created and in the option you can see the  key rotation just enable it that's it. 

If we want the key with assymetric , we can choose asymmetric option, and other steps are same like the above one.  and if you want the public key , you can go and open the custom key that you created as assyemtric and in the option you can get public key . jjust copy that and you can use that to authenticate. 

so now we have both kind of custom key  that we created . one is symmetric and other is asymmetric. you can use this key for any service for encryption . 



so for example when we try to copy the ebs snapshot (encrypted) from one region to other region. we use copy option and we will copy the ebs snapshot to other region, while copying itself we can enable the encryption or disable it . if we enable the encryption we have to choose the target kms key to wich we want to use for the encryption once done , the ebs will copy and paste it on the target with the encryption ( target kkey )

so how without using the source key we doing this, without decrypting the ebs from the source key how it goes.

so aws will manage itself like first it will decrypt the ebs with the source key and then only it will encrypt with the target key that we choose and then place it in the destination region. so first decrypt will be done and then only it will copy . so if we dont have permission for the decryption from the source then it is not possible. so we should have enough permission to decrypt the ebs volume while copying  to other region . 

################################################################################
CORS : Corss orgin resourse sharing  
&*****************************************************
SHORT


s3 transfer accelerater  -> s3 transfer acceleration uses the cloudfront edge netwrok to accelrate your uploads to s3. instead of uploading directly to your s3 bucket, you can use a URL to upload directly to an edge loaction which will then tansfer that file to s3. 

Aws datasync : Allows us to move the large amount of data from on-premises to aws . replication can be done hourly daily or weekly .

VPC endpoint enables you to privately connect your VPC to supported aws services (like s3) . VPC endpoint services powered by private link without requiring an internet gateway

Gateway endpoint using this we can connect our private subnet ec2 instance with s3 

Private link is used for opening your services in a VPC to another VPC
So we are using Aws private link for this , we no need VPC peering ,No route tables, NAT , IGW, etc, only private link is enough to communicate between vpc to other vpc 

Transist gateway  : Allows you to have transitive peering between thousands of VPC and on premises data centers, it will work has hub.



ALB : target group -> add instance -> create alb -> attach the target group -> register the servers in target group

AWS Lambda is a compute service,where you can upload your code and create lambda function. it is serverless, the lambda function can be trigrred using api gateway. 

Athena :

Athena is interactive query service  which allows you to query data located in s3 using standard SQL,
Serverless,nothing to provison , pay per query/per TB sccanned
commonly used to analyse log data stored in s3



AWS CODE COMMIT, CODE PIPELINE, CODE BUILD, CODE DEPLOY

CODE COMMIT:

it is same as git, for code commit we cant use the root account of the aws, we have to use the IAM user for cloining and other task.  same as git we can have the cli option like commit, push, and everything .  this code commit is a private repository. 
disadvantage is like very less integration with services outside aws. not having much features like git , GitHub. it has less features and restriction. 
many of them are not using this code commit , because of less features than git and bitbucket and etc. so better use git as a code repository in your company.  this code commit is a aws managed services, we dont have to worry about the downtime, scalling and other things. 

CODE PIPELINE:  

its same as Jenkins.  it will use the code build to checkout, build,scan,imagescan,imagepush (CI part) like things. Jenkins is opensource then why we have to pay for the code pipeline ? in case if we use our own Jenkins, we must have to provision ec2 instance for master node and worker node  which will be increased if we have lot of jobs and also we have to manage the whole Jenkins by ourself like patching etc. but codepipeline is fully managed by aws and it will take care of all the things like scaling and other things. 
we can also efficiently use the Jenkins rather then code pipeline, like initiating the job by docker agent which will run the job and delete by itself. According to our environment set up and our usage we can choose code pipeline or Jenkins for our CI part. 
and also code that we used for integration can only be usedin codepipeline and it cannot be export to other ci tool, for example we are movingout from aws to azure in that case we can use our code in azure because it can only used in aws , but Jenkins is not in that case we can simply create our environment in azure and we can push our code there. 

Jenkins will take care of CI part completely like building , scanning , imagepush like that , but in code pipeline it will trigger the code build to do these things, code pipeline can also use to triggere Jenkins for doing these things. 


ECR vs dockerhub  (elastic container registry )

both are image repository , in dockerhub we can store the images publicly and privately , and in ECR it is by default private and there is public also available. . what is the advantage of ECR. if we store the images in the docker hub and if the website is down we dont have any service people to contact directly or we dont have support. but in ECR is not that case. and in our company if we have 1000 of employee and where we have to give access to docker website we have to create each user in the docker. but in ECR we can integrate our IAM with that so that user can be directly synced.  so the better option is ECR or quay.io and other cloud repository like GCR (gogle) . 
IN ECR there is an option to scan the images and then  pushed in the repository. so the before using the images from the ECR we can see the scanned reports is there any security bugs are there and other things. 

so using aws config , we can configure our iam credentials and then we can pull the images from ecr and push the images to ecr, with iam user permissions. 




ECS vs EKS

ECS is aws own service which is used for orchestation like EKS/kubernetes. in ecs if we created our application which cannot be tranfered to other cloud. because it is only for aws platform , whereas in eks we can totally shift the cluster to other environment and we can use them as same as before. 


Secret management :

In aws we can use secret manager , which is only can be used in aws, but there is another one which is hashicorp vault which is can be used any where. 




ALB (Application Load Balancer) vs NLB vs GLB:

ALB:
Best for: HTTP/HTTPS (Layer 7) traffic, modern web applications, microservices, content-based routing.

Use cases:

Web applications: ALB is designed for routing HTTP/HTTPS traffic at the application layer (Layer 7). This allows it to make routing decisions based on the content of the request (such as URL path, query string, host, or headers).

NLB (Network Load Balancer):
Best for: TCP/UDP traffic, low-latency, high-throughput applications, extreme performance needs.

Use cases:

High-performance, low-latency applications: NLB operates at Layer 4 (TCP/UDP) and is designed to handle high volumes of traffic with very low latency. This makes it ideal for applications that require fast, efficient handling of network traffic.

NLB provides a static IP address for the load balancer, which can be useful when your application requires a known IP or for regulatory compliance.  (like whiteisting the ip in the other end)	

nlb CAN be used in High-Volume Streaming and Video Content Delivery applications , 

Scenario: A global live streaming service uses NLB to route video data from their content origin servers to edge servers in different geographic locations, ensuring that video streams are delivered to users with minimal delay and maximum reliability.

and also it can be used in databases servers also.

GLB: Gateway loadbalancer

Imagine you have a website that needs to be protected by a firewall. Instead of using just one firewall, which could become a bottleneck, you use multiple firewalls. The Gateway Load Balancer distributes incoming traffic evenly across all these firewalls. If one firewall fails, the load balancer automatically redirects traffic to the healthy ones, ensuring continuous protection and performance. a

BElow is the flow.  actually glb is only used for balancing the load in the firewall instance after that only it will go through the alb

Traffic enters the Gateway Load Balancer (GLB): The GLB first receives the incoming traffic.
Traffic is distributed to firewalls: The GLB then distributes this traffic across multiple firewall instances for inspection and protection.
Traffic is forwarded to the Application Load Balancer (ALB): Once the traffic has been processed and secured by the firewalls, it is sent to the ALB.
ALB routes traffic to your application servers: The ALB then directs the traffic to the appropriate application servers based on the routing rules you have set up.

it is also used for different purpose other than routing the traffic to firewal

GLB can be used to manage traffic through multiple WAF instances, protecting web applications from common threats like SQL injection and cross-site scripting (XSS)..
GLB can route traffic through analytics and monitoring appliances that provide insights into network performance, detect anomalies, and help in troubleshooting issues.


3 tier application ( frontend, backend, database)

the flow will be the user request goes to route 53 and it process the domain and get the ip and then it reaches the cloudfront, if the user requested a content which is already cached in the cloudfront then the reply will be sent back from the cloud front itself in edge location. or if the user requested a new content then the request goes from the cloudfront to the ALB and then alb will forward the traffic to the webserver which have all the content in that page , from there it will reach the backend server where our application logic will process the request and if any datas needed it will then reach to db and get the details and then process the data and sent back to the webserver (frontend page to disaply the content)


FB page
Final Flow Recap:
User Requests Page → Route 53 resolves domain → CloudFront serves static files from S3.
User Logs In → Frontend sends login data → ALB routes to EC2 instance → Backend checks credentials in RDS → Backend responds with a session token.
User Views Feed → Frontend requests posts → Backend queries RDS and S3 for posts and media → Backend responds with post data → Frontend renders the feed.


EC2 vs lamda


Use EC2 when you want full control over your infrastructure, including the operating system, network settings, and system-level configurations.
ec2 is used to process large data which need high cpu or high hardware resources. 
Complex networking, VPCs, private IPs, load balancing, and advanced routing can be done in ec2
but in lambda 
Limited networking capabilities, typically public endpoints with limited network configurations
Light to medium compute tasks that do not exceed memory/timeout limits
Limited control over infrastructure, only what AWS manages for you


IAM OIDC  (open id connect)

Scenario
Imagine you have a stateful application running in an EKS cluster that needs to access EBS volumes. You want to use IAM roles for service accounts (IRSA) to manage permissions securely.

Steps
Ensure your EKS cluster has an OIDC provider. You can create one using the AWS Management Console or eksctl:
eksctl utils associate-iam-oidc-provider --cluster <your-cluster-name> --approve

Create an IAM Role with EBS Permissions:
Create an IAM role that allows access to EBS. Attach a policy to this role that grants the necessary permissions, such as ec2:AttachVolume and ec2:DetachVolume.

Create a Kubernetes service account and annotate it with the IAM role ARN

apiVersion: v1
kind: ServiceAccount
metadata:
  name: ebs-access-sa
  namespace: default
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::<account-id>:role/<role-name>

Deploy your stateful application (e.g., a StatefulSet) and specify the service account created in the previous step


IAM OIDC :  detail explain

eksctl utils associate-iam-oidc-provider --cluster <your-cluster-name> --approve

the abve command we used in Kubernetes to associate our iam oidc proidver with our cluster, so the above command is used for that.

What does this command do?
When you create an Amazon EKS cluster, by default, the cluster does not have an OIDC provider configured for federated authentication. This command adds an OIDC provider to your EKS cluster so that your workloads or users can authenticate with AWS services securely without using long-term AWS credentials (such as access keys or secret keys).

Why is this important?
The OIDC provider integration is particularly useful for IAM roles for service accounts (IRSA) in Kubernetes. With IRSA, you can associate an IAM role to a Kubernetes service account so that your Kubernetes workloads (e.g., pods) can assume specific IAM roles to access AWS resources, like S3 buckets, DynamoDB tables, or EBS volumes, without requiring AWS access keys in the application itself.

In short, this command enables IAM roles for service accounts (IRSA) by associating an OIDC provider with your EKS cluster, allowing Kubernetes workloads to securely access AWS services.
Breaking Down the Command:
eksctl:

eksctl is a simple command-line utility for managing EKS clusters. It simplifies tasks like creating, configuring, and managing EKS clusters, which would otherwise require a lot of manual steps via AWS Console or CLI.
utils associate-iam-oidc-provider:

This is a subcommand that tells eksctl to associate an IAM OIDC identity provider with your EKS cluster.
When you run this command, eksctl automatically:
Configures the OIDC provider for your EKS cluster.
Creates a URL for the OIDC provider that allows Kubernetes to authenticate using IAM roles via OIDC.
It essentially connects your EKS cluster to AWS IAM using the OIDC standard.
--cluster <your-cluster-name>:

This specifies the name of the EKS cluster with which you want to associate the OIDC provider.
--approve:

This flag automatically approves the creation of the OIDC provider and any associated resources. Without this flag, you'd have to manually approve the action.
How does OIDC work with EKS?
In EKS, the IAM OIDC provider enables a mechanism called IAM Roles for Service Accounts (IRSA). This is a secure method to grant specific permissions to Kubernetes workloads (pods) by associating them with an IAM role.

Without OIDC:
By default, if you want to grant a pod access to AWS resources, you would have to assign an IAM role to the EC2 instance that hosts the EKS worker nodes. This approach isn't ideal because it gives broad permissions to the node, and any pod running on the node can access the resources associated with that IAM role.
With OIDC:
Instead of attaching IAM roles to EC2 instances, you can associate IAM roles directly with Kubernetes service accounts using IRSA. This allows you to give very fine-grained permissions to specific pods without needing to manage static credentials.



Enable OIDC on EKS Cluster:

When you run eksctl utils associate-iam-oidc-provider, the command sets up the OIDC provider for your EKS cluster.
The OIDC provider is associated with the EKS control plane so that your pods can authenticate with IAM using the OIDC protocol.

without associating the OIDC provider with your Amazon EKS cluster, you cannot directly assign an IAM role to a Kubernetes service account. This association is a necessary step to enable IAM roles for service accounts (IRSA) in EKS.




CloudWatch is AWS-native, so it is tightly integrated into AWS and provides excellent monitoring of AWS resources (e.g., EC2, Lambda, RDS, S3, etc.). However, CloudWatch primarily focuses on AWS environments.

Datadog, on the other hand, provides cross-cloud monitoring, meaning it can monitor AWS, Azure, Google Cloud, and on-premises infrastructure all in one place. If you're operating in a multi-cloud or hybrid environment, Datadog is extremely useful because it consolidates monitoring data from all cloud environments, including AWS and other platforms, into a single pane of glass.

CloudWatch offers basic dashboards and the ability to create custom metrics, but its visualization and dashboarding capabilities are relatively basic compared to Datadog’s.

Datadog provides more advanced and customizable dashboards, interactive visualizations, and the ability to combine metrics, logs, and traces in a unified view. You can create highly customizable and detailed views for your entire stack, not just AWS services. Datadog also supports more complex machine learning-based anomaly detection to automatically identify outliers in your data.

If you're running a microservices architecture with distributed applications, Datadog’s APM (Application Performance Monitoring) and distributed tracing can help you trace a request across all your services and see how they interact. CloudWatch can't provide the same level of integrated tracing and log correlation.

 End-to-End Application Performance Monitoring (APM):
CloudWatch can provide basic monitoring for AWS resources, but it doesn't have built-in application performance monitoring tools. It doesn't allow you to trace requests or monitor the performance of the code itself.
Datadog offers APM (Application Performance Monitoring) capabilities, which include distributed tracing, request-level insights, and detailed performance metrics for your application code. This includes monitoring the health of web requests, API calls, database queries, and other application components in a real-time, detailed manner.

Use Case:

If you're building a highly dynamic web application or a microservices architecture, Datadog’s APM tools allow you to track individual requests and see how they flow through your system, identify latency issues, and trace errors back to their source.


even datadog can trigger lambda function, but not as cloudwatch doing, but we have to create api gateway endpoint for lambda function that we created , and the endpoint should be configured in webook of datadog to trigger.


MONTIORING VS OBSERVABLITY

Monitoring: Helps you keep an eye on the system's health. It’s like having a dashboard with key metrics that give you high-level information about whether everything is working as expected or not. When a predefined threshold is breached (like high CPU usage), it triggers an alert.

Observability: Goes deeper. It’s about understanding how and why the system is behaving the way it is, and helps you explore issues, uncover unknown problems, and troubleshoot complex failures across distributed systems.

###################################################################

S3 bucket cross account replication:  ( INGORE THIS ,Below process is worked check that )

Scenario:
Apple A is a company that has some important data stored in their system (like a file in a "folder").
Apple B is another company that needs to access that data from Apple A to complete a joint project.
However, Apple B can’t just access Apple A’s data directly, because there need to be proper security controls and permissions in place to make sure only the right companies or people can access it.

Step 1: Apple A Creates a Special Role (Permission Grant)
Apple A creates a role (like a special "data access role") with permissions to access their data.
This role has:

Permission to read the files from Apple A’s storage.
Permission to write or update files to Apple B’s storage (if needed).
The role itself doesn’t do anything until someone uses it.

Step 2: Setting Up the Trust Relationship
Now, Apple A needs to decide who can use this role to access the data. This is where the trust relationship comes in.

Apple A creates a trust relationship with Apple B. This means:
Apple A says: "I trust Apple B to use the special role I created, so they can access my data for our shared project."
This allows Apple B to assume the role and access the data on behalf of Apple A.
Without this trust relationship, Apple B wouldn’t be able to use the role to access the data from Apple A.

Step 3: Apple B Uses the Role (Assumes the Role)
Now that the trust is established, Apple B can use the role created by Apple A.

Apple B uses the role created by Apple A to:
Read files from Apple A’s storage.
If needed, write to Apple B’s own storage.
However, Apple B cannot directly access Apple A's data unless it uses the role that Apple A provided through the trust relationship. It can only do this because Apple A trusted Apple B to assume this role.

Step 4: Apple B’s Storage Access Policy (Bucket Policy in Account B)
Finally, Apple B also needs to make sure that Apple A’s role is allowed to write into Apple B’s storage if the data is being copied or modified.

Apple B will configure a policy on its own storage system to accept data coming from Apple A’s role. This is like Apple B saying: "I’ll accept data from Apple A's trusted role, but only if it’s the right role."
Summary of the Process:
Apple A creates a role with permissions to access its data.
Apple A sets up a trust relationship, allowing Apple B to use the role and access Apple A's data.
Apple B assumes the role to access Apple A’s data and perform the required actions.
Apple B ensures that its own storage accepts data from Apple A's trusted role by configuring a policy.
Conclusion:
In this analogy:

Apple A creates the role and sets the permissions.
Apple A also defines a trust relationship, saying that Apple B can use the role to access its data.
Apple B assumes the role to access Apple A’s data and possibly update its own storage (just like cross-account replication in AWS).
Apple B has to ensure that its storage allows the data from Apple A's role, just like how the bucket policy in Account B ensures Account A's role can replicate to the destination bucket.
This is similar to how AWS S3 cross-account replication works with roles, trust relationships, and bucket policies.

##################################################################################

WORKED PROCESS


Create a bucket in both source and destination bucket.
create a replication rule in source bucket, it will ask for other account id and the bucket name enter the information , when creating the replication rule it will ask for "create a role " or existing role . you can choose the create role , so that it will create a role by itself accordingly.  and also I have given the code which is created (for reference) by itself.


Summary in Simple Terms:
First statement:
Allows listing and viewing the configuration and metadata (like tags, ACLs, retention settings) for objects in the regdxed and monicbucket buckets.
Second statement:
Allows replicating objects, replicating deletions, and copying tags from regdxed and monicbucket to another destination, and ensures the destination bucket owner becomes the owner of the replicated objects.

regdxed - source bucket   monicbcuket -- dest  bucket

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "s3:ListBucket",
                "s3:GetReplicationConfiguration",
                "s3:GetObjectVersionForReplication",
                "s3:GetObjectVersionAcl",
                "s3:GetObjectVersionTagging",
                "s3:GetObjectRetention",
                "s3:GetObjectLegalHold"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:s3:::regdxed",
                "arn:aws:s3:::regdxed/*",
                "arn:aws:s3:::monicbucket",
                "arn:aws:s3:::monicbucket/*"
            ]
        },
        {
            "Action": [
                "s3:ReplicateObject",
                "s3:ReplicateDelete",
                "s3:ReplicateTags",
                "s3:ObjectOwnerOverrideToBucketOwner"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:s3:::regdxed/*",
                "arn:aws:s3:::monicbucket/*"
            ]
        }
    ]
}


once created , then we have to move to account B and we have to create bucket policy to account B . below is the policy .

{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Principal": {
				"AWS": "arn:aws:iam::156041398855:role/service-role/s3crr_role_for_regdxed"   -- this is the role which we above created( auto created by replic)
			},
			"Action":  [
			   "s3:ReplicateObject",
               "s3:ReplicateDelete",
               "s3:PutBucketVersioning",
               "s3:GetBucketVersioning",
               "s3:List*"
            ],
			"Resource": [
                "arn:aws:s3:::monicbucket",
                "arn:aws:s3:::monicbucket/*"
            ]
		}
	]
}

once this is done, try to put object on source it will reflect in the destination. 

NOte: to replicate the previous object we can use s3 sync command to do that instead of replicating using batch process. if you need batch process we can enale it while creating the replication rule itself. 




s3:ReplicateDelete: Permission to replicate deletions, meaning if an object is deleted from the source bucket, it should be deleted from the destination as well.
s3:ObjectOwnerOverrideToBucketOwner: Permission to make the destination bucket owner the owner of the replicated objects, even if the source account owns them.
s3:ReplicateObject: Permission to replicate an object from one bucket to another
s3:ReplicateTags: Permission to replicate object tags from the source to the destination bucket.


VIDEO REF : https://www.youtube.com/watch?v=tEpJxHlb5qQ



############################################################################
Private hosted zone and public hosted zone  ;

Private hosted zone: This will not face the internet . The domain name that created in the route53 with private hosted zone will be associated with our VPC. so this domain name will be work withhin that VPC. so its privatly hosted. It cannt be accessed through outside. So only within that VPC we can able to access that domain . outside the vpc also we cant able to access.   NO need to buy the domain name and all. 



Public hosted zone : this is usual thing that we do, purchase the domain name and configure in the route 53 which can be access via internet



GIT STATERGY

main (or master):

The stable production branch.
Only contains tested and deployed code.
Used for hotfixes and as the source for release branches.

develop:

Integration branch where developers merge their feature branches.
Prepares code for the next release cycle

.
Feature branches (e.g., feature/add-login):

Created for individual features or tasks.
Merged into develop after completion and code review.


Release branches (e.g., release/v1.2):

Created from develop when a release is being prepared.
Stabilizes the release with bug fixes, final testing, and minor adjustments.
Merged into main and develop upon release.


Hotfix branches (e.g., hotfix/fix-payment-bug):

Created from main to address urgent production issues.
Merged back into main and develop.


Real-World Scenario Example
Project: E-commerce Website
Current production version: v1.1 (on main).
Next planned release: v1.2.
Feature Development:

feature/add-to-cart branch is created, developed, and merged into develop.
feature/payment-gateway branch is created, developed, and merged into develop.

Release Preparation:

A release/v1.2 branch is created from develop for testing.
QA tests the release branch.
Minor bugs in the checkout flow are fixed in release/v1.2. directly

Production Deployment:

Once testing is complete, release/v1.2 is merged into main and devleop
The CI/CD pipeline automatically deploys main to production.

Hotfix:

A critical payment bug is discovered in production (v1.2).
A hotfix/fix-payment-bug branch is created from main.
The fix is tested and merged into both main and develop.


END




How can a current instance be added to a new Autoscaling group?How can a current instance be added to a new Autoscaling group?

42. What are the different types of instances available?
Below we have mentioned the following types of instances that are available:

General-purpose
Storage optimize
Accelerated computing
Computer-optimized
Memory-optimized

4. Differentiate between vertical and horizontal scaling in AWS.
The main difference between vertical and horizontal scaling is how you add compute resources to your infrastructure. In vertical scaling, more power is added to the existing machine. In contrast, in horizontal scaling, additional resources are added to the system with the addition of more machines into the network so that the workload and processing are shared among multiple devices.

Amazon VPC Flow Logs:

Amazon VPC Flow logs collect information specific to the IP traffic, incoming and outgoing from the Amazon Virtual Private Cloud (Amazon VPC) network interfaces. They can be applied, as per requirements, at the VPC, subnet, or individual Elastic Network Interface level. VPC Flow log data is stored using Amazon CloudWatch Logs. To perform any additional processing or analysis, the VPC Flow log data can be exported using Amazon CloudWatch. 

AWS CloudTrail: 

This AWS service facilitates security analysis, compliance auditing, and resource change tracking of an AWS environment. It can also provide a history of AWS API calls for a particular account. CloudTrail is an essential AWS service required to understand AWS use and should be enabled in all AWS regions for all AWS accounts, irrespective of where the services are deployed. CloudTrail delivers log files and an optional log file integrity validation to a designated Amazon S3 (Amazon Simple Storage Service) bucket once almost every five minutes

18. What is a DDoS attack, and how can you handle it?
A Denial of Service (DoS) attack occurs when a malicious attempt affects the availability of a particular system, such as an application or a website, to the end-users. A DDoS attack or a Distributed Denial of Service attack occurs when the attacker uses multiple sources to generate the attack.DDoS attacks are generally segregated based on the layer of the Open Systems Interconnection (OSI) model that they attack. The most common DDoS attacks tend to be at the Network, Transport, Presentation, and Application layers, corresponding to layers 3, 4, 6, and 7, respectively.



CICD work flow in aws:

In AWS cicd pipeline our sourcecode is placed in the GIT repository, where all our application code will be placed there and if there is any change in the code it will trigger the code-pipeline. code-pipeline is nothing but it is an orchestrator which will triger the code build. where all our pipeline script will be placed in the buildspec.yaml. in that we will mention all the things like the code should go thirough unit test, code quality , and then build the image  and then it will scan the image, once image build it will push the image to docker hub,  so these things are coded in the buildspec.yaml file. so the code pipeline trigger the code build , which will run all these stages once it is completed , then it will trigger the code-deploy, so in the code-deploy we mention where to deploy our application whether its ec2 or ecs kind of things. so in case if we have to push the code to eks . then we dont need the code deply , in the buildspec.yaml itself we can mention in the post-build like 

    - echo "Deploying to EKS..."
      - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region us-east-1
      - kubectl apply -f $DEPLOYMENT_MANIFEST
      - echo "Deployment successful!"

so, everything will be done in buildspec itself. 



3. CI/CD Workflow for Production Deployment
1️⃣ Developer Pushes Code to GitHub
2️⃣ AWS CodePipeline Triggers CodeBuild
3️⃣ CodeBuild Executes the buildspec.yml
✅ Unit Tests (pytest)
✅ Code Quality (pylint)
✅ Security Scans (bandit, safety)
✅ API & Load Testing (pytest, locust) and SonarQube test will be done
✅ Docker Image Build & Scan (trivy)
4️⃣ Docker Image is Pushed to DockerHub
5️⃣ EC2 Auto Scaling Group  or ec2 Pulls the New Image & Deploys

Unit Tests	                pytest	✅ Yes
Code Quality (Linting)	        pylint	✅ Yes
Security Scan	                bandit	✅ Yes
Integration Tests (APIs & DBs)	pytest + requests	✅ Yes
Docker Image Security Scan	trivy	✅ Yes


BUILD SPEC.YML


version: 0.2

env:
  variables:
    SONAR_HOST_URL: "https://sonarqube.example.com"   # Replace with your SonarQube server URL
    PROJECT_KEY: "my-python-project"
    REGISTRY_URL: "quay.io/your-namespace"
  parameter-store:
    SONAR_TOKEN: "/sonarqube/token"   # Fetch from AWS Systems Manager Parameter Store
    QUAY_USERNAME: "/quay/username"
    QUAY_PASSWORD: "/quay/password"

phases:
  install:
    runtime-versions:
      python: 3.9
      java: corretto11   # Required for SonarScanner
      docker: latest
    commands:
      - echo "Installing dependencies..."
      - pip install --upgrade pip
      - pip install -r requirements.txt
      - echo "Installing SonarScanner..."
      - curl -o sonar-scanner.zip https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-4.7.0.2747-linux.zip
      - unzip sonar-scanner.zip
      - export PATH=$PATH:$(pwd)/sonar-scanner-4.7.0.2747-linux/bin
      - echo "Installing security tools..."
      - pip install bandit safety

  pre_build:
    commands:
      - echo "Running security scans..."
      - bandit -r .   # Security scan for Python
      - safety check   # Check for known vulnerabilities in dependencies
      - echo "Running unit tests..."
      - pytest --junitxml=report.xml  # Run unit tests
      - echo "Running SonarQube Analysis..."
      - sonar-scanner \
          -Dsonar.projectKey=$PROJECT_KEY \
          -Dsonar.sources=. \
          -Dsonar.host.url=$SONAR_HOST_URL \
          -Dsonar.login=$SONAR_TOKEN \
          -Dsonar.python.coverage.reportPaths=coverage.xml

  build:
    commands:
      - echo "Logging into Quay.io..."
      - echo $QUAY_PASSWORD | docker login -u $QUAY_USERNAME --password-stdin quay.io
      - BUILD_TAG=$(date +%Y%m%d%H%M%S)
      - IMAGE_NAME="$REGISTRY_URL/my-python-app:$BUILD_TAG"
      - echo "Building Docker image: $IMAGE_NAME"
      - docker build -t $IMAGE_NAME .
      - docker push $IMAGE_NAME
      - echo "Docker image pushed successfully!"

  post_build:
    commands:
      - echo "Updating Kubernetes deployment manifest..."
      - sed -i "s|image: .*|image: $IMAGE_NAME|" deployment.yaml
      - git config --global user.email "cicd-bot@example.com"
      - git config --global user.name "CI/CD Bot"
      - git add deployment.yaml
      - git commit -m "Updated deployment image to $IMAGE_NAME"
      - git push origin main
      - echo "Deployment manifest updated successfully!"

artifacts:
  files:
    - report.xml


Install	       Install dependencies, SonarScanner, security tools (Bandit, Safety)
Pre-Build	Run security scans, unit tests, and SonarQube analysis
Build	         Build and push Docker image to Quay.io
Post-Build	Update Kubernetes manifest in Git for ArgoCD deployment



Instead of installing tools on every build, create a custom Docker image with all dependencies pre-installed.
Store this image in Amazon ECR or Quay.io and use it in your CodeBuild project.
so while creating the code build use this image.

AFTER USING PREINSTALLED IMAGE


version: 0.2

env:
  variables:
    SONAR_HOST_URL: "https://sonarqube.example.com"
    PROJECT_KEY: "my-python-project"
    REGISTRY_URL: "quay.io/your-namespace"
  parameter-store:
    SONAR_TOKEN: "/sonarqube/token"
    QUAY_USERNAME: "/quay/username"
    QUAY_PASSWORD: "/quay/password"

phases:
  pre_build:
    commands:
      - echo "Fetching credentials from AWS Parameter Store..."
      - export SONAR_TOKEN=$(aws ssm get-parameter --name /sonarqube/token --with-decryption --query Parameter.Value --output text)
      - export QUAY_USERNAME=$(aws ssm get-parameter --name /quay/username --with-decryption --query Parameter.Value --output text)
      - export QUAY_PASSWORD=$(aws ssm get-parameter --name /quay/password --with-decryption --query Parameter.Value --output text)

      - echo "Running Security Scans..."
      - bandit -r .  # Security scanning for Python code
      - safety check  # Dependency vulnerability scanning

      - echo "Running Unit Tests..."
      - pytest --junitxml=report.xml  # Running Python unit tests

      - echo "Running SonarQube Analysis..."
      - sonar-scanner \
          -Dsonar.projectKey=$PROJECT_KEY \
          -Dsonar.sources=. \
          -Dsonar.host.url=$SONAR_HOST_URL \
          -Dsonar.login=$SONAR_TOKEN \
          -Dsonar.python.coverage.reportPaths=coverage.xml

  build:
    commands:
      - echo "Logging into Quay.io..."
      - echo $QUAY_PASSWORD | docker login -u $QUAY_USERNAME --password-stdin quay.io

      - BUILD_TAG=$(date +%Y%m%d%H%M%S)
      - IMAGE_NAME="$REGISTRY_URL/my-python-app:$BUILD_TAG"
      - echo "Building Docker image: $IMAGE_NAME"
      - docker build -t $IMAGE_NAME .

      - echo "Scanning Docker image for vulnerabilities using Trivy..."
      - trivy image --exit-code 1 --severity CRITICAL,HIGH $IMAGE_NAME
      - echo "Trivy scan passed!"

      - echo "Pushing Docker image to Quay.io..."
      - docker push $IMAGE_NAME
      - echo "Docker image pushed successfully!"

  post_build:
    commands:
      - echo "Updating Kubernetes Deployment YAML..."
      - sed -i "s|image: .*|image: $IMAGE_NAME|" deployment.yaml
      - git config --global user.email "cicd-bot@example.com"
      - git config --global user.name "CI/CD Bot"
      - git add deployment.yaml
      - git commit -m "Updated deployment image to $IMAGE_NAME"
      - git push origin main
      - echo "Deployment YAML updated successfully!"

artifacts:
  files:
    - report.xml


NOTE: instead of installing SonarQube and trivy in the code-build image, just we can have separate instance where both installed separately and running so that all the pipelines can use this server for sonarque and scaning images , instead of installing them in the codebuild image which is not stable.  so if we installed separately means we can call this remotely in the buildspec.yml file and we can do the scan . 




CODE DEPLOY :


PYTHON CODE INSIDE FILE

Docker file
app.py   - application code 
appspec.yml     --- mentioned below
buildspec.yml
requirements.txt  - (List of dependencies) like flash gunicorn
start_container.sh   
stop_container.sh

IN APPSEPC.YML we have included start_container.sh and stop_container.sh

hooks:
 ApplicationStop:
  - location: stop_container.sh ( In this sh , we have to stop the previously running container, why because before inititating the deployment we have to stop the previous one right)
    timeout :300
    runas: root
 AfterInstall: 
 - location: start_container.sh   --( in this sh  we have two commands, pull the docker image and run the container , I have mentioned the command below)
    timeout :300
    runas: root
   


start_container.sh
docker pull abhishekf5/simple-phython-flask-app
docker run -d -p 5000:5000 abhishekf5/simple-phython-flask-app

Stop_container.sh    - which is used to remove the previous container, otherwise when the new container created it will be collapsed 

containerid= 'docker ps | awk -F " " '{print $1}''
docker rm -f $containerid   


code deploy -> create application -> Application name ( sample-python-flask-app ) we have many modules so each module is an application right for example payment moduel is an application so we can give payment-dev as a application name. but in this we followed the video so gave sample-python  - and next option is compute platform in this we can choose where we going to deploy the application ( ec2/onpremisis , ecs, or lamda) choose ec2/onpremisis.  and create application.  so if we have many modeules like 50 modules we have to create in this way for each module as an application . to automate we can use CLI for creating bunch

Now create an ec2 instance (where our application is going to be installed as a docker , so install the docker also in this ec2). ec2 name sample-python and create a tag "environment : prod"  "project : payment"  so when our we going to deploy the application we will choose the tag name so that the application will be install in the all the ec2 instance which has this tag name . 
Once created the ec2 instance we have to install the code-deploy agent in the ec2 instance. ( code-deploy should be install in all the ec2 instance where code-deploy is going to deploy our application )

Note : IAM Roles is mainly used for services to talk with other services whereas IAM user is to used for talk with services direclty

NOW create a role :  for giving permison for accessing the code-deploy 
entity - aws service  usecase-> code deploy   rolenaeme -> ec2-codedeply-role    (permission will be automatically taken check that) . once role create attach to the ec2 instance that we created.

Now we come back to code-deploy ( till now we created only application) 

go to the code-deply - go to the applicaction that we created --> there will be option for ( deploymets and deployment group)  deployment group is nothing but target group where our application is going to be installed.   so choose the deployment group and give deployemnt group name "sample-python-app"  and choose the role " that we created above " ( in that role we just give access to code-deploy permission , so go and edit that role and give access to ec2 also . so that this role has permssion for ec2 to acess the code-deply and code-deploy to access the ec2 , both way it can be accessed. or if you want you can create 2 seperate roles )
deployment type -> choose "in-place"  just for demo ,, we do have blue-green deployment also
environment configuration ->  choose amazon ec2 instance , we do have also auto scaling groups
Once ec2 instance chossed, it will ask the tag name, enter the tag name that we given to ec2 isntance. so all the instance that have this tag name will be considered. 
and create deployment group ( we do have some other option also LB and other but we dont do that for demo) finally deployment group created. now we have to create the deployment which I mentioned in starting. 

for code build we have buildspec.yaml like that for create deployment we have appsepc.yml

Create deployment ( in this we will say where our application code is present )
create deployment -> enter deployment group name  which we created above " sample-python-app"  and then  choose the git hub option  where our code is stored ( we have other option s3 bucket) and then give repository name   and then create deployment.

once we click on the create deployment it will start deploying the application on our ec2 instance as a docker image ( this is manual for testing purpose, for production level it will automaticall run when there is a code change ) . so what are all the process it is doing while depolyment ?

there are many steps like (Applicationstop, dowloadBundle, BeforeInstall, Install, AfterInstall, ApplicationStart, validateService). In these steps we going to run oly AfterInstall and ApplicationStop , where in these we have already mentioned 2 scripts above to run on this in the appspec.yaml.  All together is when we start the deployment it will run the appspec.yml file configuration , in that ApplicatonStop and AfterInstall are going to run the script .  so first it will remove the old container which is running , and then it will pull the new images run the new container with the changes that happened in the code . 

Now Till Now we created the CODE DEPLOY  and tested. Now we have to include this in the code pipeline , so once the build is completed it will automatically trigger the code-deploy of the deployment 

so go to the code-pipeline and choose our code-pipeline and click the add stage and choose the code-deploy  and choose the application name (code-deplyname that we initiall created when we start the code-deploy) and choose the deployment group and click done. thats it .  Now if we have any changes in the code it will automatically trigger the pipeline. 


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

cross account s3
s3 transffer accelerator tool
aws datasysnc 
creating cloutfront distribution for s3 bucket
Mounting the s3 bucket in server (both windows and linux), using storage gateway 
Adding storage to the server , increasing root volume.
#######################################
Moving EBS volume from one region to other region :  take a snapshot of volume , move the snapshot to other region ( by copying we can choose other region), create a volume using that snapshot.

####################################
Moving one ec2 instance from one region to other region -->  take snapshot of the ebs volume -> go to snapshot and convert the snapshot into ami -> once ami created move that ami to other region ( by copyying chossing other region)-> once done create ec2 in other region
######################################
Moving snapshot from one acc to other acc -> (if the snapshot is encrypted we have to make a copy of the source snapshot, while making the copy we have to choose "diffrent key for encryption")

Choose the copied snapshot and choose modify permission and give the account ID (which you need to share) 

we need to share the encryption key to the other account -> go to IAM -> encryption key - > choose the key that you used -> "OTHER AWS account" option enter the  other aws account id

NOW both snapshot and key are shared to other account.

CNAME & ALIAS:

Cname : routes traffic to another domain
Cname  -> we can give cname for one domain name to other domain name  eg : dev.srini.com  cname -> prod.srini.com

where here domain name is srini.com  --> we cannot create cname for srini.com -> prod.srini.com   it will not accept. 

ALias -> here we can create alias directly for srini.com 
Alias can be used for other aws services endpoint like s3bucket , cloudfront, VPC endpoint and also LB

eg : srini.com -> s3url (or) vpcendpoint etc..   


############################################

Mounting EFS for the requested server.
giving access to s3 bucket using VPC endpoint ( amazonn s3 gateway)
Creating private link using vpc endpoint service 
Connecting two VPC using transist gateway

 






cname and alias





Dec_2024


virtual private gateway
alias record in s3
Custom domain for an Amazon S3 bucket using Route 53?
VPN
SAML login

103.98.209.18

Punitharaja25@gmail.com

Yuvayazhini11$
https://www.youtube.com/watch?v=pXNB8uAGlx8 -- cicd


03.03.2025    interviee 
